---
title: "security: A one-prompt attack that breaks LLM safety alignment"
date: 2026-02-09T17:12:11.000Z
slug: a-one-prompt-attack-that-breaks-llm-safety-alignment
update_categories: ["security"]
update_tags: ["LLM safety", "prompt injection", "alignment", "insufficient source content"]
update_bullets: ["No description of the “one-prompt attack” mechanism is included in the supplied content.", "No affected models, attack prerequisites, impact assessment, or evaluation results are provided in the excerpt.", "No defenses, mitigations, or recommended best practices are present in the excerpt."]
timeframes: ["2026-02"]
link: "https://www.microsoft.com/en-us/security/blog/2026/02/09/prompt-attack-breaks-llm-safety/"
source: "Microsoft Security Blog"
timeframeKey: "2026-02"
id: "C0C892DFBC28BCD29D63C61BA9FAEEF2A72D6F58BD56A1F70ABD0F0F38AA1006"
contentHash: "4327685D2AF1622DC0493ACDA0CC7E43D27F024C948046ABF4C16560981C688F"
draft: false
type: "updates2"
llmSummary: "The provided excerpt is only a short teaser and does not include the article’s technical details, findings, or mitigations."
---

The provided excerpt is only a short teaser and does not include the article’s technical details, findings, or mitigations.

- **Source:** [Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2026/02/09/prompt-attack-breaks-llm-safety/)
