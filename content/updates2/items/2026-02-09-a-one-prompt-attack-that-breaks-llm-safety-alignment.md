---
title: "security: A one-prompt attack that breaks LLM safety alignment"
date: 2026-02-09T17:12:11.000Z
slug: a-one-prompt-attack-that-breaks-llm-safety-alignment
update_categories: ["security"]
update_tags: ["LLM security", "prompt injection", "safety alignment", "Microsoft Security Blog"]
update_bullets: ["No technical details about the attack, impacted models, conditions, or mitigations are included in the supplied content.", "Source referenced: Microsoft Security Blog, dated 2026-02-09 (per URL)."]
timeframes: ["2026-02"]
link: "https://www.microsoft.com/en-us/security/blog/2026/02/09/prompt-attack-breaks-llm-safety/"
source: "Microsoft Security Blog"
timeframeKey: "2026-02"
id: "3A9FB19A143340C5EC8D55B5403D15875DFCF647A780DAF588D455BD63A37BB5"
contentHash: "4327685D2AF1622DC0493ACDA0CC7E43D27F024C948046ABF4C16560981C688F"
draft: false
type: "updates2"
llmSummary: "The provided excerpt only states that a Microsoft Security Blog post titled “A one-prompt attack that breaks LLM safety alignment” exists and emphasizes the importance of safety alignment for LLMs and diffusion models."
---

The provided excerpt only states that a Microsoft Security Blog post titled “A one-prompt attack that breaks LLM safety alignment” exists and emphasizes the importance of safety alignment for LLMs and diffusion models.

- **Source:** [Microsoft Security Blog](https://www.microsoft.com/en-us/security/blog/2026/02/09/prompt-attack-breaks-llm-safety/)
