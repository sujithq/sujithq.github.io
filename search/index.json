[{"content":"Part 7: Testing and Debugging Async Code You‚Äôve learned how to write async code ‚Äî but how do you test it and debug it when things go wrong? Async introduces unique challenges: hidden continuations, exceptions in background tasks, and ‚Äúlost‚Äù operations. Let‚Äôs tackle them.\nWriting Async Unit Tests Most modern test frameworks (xUnit, NUnit, MSTest) fully support async tests.\n‚úÖ Correct:\n[Fact] // xUnit public async Task GetNumberAsync_Returns42() { var result = await GetNumberAsync(); Assert.Equal(42, result); } ‚ö†Ô∏è Avoid this (blocks async code and may deadlock):\n[Fact] public void BadTest() { var result = GetNumberAsync().Result; // ‚ùå Assert.Equal(42, result); } Rule: Your test method should return Task if it calls async code.\nMocking Async Methods When testing async code, you‚Äôll often need to mock dependencies.\nExample with Moq:\nvar mockRepo = new Mock\u0026lt;IUserRepository\u0026gt;(); mockRepo.Setup(r =\u0026gt; r.GetUserAsync(It.IsAny\u0026lt;int\u0026gt;())) .ReturnsAsync(new User { Id = 1, Name = \u0026#34;Alice\u0026#34; }); var service = new UserService(mockRepo.Object); var user = await service.GetUserAsync(1); Assert.Equal(\u0026#34;Alice\u0026#34;, user.Name); Here, ReturnsAsync makes it easy to simulate async results.\nTesting Multiple Tasks and Exceptions When using Task.WhenAll, multiple exceptions are aggregated. Test them explicitly:\nvar t1 = FailsAsync(); var t2 = FailsAsync(); var ex = await Assert.ThrowsAsync\u0026lt;AggregateException\u0026gt;(async () =\u0026gt; await Task.WhenAll(t1, t2)); Assert.Equal(2, ex.InnerExceptions.Count); Or unwrap with try/catch:\ntry { await Task.WhenAll(t1, t2); } catch (Exception e) { // e is AggregateException in .NET Framework, but flattens to first inner in .NET Core when awaited; check tasks for details var errors = new[] { t1.Exception, t2.Exception }; } Debugging Async Code in Visual Studio Async stacks can look confusing, but Visual Studio helps:\nTasks Window (Debug \u0026gt; Windows \u0026gt; Parallel Tasks)\nShows all active tasks, their status, and call stacks.\nAsync Call Stacks (Debug \u0026gt; Options \u0026gt; Enable Just My Code and ensure \u0026ldquo;Show Async Call Stack\u0026rdquo;)\nPreserves the logical async flow across awaits.\nConfigure Breakpoints\nUse conditions and actions to break when a specific OperationId or correlation ID is present in logs.\nTracing Async Flows with Logging and Activity Logging is critical to see what‚Äôs happening between awaits.\nExample with ILogger (ASP.NET Core):\npublic async Task ProcessAsync(ILogger logger, CancellationToken ct) { logger.LogInformation(\u0026#34;Starting work\u0026#34;); await Task.Delay(1000, ct); logger.LogInformation(\u0026#34;Work complete\u0026#34;); } For cross-service tracing, use System.Diagnostics.Activity (and OpenTelemetry if available):\nusing var activity = new Activity(\u0026#34;FetchCustomer\u0026#34;) .AddTag(\u0026#34;customer.id\u0026#34;, id.ToString()); activity.Start(); var customer = await repo.GetCustomerAsync(id, ct); activity.Stop(); With OpenTelemetry exporters (Jaeger, Zipkin, OTLP), you can see async spans stitched together.\nHandling Unobserved and Fire‚Äëand‚ÄëForget Exceptions Async tasks can fail silently if you forget to await them. .NET raises TaskScheduler.UnobservedTaskException, but it‚Äôs often too late and can be GC‚Äëdependent.\n‚úÖ Patterns that help:\n// 1) Prefer awaiting await DoWorkAsync(); // 2) If fire-and-forget is necessary, capture errors _ = Task.Run(async () =\u0026gt; { try { await DoWorkAsync(); } catch (Exception ex) { logger.LogError(ex, \u0026#34;Background task failed\u0026#34;); } }); In libraries, consider returning an IAsyncDisposable that manages background lifetimes so callers can dispose and observe errors on shutdown.\nDeterministic Tests with Time and Cancellation Make tests reliable by controlling time and tokens:\nPass a CancellationToken into your methods and cancel in tests to assert behavior. Abstract timers/delays behind an interface so you can fake time in tests (e.g., wrap Task.Delay behind ITimer.DelayAsync). Example fake timer:\npublic interface ITimer { Task Delay(TimeSpan delay, CancellationToken ct = default); } public sealed class RealTimer : ITimer { public Task Delay(TimeSpan delay, CancellationToken ct = default) =\u0026gt; Task.Delay(delay, ct); } public sealed class FakeTimer : ITimer { public TaskCompletionSource Completed { get; } = new(); public Task Delay(TimeSpan delay, CancellationToken ct = default) =\u0026gt; Completed.Task; } Common Testing Smells (and Fixes) Using .Result/.Wait() in tests ‚Üí switch to async Task tests. Races on shared state ‚Üí isolate state per test or use fixtures with fresh instances. Flaky tests due to real network/IO ‚Üí mock or use local test servers/files. Hidden deadlocks ‚Üí remove context capture in test targets (ConfigureAwait(false) in libraries). Key Takeaways Test methods should be async Task ‚Äî never block on async. Use mocking frameworks‚Äô async helpers (ReturnsAsync) to simulate work. Visual Studio‚Äôs Tasks and Async Call Stacks make async debugging tractable. Add structured logging and activities for traceability across awaits. Treat fire‚Äëand‚Äëforget carefully; capture and log exceptions. Series Navigation Previous: Part 6 ‚Äì Advanced Topics Series Index: Overview\nüéâ Series Recap Intro ‚Äî why async matters Deep Dive ‚Äî how async/await works Pitfalls ‚Äî deadlocks, async void, ConfigureAwait Patterns ‚Äî parallelism, cancellation, streaming, timeouts Real‚ÄëWorld ‚Äî APIs, files, DB, UI, services Advanced ‚Äî ValueTask, custom awaiters, coordination Testing \u0026amp; Debugging ‚Äî reliable tests and effective troubleshooting You now have a complete async/await toolbox for C#. Happy coding!\n","date":"2025-10-29T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part7/cover_hu_23237e53f25bf31a.jpg","image":"https://quintelier.dev/posts/2025/10/csharp-async-await-part7/cover_hu_5e040329a9201f2c.jpg","permalink":"https://quintelier.dev/posts/2025/10/csharp-async-await-part7/","title":"Mastering Asynchronous Programming with C# async/await - Part 7: Testing \u0026 Debugging","webpImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part7/cover_hu_36b8576be8de006a.webp"},{"content":"Part 6: Advanced Topics Now that we‚Äôve mastered the basics and patterns, let‚Äôs go further into advanced async scenarios you‚Äôll meet when optimizing throughput or designing libraries.\nValueTask vs Task Task is a reference type allocated on the heap. In ultra‚Äëhot paths where many calls complete synchronously, ValueTask can save allocations.\npublic async ValueTask\u0026lt;int\u0026gt; GetNumberAsync(bool cached) { if (cached) return 42; // completes synchronously (no allocation) await Task.Delay(1000); // completes asynchronously return 42; } Guidelines\nPrefer Task by default (simpler, safer). Consider ValueTask when profiling shows allocation pressure and many sync completions. Don‚Äôt await a ValueTask more than once and don‚Äôt store it; convert to Task via .AsTask() if you must pass it around. Custom Awaiters (rare, but educational) Anything with a GetAwaiter() that returns an awaiter implementing INotifyCompletion (or ICriticalNotifyCompletion) is awaitable.\npublic sealed class DelayAwaitable { private readonly int _ms; public DelayAwaitable(int ms) =\u0026gt; _ms = ms; public TaskAwaiter GetAwaiter() =\u0026gt; Task.Delay(_ms).GetAwaiter(); } // usage await new DelayAwaitable(250); Console.WriteLine(\u0026#34;custom awaiter resumed\u0026#34;); When would you do this?\nDomain‚Äëspecific scheduling/timing semantics. Interop layers.\nMost apps never need it; understanding it helps you reason about await. Async Coordination Primitives Limit concurrency with SemaphoreSlim var gate = new SemaphoreSlim(3); // allow 3 concurrent ops async Task ProcessAsync(string item) { await gate.WaitAsync(); try { await DoWorkAsync(item); } finally { gate.Release(); } } Use this to throttle IO‚Äëheavy fan‚Äëout work (APIs, disk, DB) and avoid overloading dependencies.\nPipelines with Channel\u0026lt;T\u0026gt; System.Threading.Channels enables high‚Äëthroughput producer/consumer pipelines.\nvar channel = Channel.CreateUnbounded\u0026lt;string\u0026gt;(); // producer _ = Task.Run(async () =\u0026gt; { foreach (var url in urls) await channel.Writer.WriteAsync(url); channel.Writer.Complete(); }); // consumer(s) await foreach (var url in channel.Reader.ReadAllAsync()) { var html = await client.GetStringAsync(url); Console.WriteLine($\u0026#34;{url} -\u0026gt; {html.Length} bytes\u0026#34;); } Channels support backpressure (bounded channels), multiple consumers, and graceful completion.\nPerformance Tuning Checklist Avoid unnecessary Task.Run in ASP.NET Core ‚Äî IO work should be awaited; Task.Run is for CPU‚Äëbound work that you explicitly want off the request thread. Batch awaits with Task.WhenAll to reduce continuation overhead when independent operations can run together. Use ConfigureAwait(false) in libraries/background services to skip context capture. (In ASP.NET Core there‚Äôs no synchronization context, but it still avoids overhead.) Minimize layers of trivial async wrappers; if you just return a task, you may not need async/await at that layer. Pool and reuse expensive objects correctly (e.g., reuse HttpClient via IHttpClientFactory). Measure first with a profiler (allocs, context switches, contention) before micro‚Äëoptimizing. Debuggable, Defensive Async Prefer cancellable APIs: accept a CancellationToken. Use timeouts where appropriate to avoid stuck requests. Add structured logging around awaits (operation name, correlation IDs). In fire‚Äëand‚Äëforget scenarios, capture and log exceptions inside the launched task. Key Takeaways ValueTask can reduce allocations in hot paths; default to Task otherwise. Custom awaiters are a niche but deepen understanding of await. SemaphoreSlim and Channel\u0026lt;T\u0026gt; are your go‚Äëto tools for concurrency control and pipelines. Tune with evidence: profile, batch awaits, and avoid unnecessary context capture. üëâ In Part 7, we‚Äôll wrap up with testing and debugging async code: async unit tests, mocking, Visual Studio‚Äôs async tools, and tracing async flows.\nSeries Navigation Previous: Part 5 ‚Äì Real-World Use Cases Series Index: Overview Next: Part 7 ‚Äì Testing \u0026amp; Debugging\n","date":"2025-10-22T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part6/cover_hu_2cd80681e325619.jpg","image":"https://quintelier.dev/posts/2025/10/csharp-async-await-part6/cover_hu_5aa51e2879b82e48.jpg","permalink":"https://quintelier.dev/posts/2025/10/csharp-async-await-part6/","title":"Mastering Asynchronous Programming with C# async/await - Part 6: Advanced Topics","webpImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part6/cover_hu_6d3945c151ef9d05.webp"},{"content":"Part 5: Real-World Use Cases We‚Äôve covered the foundations, pitfalls, and async patterns. Now let‚Äôs see how async/await shows up in real applications ‚Äî from APIs to UI apps.\nUse Case 1: Calling Web APIs with HttpClient HttpClient is fully async, making it ideal for network calls.\npublic async Task GetWeatherAsync() { var client = new HttpClient(); var response = await client.GetStringAsync(\u0026#34;https://api.weather.com/data\u0026#34;); Console.WriteLine(response); } This avoids blocking threads while waiting for the response.\nUse Case 2: File I/O The System.IO namespace supports async methods:\npublic async Task WriteFileAsync(string path, string content) { await File.WriteAllTextAsync(path, content); } Similarly, reading large files:\npublic async Task\u0026lt;string\u0026gt; ReadFileAsync(string path) { return await File.ReadAllTextAsync(path); } This keeps applications responsive even with big files.\nUse Case 3: Database Access with EF Core Entity Framework Core has async APIs for queries:\nvar users = await db.Users.Where(u =\u0026gt; u.IsActive).ToListAsync(); This is critical in ASP.NET Core apps ‚Äî the thread isn‚Äôt blocked while waiting for the database.\nUse Case 4: Responsive UIs In WPF or WinForms, async keeps the UI thread free:\nprivate async void Button_Click(object sender, EventArgs e) { StatusLabel.Text = \u0026#34;Loading...\u0026#34;; var data = await GetDataAsync(); StatusLabel.Text = $\u0026#34;Done: {data}\u0026#34;; } Without async, the UI would freeze until the task completes.\nUse Case 5: Background Services In ASP.NET Core, background tasks are implemented with IHostedService:\npublic class Worker : BackgroundService { protected override async Task ExecuteAsync(CancellationToken stoppingToken) { while (!stoppingToken.IsCancellationRequested) { Console.WriteLine(\u0026#34;Worker running...\u0026#34;); await Task.Delay(1000, stoppingToken); } } } Async keeps the service efficient and responsive to cancellation.\nKey Takeaways HttpClient is async-first ‚Äî perfect for APIs. File I/O with async avoids UI freezes. EF Core async queries keep ASP.NET scalable. UI apps must use async to stay responsive. Background services rely on async for efficiency. Series Navigation Previous: Part 4 ‚Äì Patterns Series Index: Overview Next: Part 6 ‚Äì Advanced Topics\n","date":"2025-10-15T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part5/cover_hu_b498f36dfda13462.jpg","image":"https://quintelier.dev/posts/2025/10/csharp-async-await-part5/cover_hu_3b45e400d4d950ff.jpg","permalink":"https://quintelier.dev/posts/2025/10/csharp-async-await-part5/","title":"Mastering Asynchronous Programming with C# async/await - Part 5: Real-World Use Cases","webpImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part5/cover_hu_85617ee8c308610a.webp"},{"content":"Part 4: Patterns with Async So far, we‚Äôve looked at the basics and pitfalls of async/await. Now let‚Äôs move to patterns that help you handle real-world scenarios like parallelism, cancellation, streaming, and timeouts.\nPattern 1: Running Tasks in Parallel Sometimes you want to start multiple operations and wait for all of them.\npublic async Task FetchInParallelAsync() { var client = new HttpClient(); var task1 = client.GetStringAsync(\u0026#34;https://example.com/page1\u0026#34;); var task2 = client.GetStringAsync(\u0026#34;https://example.com/page2\u0026#34;); var results = await Task.WhenAll(task1, task2); Console.WriteLine($\u0026#34;Page1 length: {results[0].Length}\u0026#34;); Console.WriteLine($\u0026#34;Page2 length: {results[1].Length}\u0026#34;); } ‚úÖ Use Task.WhenAll when you need all tasks to finish.\n‚úÖ Use Task.WhenAny when you only care about the first result:\nvar firstFinished = await Task.WhenAny(task1, task2); Console.WriteLine(await firstFinished); Pattern 2: Cancelling Tasks Long-running operations should be cancellable. Enter CancellationToken.\npublic async Task DoWorkAsync(CancellationToken token) { for (int i = 0; i \u0026lt; 10; i++) { token.ThrowIfCancellationRequested(); Console.WriteLine($\u0026#34;Step {i}\u0026#34;); await Task.Delay(500, token); } } Usage:\nvar cts = new CancellationTokenSource(); var task = DoWorkAsync(cts.Token); // Cancel after 2 seconds cts.CancelAfter(TimeSpan.FromSeconds(2)); try { await task; } catch (OperationCanceledException) { Console.WriteLine(\u0026#34;Task was cancelled.\u0026#34;); } Pattern 3: Async Streams Introduced in C# 8, IAsyncEnumerable\u0026lt;T\u0026gt; lets you stream data asynchronously.\npublic async IAsyncEnumerable\u0026lt;int\u0026gt; GetNumbersAsync() { for (int i = 1; i \u0026lt;= 5; i++) { await Task.Delay(500); // simulate delay yield return i; } } Usage:\nawait foreach (var number in GetNumbersAsync()) { Console.WriteLine(number); } This is great for processing large data sets or network streams without loading everything into memory.\nPattern 4: Timeouts with Task.Delay You can combine Task.Delay with Task.WhenAny to implement timeouts:\npublic async Task\u0026lt;string\u0026gt; FetchWithTimeoutAsync(string url, int timeoutMs) { var client = new HttpClient(); var fetchTask = client.GetStringAsync(url); var timeoutTask = Task.Delay(timeoutMs); var finished = await Task.WhenAny(fetchTask, timeoutTask); if (finished == timeoutTask) throw new TimeoutException(\u0026#34;The request timed out\u0026#34;); return await fetchTask; // Safe to await now } Bonus: Retry Pattern Transient failures (like network hiccups) are common. A simple retry looks like this:\npublic async Task\u0026lt;T\u0026gt; RetryAsync\u0026lt;T\u0026gt;(Func\u0026lt;Task\u0026lt;T\u0026gt;\u0026gt; operation, int maxRetries = 3) { for (int i = 0; i \u0026lt; maxRetries; i++) { try { return await operation(); } catch when (i \u0026lt; maxRetries - 1) { await Task.Delay(500); // backoff } } throw new Exception(\u0026#34;Operation failed after retries.\u0026#34;); } Key Takeaways Use Task.WhenAll/Task.WhenAny for parallelism. Always provide cancellation with CancellationToken. Use IAsyncEnumerable\u0026lt;T\u0026gt; for streaming async data. Combine Task.Delay with Task.WhenAny for timeouts. Add retries for resilient systems. üëâ In Part 5, we‚Äôll look at real-world use cases:\nCalling web APIs with HttpClient File I/O with async streams Database queries with EF Core Keeping UIs responsive Series Navigation Previous: Part 3 ‚Äì Pitfalls \u0026amp; Best Practices Series Index: Overview Next: Part 5 ‚Äì Real-World Use Cases\n","date":"2025-10-08T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part4/cover_hu_133c30b8b8b46cb9.jpg","image":"https://quintelier.dev/posts/2025/10/csharp-async-await-part4/cover_hu_7a707f4d0454428d.jpg","permalink":"https://quintelier.dev/posts/2025/10/csharp-async-await-part4/","title":"Mastering Asynchronous Programming with C# async/await - Part 4: Patterns with Async","webpImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part4/cover_hu_8076eb31eec72d7.webp"},{"content":"Introduction Welcome to Part 5 of the GitHub Certification Journey! üõ°Ô∏è\nAfter mastering GitHub Foundations (GH-900), GitHub Administration (GH-100), GitHub Actions (GH-200), and GitHub Copilot (GH-300), you\u0026rsquo;re ready to dive deep into GitHub Advanced Security - the comprehensive security platform that transforms how teams identify, manage, and remediate vulnerabilities throughout the software development lifecycle. The GH-500 GitHub Advanced Security certification validates your ability to implement, configure, and manage enterprise-grade security practices.\nThis comprehensive guide provides everything needed to pass the GH-500 exam and become a GitHub security expert. Whether you\u0026rsquo;re a security engineer, DevSecOps specialist, or developer looking to enhance security practices, this preparation roadmap will take you from basic security awareness to advanced enterprise security management.\nCertification Overview About GH-500 GitHub Advanced Security Certification Name: GitHub Advanced Security Exam Code: GH-500 Duration: 150 minutes Question Count: ~75 questions Passing Score: 700/1000 (approximately 70%) Cost: $99 USD Validity: 3 years from certification date Prerequisites: GitHub Foundations (GH-900), GitHub Administration (GH-100), GitHub Actions (GH-200), and GitHub Copilot (GH-300) recommended Who Should Take This Exam Security engineers implementing DevSecOps practices DevSecOps specialists securing CI/CD pipelines Compliance officers ensuring regulatory adherence Security architects designing secure development workflows Platform engineering teams implementing security automation Technical leads responsible for application security Exam Domains Breakdown The GH-500 exam covers five main domains with specific weightings:\nDomain 1: Describe GHAS Security Features and Functionality (15%) Core Competencies:\nUnderstanding GitHub Advanced Security feature set Contrasting open source vs GHAS premium features Explaining Security Overview dashboard capabilities Identifying vulnerability detection mechanisms Describing SDLC integration points Key Skills:\nGHAS feature differentiation and licensing Security Overview navigation and interpretation Vulnerability lifecycle management Developer workflow integration Access control and permissions management Domain 2: Configure and Use Secret Scanning (15%) Core Competencies:\nConfiguring secret scanning for repositories Managing push protection policies Implementing custom secret patterns Handling secret scanning alerts Understanding validity checks and partner patterns Key Skills:\nSecret pattern recognition and configuration Push protection implementation Alert triage and remediation workflows Custom pattern development Third-party integrations and notifications Domain 3: Configure and Use Dependabot and Dependency Review (35%) Core Competencies:\nUnderstanding dependency graph functionality Configuring Dependabot alerts and security updates Implementing Dependency Review for pull requests Managing vulnerability remediation workflows Generating and interpreting SBOMs Key Skills:\nDependency vulnerability identification Automated security update configuration License compliance monitoring SBOM generation and analysis Vulnerability assessment and prioritisation Domain 4: Configure and Use Code Scanning with CodeQL (25%) Core Competencies:\nSetting up CodeQL code scanning workflows Understanding SARIF report formats Implementing third-party analysis tools Managing code scanning alerts and remediation Configuring advanced CodeQL queries Key Skills:\nCodeQL workflow configuration and optimisation SARIF result interpretation and management Custom query development Integration with third-party security tools Alert filtering and baseline management Domain 5: Describe GHAS Best Practices and Corrective Measures (10%) Core Competencies:\nImplementing security policy governance Establishing vulnerability management processes Configuring organisation-wide security policies Monitoring security metrics and compliance Training development teams on secure practices Key Skills:\nSecurity governance framework implementation Compliance reporting and audit trails Team training and awareness programs Security metrics analysis Continuous improvement processes Complete Study Plan Phase 1: Security Foundation Building (Weeks 1-2) Week 1: GHAS Overview and Setup Understand GitHub Advanced Security licensing and features Explore Security Overview dashboard capabilities Learn about vulnerability detection mechanisms Practice with GHAS feature activation Daily Tasks:\nExplore Security Overview for different repository types Compare open source vs GHAS premium features Practice enabling GHAS features across organisations Review security policies and access controls Hands-On Labs:\n# Enable GHAS features via GitHub API curl -X PATCH \\ -H \u0026#34;Authorization: token $GITHUB_TOKEN\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ https://api.github.com/repos/OWNER/REPO \\ -d \u0026#39;{ \u0026#34;security_and_analysis\u0026#34;: { \u0026#34;advanced_security\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;enabled\u0026#34;}, \u0026#34;secret_scanning\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;enabled\u0026#34;}, \u0026#34;secret_scanning_push_protection\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;enabled\u0026#34;} } }\u0026#39; Week 2: Security Dashboard Mastery Navigate Security Overview for enterprise insights Understand vulnerability aggregation and reporting Practice with security metrics and KPIs Learn about compliance and audit capabilities Practice Projects:\nSet up organisation-wide security monitoring Create security dashboards for different teams Implement security policy templates Configure notification and escalation workflows Phase 2: Secret and Dependency Security (Weeks 3-4) Week 3: Secret Scanning Implementation Configure secret scanning for various repository types Implement push protection across organisations Create custom secret patterns for proprietary systems Practice secret remediation workflows Advanced Configuration:\n# .github/secret_scanning.yml # Custom secret scanning configuration paths-ignore: - \u0026#34;docs/**\u0026#34; - \u0026#34;test/**/*.md\u0026#34; # Custom pattern example (organisation level) name: \u0026#34;Custom API Key Pattern\u0026#34; type: \u0026#34;custom\u0026#34; regex: \u0026#34;my-api-[a-zA-Z0-9]{32}\u0026#34; secret_type: \u0026#34;my_custom_api_key\u0026#34; Week 4: Dependency Security Mastery Master Dependabot configuration and automation Implement Dependency Review for pull request workflows Practice with SBOM generation and analysis Learn advanced vulnerability assessment techniques Dependency Configuration Labs:\n# .github/dependabot.yml version: 2 updates: - package-ecosystem: \u0026#34;npm\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; day: \u0026#34;monday\u0026#34; time: \u0026#34;09:00\u0026#34; assignees: - \u0026#34;security-team\u0026#34; reviewers: - \u0026#34;platform-team\u0026#34; commit-message: prefix: \u0026#34;security\u0026#34; prefix-development: \u0026#34;dev-deps\u0026#34; open-pull-requests-limit: 5 ignore: - dependency-name: \u0026#34;express\u0026#34; versions: [\u0026#34;4.x\u0026#34;, \u0026#34;5.x\u0026#34;] Phase 3: Code Scanning Excellence (Weeks 5-6) Week 5: CodeQL Implementation Set up CodeQL workflows for multiple languages Understand SARIF report structure and analysis Implement custom CodeQL queries Practice with baseline management and alert triage CodeQL Workflow Setup:\nname: \u0026#34;CodeQL Advanced Analysis\u0026#34; on: push: branches: [main, develop] pull_request: branches: [main] schedule: - cron: \u0026#39;0 2 * * 1\u0026#39; # Weekly Monday 2 AM jobs: analyze: name: Analyze runs-on: ubuntu-latest permissions: actions: read contents: read security-events: write strategy: fail-fast: false matrix: language: [\u0026#39;javascript\u0026#39;, \u0026#39;python\u0026#39;, \u0026#39;java\u0026#39;] steps: - name: Checkout repository uses: actions/checkout@v4 - name: Initialize CodeQL uses: github/codeql-action/init@v3 with: languages: ${{ matrix.language }} config-file: ./.github/codeql/codeql-config.yml - name: Autobuild uses: github/codeql-action/autobuild@v3 - name: Perform CodeQL Analysis uses: github/codeql-action/analyze@v3 with: category: \u0026#34;/language:${{matrix.language}}\u0026#34; Week 6: Third-Party Integration Integrate third-party security scanning tools Understand SARIF upload mechanisms Practice with multiple security tool orchestration Learn about security tool consolidation strategies Third-Party Integration Examples:\n# SARIF upload from third-party tools - name: Upload SARIF file uses: github/codeql-action/upload-sarif@v3 with: sarif_file: results.sarif category: \u0026#34;third-party-security-scanner\u0026#34; Phase 4: Enterprise Security Governance (Weeks 7-8) Week 7: Policy Implementation Configure organisation-wide security policies Implement branch protection with security requirements Set up compliance reporting and audit trails Practice with security policy enforcement Enterprise Policy Configuration:\n# Organisation security policy template # Branch protection with security requirements { \u0026#34;required_status_checks\u0026#34;: { \u0026#34;strict\u0026#34;: true, \u0026#34;contexts\u0026#34;: [ \u0026#34;CodeQL\u0026#34;, \u0026#34;Dependency Review\u0026#34;, \u0026#34;Secret Scanning\u0026#34; ] }, \u0026#34;enforce_admins\u0026#34;: true, \u0026#34;required_pull_request_reviews\u0026#34;: { \u0026#34;required_approving_review_count\u0026#34;: 2, \u0026#34;dismiss_stale_reviews\u0026#34;: true, \u0026#34;require_code_owner_reviews\u0026#34;: true }, \u0026#34;restrictions\u0026#34;: null, \u0026#34;allow_force_pushes\u0026#34;: false, \u0026#34;allow_deletions\u0026#34;: false } Week 8: Monitoring and Metrics Implement security metrics dashboards Set up alerting and notification systems Practice with compliance reporting Learn about continuous security improvement Security Metrics Tracking:\n# Security metrics collection script import requests import json from datetime import datetime, timedelta def get_security_metrics(org, token): headers = { \u0026#39;Authorization\u0026#39;: f\u0026#39;token {token}\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/vnd.github.v3+json\u0026#39; } # Get secret scanning alerts secret_alerts = requests.get( f\u0026#39;https://api.github.com/orgs/{org}/secret-scanning/alerts\u0026#39;, headers=headers ).json() # Get dependency alerts repos = requests.get( f\u0026#39;https://api.github.com/orgs/{org}/repos\u0026#39;, headers=headers ).json() total_repos = len(repos) enabled_repos = sum(1 for repo in repos if repo.get(\u0026#39;security_and_analysis\u0026#39;, {}) .get(\u0026#39;advanced_security\u0026#39;, {}) .get(\u0026#39;status\u0026#39;) == \u0026#39;enabled\u0026#39;) return { \u0026#39;total_repositories\u0026#39;: total_repos, \u0026#39;ghas_enabled_repositories\u0026#39;: enabled_repos, \u0026#39;secret_scanning_alerts\u0026#39;: len(secret_alerts), \u0026#39;coverage_percentage\u0026#39;: (enabled_repos / total_repos) * 100 } Hands-On Laboratory Exercises Lab 1: Enterprise Secret Scanning Implementation Implement comprehensive secret scanning across an organisation with custom patterns and remediation workflows.\nObjectives:\nConfigure organisation-wide secret scanning Implement custom secret patterns Set up automated remediation workflows Create security incident response procedures Implementation Steps:\nEnable Secret Scanning Organisation-Wide: #!/bin/bash # Enable secret scanning for all repositories in organisation ORG=\u0026#34;your-organisation\u0026#34; TOKEN=\u0026#34;your-github-token\u0026#34; # Get all repositories repos=$(curl -s -H \u0026#34;Authorization: token $TOKEN\u0026#34; \\ \u0026#34;https://api.github.com/orgs/$ORG/repos?per_page=100\u0026#34; | \\ jq -r \u0026#39;.[].name\u0026#39;) # Enable secret scanning for each repository for repo in $repos; do echo \u0026#34;Enabling secret scanning for $repo\u0026#34; curl -X PATCH \\ -H \u0026#34;Authorization: token $TOKEN\u0026#34; \\ -H \u0026#34;Accept: application/vnd.github.v3+json\u0026#34; \\ \u0026#34;https://api.github.com/repos/$ORG/$repo\u0026#34; \\ -d \u0026#39;{ \u0026#34;security_and_analysis\u0026#34;: { \u0026#34;secret_scanning\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;enabled\u0026#34;}, \u0026#34;secret_scanning_push_protection\u0026#34;: {\u0026#34;status\u0026#34;: \u0026#34;enabled\u0026#34;} } }\u0026#39; done Custom Secret Pattern Configuration: { \u0026#34;name\u0026#34;: \u0026#34;Internal API Key Pattern\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Detects internal API keys for proprietary systems\u0026#34;, \u0026#34;secret_type\u0026#34;: \u0026#34;internal_api_key\u0026#34;, \u0026#34;regex\u0026#34;: \u0026#34;(?i)internal[_-]?api[_-]?key[\\\u0026#34;\u0026#39;]?\\\\s*[:=]\\\\s*[\\\u0026#34;\u0026#39;]?([a-z0-9]{40})[\\\u0026#34;\u0026#39;]?\u0026#34;, \u0026#34;test_cases\u0026#34;: [ { \u0026#34;text\u0026#34;: \u0026#34;internal_api_key = \\\u0026#34;a1b2c3d4e5f6789012345678901234567890abcd\\\u0026#34;\u0026#34;, \u0026#34;should_match\u0026#34;: true } ] } Lab 2: Advanced Dependency Security Pipeline Create a comprehensive dependency security pipeline with automated remediation and compliance reporting.\nObjectives:\nImplement advanced Dependabot configuration Set up dependency review automation Create SBOM generation workflows Build compliance reporting dashboards Advanced Dependabot Configuration:\n# .github/dependabot.yml version: 2 registries: private-npm: type: npm-registry url: https://npm.internal.company.com token: ${{secrets.NPM_PRIVATE_TOKEN}} updates: # Production dependencies - package-ecosystem: \u0026#34;npm\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;daily\u0026#34; time: \u0026#34;02:00\u0026#34; assignees: - \u0026#34;security-team\u0026#34; - \u0026#34;platform-team\u0026#34; reviewers: - \u0026#34;security-lead\u0026#34; commit-message: prefix: \u0026#34;security(deps)\u0026#34; allow: - dependency-type: \u0026#34;direct\u0026#34; - dependency-type: \u0026#34;indirect\u0026#34; ignore: - dependency-name: \u0026#34;*\u0026#34; update-types: [\u0026#34;version-update:semver-major\u0026#34;] open-pull-requests-limit: 10 registries: - private-npm # Development dependencies - package-ecosystem: \u0026#34;npm\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; target-branch: \u0026#34;develop\u0026#34; allow: - dependency-type: \u0026#34;development\u0026#34; commit-message: prefix: \u0026#34;dev-deps\u0026#34; SBOM Generation Workflow:\nname: Generate SBOM and Security Report on: push: branches: [main] schedule: - cron: \u0026#39;0 6 * * *\u0026#39; # Daily at 6 AM jobs: generate-sbom: runs-on: ubuntu-latest permissions: contents: read security-events: write steps: - uses: actions/checkout@v4 - name: Generate SBOM uses: anchore/sbom-action@v0 with: path: ./ format: spdx-json - name: Upload SBOM uses: actions/upload-artifact@v4 with: name: sbom path: ./*.spdx.json - name: Dependency Review uses: actions/dependency-review-action@v4 with: fail-on-severity: high allow-licenses: MIT, Apache-2.0, BSD-3-Clause Lab 3: CodeQL Custom Query Development Develop custom CodeQL queries for organisation-specific security requirements.\nObjectives:\nCreate custom CodeQL queries for specific vulnerabilities Implement query testing and validation Set up custom query distribution Build security query governance Custom CodeQL Query Example:\n/** * @name Hardcoded API endpoints * @description Finds hardcoded API endpoints that might expose sensitive services * @kind problem * @problem.severity warning * @precision medium * @id custom/hardcoded-api-endpoints * @tags security * external/cwe/cwe-200 */ import javascript from StringLiteral str where str.getValue().regexpMatch(\u0026#34;https?://[a-zA-Z0-9.-]+\\\\.(internal|corp|local)/.*\u0026#34;) or str.getValue().regexpMatch(\u0026#34;https?://.*(api|service)\\\\.(internal|corp|local).*\u0026#34;) select str, \u0026#34;Hardcoded internal API endpoint found: \u0026#34; + str.getValue() CodeQL Configuration File:\n# .github/codeql/codeql-config.yml name: \u0026#34;Custom CodeQL Configuration\u0026#34; queries: - uses: security-extended - uses: ./.github/codeql/custom-queries/ paths-ignore: - \u0026#34;test/**\u0026#34; - \u0026#34;docs/**\u0026#34; - \u0026#34;**/*.md\u0026#34; paths: - \u0026#34;src/**\u0026#34; - \u0026#34;lib/**\u0026#34; Enterprise Security Best Practices Security Policy Framework 1. Repository Security Standards:\n# Security policy template security_requirements: code_scanning: required: true tools: [\u0026#34;CodeQL\u0026#34;, \u0026#34;SonarCloud\u0026#34;] frequency: \u0026#34;on_push\u0026#34; secret_scanning: enabled: true push_protection: true custom_patterns: true dependency_scanning: dependabot_enabled: true security_updates: true vulnerability_threshold: \u0026#34;high\u0026#34; branch_protection: required_reviews: 2 dismiss_stale_reviews: true require_code_owner_reviews: true required_status_checks: - \u0026#34;CodeQL\u0026#34; - \u0026#34;Dependency Review\u0026#34; 2. Incident Response Procedures:\n# Security incident response workflow incident_response: critical_vulnerabilities: response_time: \u0026#34;2 hours\u0026#34; escalation: [\u0026#34;security-team\u0026#34;, \u0026#34;platform-team\u0026#34;] actions: - immediate_assessment - impact_analysis - remediation_planning - stakeholder_notification secret_exposure: response_time: \u0026#34;30 minutes\u0026#34; actions: - revoke_exposed_secrets - assess_potential_impact - implement_new_credentials - audit_access_logs Compliance and Audit Framework Security Metrics Dashboard:\n# Security compliance reporting class SecurityComplianceReporter: def __init__(self, github_client): self.client = github_client def generate_compliance_report(self, org): report = { \u0026#39;timestamp\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;organisation\u0026#39;: org, \u0026#39;metrics\u0026#39;: {} } # GHAS adoption metrics repos = self.client.get_org_repos(org) total_repos = len(repos) ghas_enabled = sum(1 for r in repos if r.advanced_security_enabled) report[\u0026#39;metrics\u0026#39;][\u0026#39;ghas_adoption\u0026#39;] = { \u0026#39;total_repositories\u0026#39;: total_repos, \u0026#39;ghas_enabled\u0026#39;: ghas_enabled, \u0026#39;adoption_percentage\u0026#39;: (ghas_enabled / total_repos) * 100 } # Vulnerability metrics vulnerabilities = self.get_vulnerability_summary(org) report[\u0026#39;metrics\u0026#39;][\u0026#39;vulnerabilities\u0026#39;] = vulnerabilities return report Exam Tips and Strategies Technical Preparation Focus Areas Secret Scanning Mastery:\nCustom pattern development and testing Push protection configuration and bypasses Alert triage and remediation workflows Third-party integration and notifications Compliance reporting and audit trails Dependency Security Expertise:\nDependabot configuration optimisation License compliance monitoring SBOM generation and analysis Vulnerability assessment and prioritisation Automated remediation workflows Code Scanning Proficiency:\nCodeQL workflow optimisation Custom query development and testing SARIF report analysis and management Third-party tool integration Baseline management and alert filtering Exam Day Strategy Time Management:\nAllocate 2 minutes per question average Focus on scenarios requiring practical knowledge Use flag feature for complex questions Reserve time for final review Question Approach:\nIdentify the security domain being tested Consider enterprise vs repository-level implications Evaluate compliance and governance aspects Apply hands-on configuration knowledge Select the most secure and scalable solution Common Exam Topics:\nConfiguration file syntax and structure API integration and automation Workflow integration and dependencies Enterprise policy implementation Compliance and reporting requirements Official Study Resources GitHub Documentation GitHub Advanced Security Documentation Secret Scanning Guide Dependabot Documentation Code Scanning with CodeQL Microsoft Learn Paths Secure your repository\u0026rsquo;s supply chain Configure Dependabot security updates Secure code with CodeQL Practice Platforms GitHub Security Lab CodeQL Query Console GitHub Skills - Secure Code Game Additional Practice Resources Security Communities GitHub Security Advisories CodeQL Community Queries Security Best Practices Hands-On Practice Set up GHAS in personal/test organisations Contribute to open-source security projects Practice with vulnerable code repositories Build custom security automation workflows Final Preparation Checklist Technical Skills Validation Can configure GHAS features across organisations Understand secret scanning patterns and remediation Master Dependabot configuration and automation Know CodeQL workflow setup and customisation Can create custom CodeQL queries Understand SARIF format and third-party integrations Know enterprise security policy implementation Can set up compliance monitoring and reporting Understand vulnerability lifecycle management Know security incident response procedures Can implement security automation workflows Understand licensing and cost implications Exam Readiness Assessment Completed all study plan phases Finished hands-on security laboratories Practiced with enterprise scenarios Reviewed official documentation thoroughly Taken practice assessments consistently Confident in all security domains Registered for exam date Prepared exam day logistics Next Steps in Your Journey After achieving GH-500 certification, consider these advanced paths:\nAdvanced Certifications GitHub Administration (GH-100): Enterprise administration and governance GitHub Enterprise Consulting: Partner program certification Security Specialisation: Advanced security consulting roles Career Advancement Security Architect: Design enterprise security frameworks DevSecOps Lead: Implement organisation-wide security practices Compliance Manager: Ensure regulatory and audit compliance Security Consultant: Provide GitHub security expertise to enterprises Conclusion The GH-500 GitHub Advanced Security certification validates your expertise in implementing enterprise-grade security practices and positions you as a crucial contributor to any organisation\u0026rsquo;s security posture. By following this comprehensive study guide, completing the hands-on security laboratories, and practising with real-world scenarios, you\u0026rsquo;ll be well-prepared to pass the exam and excel in your security career.\nRemember that security is an evolving landscape, so continue learning about new threats, vulnerabilities, and security practices. The skills you develop preparing for this certification will serve you throughout your career in cybersecurity and DevSecOps.\nReady to secure everything? üîí\nStart your GitHub Advanced Security mastery journey today, and join the ranks of certified security professionals who are protecting organisations and their software supply chains worldwide.\nThis guide is part of the GitHub Certification Journey series. Previous: GitHub Actions (GH-200) | Next: GitHub Administration (GH-100) - Coming Soon\nHave questions about GitHub Advanced Security certification? Connect with me on LinkedIn or GitHub for guidance and support.\n","date":"2025-10-05T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/10/github-certification-journey-gh-500/cover_hu_c593f8ef7dfe8d52.jpg","image":"https://quintelier.dev/posts/2025/10/github-certification-journey-gh-500/cover_hu_a3a7ecd8e7464434.jpg","permalink":"https://quintelier.dev/posts/2025/10/github-certification-journey-gh-500/","title":"üîí GitHub Advanced Security Certification Guide (GH-500) - Complete Prep","webpImage":"https://quintelier.dev/posts/2025/10/github-certification-journey-gh-500/cover_hu_aacb9dad958ffda.webp"},{"content":"Part 3: Common Pitfalls \u0026amp; Best Practices By now, you know how async/await works and how to run tasks sequentially or in parallel. But async programming in C# has some traps that can lead to deadlocks, unhandled exceptions, or subtle bugs.\nLet‚Äôs go through the most common pitfalls and how to avoid them.\nPitfall 1: async void public async void DoWork() { await Task.Delay(1000); throw new Exception(\u0026#34;Boom!\u0026#34;); } Why it‚Äôs bad:\nYou can‚Äôt await it ‚Üí no way to know when it‚Äôs done. Exceptions go directly to the synchronization context (e.g., crash the app). ‚úÖ Best practice:\nUse async Task instead. Only use async void for event handlers (where the signature is fixed). public async Task DoWorkAsync() { await Task.Delay(1000); throw new Exception(\u0026#34;Handled safely!\u0026#34;); } Pitfall 2: Blocking Async Code Mixing sync and async is dangerous:\n// BAD var result = GetDataAsync().Result; // BAD GetDataAsync().Wait(); This can cause a deadlock in UI apps (WinForms/WPF/ASP.NET classic) because:\nThe calling thread is blocked. The async method tries to resume on that same thread ‚Üí stuck forever. ‚úÖ Best practice:\nAlways await async methods. If you really must call async code synchronously, use .GetAwaiter().GetResult() in console apps ‚Äî but avoid it in production. Pitfall 3: Context Capture \u0026amp; ConfigureAwait(false) By default, await captures the synchronization context (e.g., UI thread) to resume on it:\nawait Task.Delay(1000); // Resumes on the original context (UI thread in WPF/WinForms) This is useful in UI apps but unnecessary in library or backend code. It can also hurt performance.\n‚úÖ Best practice:\nUse ConfigureAwait(false) in libraries or background services:\nawait Task.Delay(1000).ConfigureAwait(false); // Resumes on a thread pool thread, not necessarily the UI thread Pitfall 4: Exception Handling in Async Code Async exceptions propagate naturally, but you need to await to catch them:\ntry { await DoWorkAsync(); } catch (Exception ex) { Console.WriteLine($\u0026#34;Caught: {ex.Message}\u0026#34;); } But what if you run multiple tasks?\nvar task1 = DoWorkAsync(); var task2 = DoWorkAsync(); await Task.WhenAll(task1, task2); If both throw exceptions, Task.WhenAll aggregates them into an AggregateException.\n‚úÖ Best practice:\nAlways wrap await in try/catch if you expect exceptions. Inspect task.Exception.InnerExceptions when using Task.WhenAll. Pitfall 5: Fire-and-Forget Tasks Sometimes devs write:\nDoWorkAsync(); // No await Console.WriteLine(\u0026#34;Moving on...\u0026#34;); If DoWorkAsync fails, you‚Äôll never know ‚Äî the exception is lost.\n‚úÖ Best practice:\nOnly fire-and-forget if you really don‚Äôt care about the result. If you must, handle exceptions explicitly: _ = Task.Run(async () =\u0026gt; { try { await DoWorkAsync(); } catch (Exception ex) { Console.WriteLine($\u0026#34;Fire-and-forget error: {ex.Message}\u0026#34;); } }); Key Takeaways Avoid async void except for event handlers. Don‚Äôt block async code with .Result or .Wait(). Use ConfigureAwait(false) in libraries and services. Always handle exceptions with try/catch or aggregate checks. Be cautious with fire-and-forget tasks. üëâ In Part 4, we‚Äôll cover async patterns:\nRunning multiple tasks in parallel (Task.WhenAll, Task.WhenAny) Cancelling tasks with CancellationToken Async streams (IAsyncEnumerable\u0026lt;T\u0026gt;, await foreach) Timeouts and retries Series Navigation Previous: Part 2 ‚Äì Deep Dive Series Index: Overview Next: Part 4 ‚Äì Patterns\n","date":"2025-10-01T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part3/cover_hu_6816a91c316a13cd.jpg","image":"https://quintelier.dev/posts/2025/10/csharp-async-await-part3/cover_hu_41898da7d42f4cfc.jpg","permalink":"https://quintelier.dev/posts/2025/10/csharp-async-await-part3/","title":"Mastering Asynchronous Programming with C# async/await - Part 3: Common Pitfalls and Best Practices","webpImage":"https://quintelier.dev/posts/2025/10/csharp-async-await-part3/cover_hu_9ecaa2f5455e1ced.webp"},{"content":"The Challenge: The 90-Day Dependabot Pause If you\u0026rsquo;re managing multiple repositories in a GitHub organization, you\u0026rsquo;ve likely encountered a frustrating limitation: Dependabot automatically pauses after 90 days of inactivity. When there are no merged pull requests or manual triggers, Dependabot simply stops creating new dependency updates. For organizations with dozens or hundreds of repositories, this can lead to a security and maintenance nightmare.\nThe problem compounds quickly:\nSome repositories might be stable but still need security updates Manual intervention across many repos is time-consuming and error-prone By the time you notice Dependabot has paused, you\u0026rsquo;re already behind on updates Re-enabling requires manual UI interaction for each repository The Solution: A Two-Pronged Automation Strategy This comprehensive GitHub Actions workflow system tackles this problem through both proactive prevention and early detection. Let me walk you through how it works.\nArchitecture Overview The solution consists of three interconnected workflows:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ GitHub Organization ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§ ‚îÇ Inactivity Report ‚îÇ Keep-Alive (Scheduled) ‚îÇ ‚îÇ (Daily Monitor) ‚îÇ (Quarterly Maintenance) ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚Ä¢ Scans all repos ‚îÇ ‚Ä¢ Nudges open PRs ‚îÇ ‚îÇ ‚Ä¢ Detects ‚â•75d inactivity ‚îÇ ‚Ä¢ Creates reminder issues ‚îÇ ‚îÇ ‚Ä¢ Reports to central repo ‚îÇ ‚Ä¢ Prevents 90d pause ‚îÇ ‚îÇ ‚Ä¢ Early warning system ‚îÇ ‚Ä¢ Distributed intervention ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò Workflow Summary Workflow Filename Purpose Schedule Scope Dependabot Inactivity Report dependabot-inactivity-report.yml Monitor \u0026amp; alert on inactive repos Daily at 1 AM UTC Organization-wide (read-only) Dependabot Keep-Alive dependabot-keep-alive.yml Reusable template for keeping Dependabot active Called by other workflows Single repo or org-wide Dependabot Keep-Alive Scheduled dependabot-keep-alive-scheduled.yml Scheduled caller for keep-alive operations Quarterly (every 3 months) Organization-wide (write) Workflow #1: Dependabot Inactivity Report (Monitoring \u0026amp; Alerting) Purpose The Dependabot Inactivity Report workflow acts as your early warning system, identifying repositories approaching the 90-day inactivity threshold before Dependabot pauses.\nKey Features üîç Smart Discovery\n# Uses GitHub Search API to find Dependabot-enabled repos search_query=\u0026#34;org:${ORG}+path:.github+filename:dependabot.yml\u0026#34; Instead of iterating through all repositories, it uses GitHub\u0026rsquo;s Search API to efficiently find only repositories with Dependabot configurations.\n‚è∞ Configurable Threshold\ncutoff-days: 75 # Alert 15 days before the 90-day pause By default, it alerts when repositories hit 75 days of inactivity, giving you a 15-day buffer to take action.\nüéØ Flexible Filtering\nrepo-includes: \u0026#39;frontend-*,backend-*\u0026#39; # Include specific patterns repo-excludes: \u0026#39;*-archive,*-deprecated\u0026#39; # Exclude patterns Supports wildcard patterns to focus on repositories that matter and ignore archived or deprecated projects.\nüìä Centralized Reporting\nCreates a single issue in your workflow repository with all inactive repositories:\nDependabot inactivity report (‚â•75 days) ‚Äî @your-team The following repositories are nearing the 90-day Dependabot pause threshold: org/repo1 - last activity: 2025-06-15T10:30:00Z (77d ago) org/repo2 - last activity: 2025-06-10T14:20:00Z (82d ago) org/repo3 - no Dependabot activity found üõ°Ô∏è Robust Error Handling\nGracefully handles:\n404 errors for repositories without Actions enabled Access permission issues Missing Dependabot workflows API rate limits When It Runs Daily at 1 AM UTC via scheduled cron On-demand via workflow_dispatch for testing Dry-run mode available for validation Complete Workflow File Here\u0026rsquo;s the complete dependabot-inactivity-report.yml workflow:\nname: Dependabot Inactivity Report on: schedule: - cron: \u0026#34;0 1 * * *\u0026#34; workflow_dispatch: inputs: org: description: \u0026#39;Organization name\u0026#39; required: false default: \u0026#39;your-org\u0026#39; runner: description: \u0026#39;JSON array of runner labels\u0026#39; required: false default: \u0026#39;[\u0026#34;ubuntu-latest\u0026#34;]\u0026#39; dry-run: description: \u0026#39;If true, only writes a job summary (no issue).\u0026#39; required: false type: boolean default: true cutoff-days: description: \u0026#39;Cutoff days for inactivity\u0026#39; required: false type: number default: 75 team-handle: description: \u0026#39;team handle to mention in the issue\u0026#39; required: false default: \u0026#39;@your-org/maintainers\u0026#39; repo-includes: description: \u0026#39;Comma-separated list of repo name patterns to include (supports wildcards)\u0026#39; required: false default: \u0026#39;\u0026#39; repo-excludes: description: \u0026#39;Comma-separated list of repo name patterns to exclude (supports wildcards)\u0026#39; required: false default: \u0026#39;\u0026#39; permissions: contents: read issues: write env: ORG: ${{ github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs.org || \u0026#39;your-org\u0026#39; }} TEAM_HANDLE: ${{ github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs[\u0026#39;team-handle\u0026#39;] || \u0026#39;@your-org/maintainers\u0026#39; }} CUTOFF_DAYS: ${{ github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs[\u0026#39;cutoff-days\u0026#39;] || 75 }} REPO_INCLUDES: ${{ github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs[\u0026#39;repo-includes\u0026#39;] || \u0026#39;\u0026#39; }} REPO_EXCLUDES: ${{ github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs[\u0026#39;repo-excludes\u0026#39;] || \u0026#39;\u0026#39; }} jobs: scan: strategy: matrix: runner: ${{ fromJSON(github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs.runner || \u0026#39;[\u0026#34;ubuntu-latest\u0026#34;]\u0026#39;) }} runs-on: ${{ matrix.runner }} env: GH_TOKEN: ${{ secrets.ORG_READ_TOKEN }} steps: - name: Scan org repos for inactivity id: scan shell: bash run: | set -euo pipefail echo \u0026#34;üîç Scanning organization: $ORG (Dependabot-enabled repos only)\u0026#34; echo \u0026#34;‚è∞ Cutoff days: $CUTOFF_DAYS\u0026#34; if [[ -n \u0026#34;$REPO_INCLUDES\u0026#34; ]]; then echo \u0026#34;‚úÖ Including repos matching: $REPO_INCLUDES\u0026#34; fi if [[ -n \u0026#34;$REPO_EXCLUDES\u0026#34; ]]; then echo \u0026#34;‚ùå Excluding repos matching: $REPO_EXCLUDES\u0026#34; fi # Helper functions matches_pattern() { local repo=\u0026#34;$1\u0026#34; patterns=\u0026#34;$2\u0026#34; [[ -z \u0026#34;$patterns\u0026#34; ]] \u0026amp;\u0026amp; return 0 IFS=\u0026#39;,\u0026#39; read -ra PATTERN_ARRAY \u0026lt;\u0026lt;\u0026lt; \u0026#34;$patterns\u0026#34; for pattern in \u0026#34;${PATTERN_ARRAY[@]}\u0026#34;; do pattern=$(echo \u0026#34;$pattern\u0026#34; | xargs) [[ -z \u0026#34;$pattern\u0026#34; ]] \u0026amp;\u0026amp; continue [[ \u0026#34;$repo\u0026#34; == $pattern ]] \u0026amp;\u0026amp; return 0 done return 1 } should_include_repo() { local repo=\u0026#34;$1\u0026#34; if [[ -n \u0026#34;$REPO_INCLUDES\u0026#34; ]] \u0026amp;\u0026amp; ! matches_pattern \u0026#34;$repo\u0026#34; \u0026#34;$REPO_INCLUDES\u0026#34;; then return 1 fi if [[ -n \u0026#34;$REPO_EXCLUDES\u0026#34; ]] \u0026amp;\u0026amp; matches_pattern \u0026#34;$repo\u0026#34; \u0026#34;$REPO_EXCLUDES\u0026#34;; then return 1 fi return 0 } # Find repos with Dependabot using GitHub Search API search_query=\u0026#34;org:${ORG}+path:.github+filename:dependabot.yml\u0026#34; repos_with_dependabot=\u0026#34;$(gh api \u0026#34;search/code?q=${search_query}\u0026#34; --jq \u0026#39;.items[].repository.full_name\u0026#39; | sort -u || true)\u0026#34; if [[ -z \u0026#34;$repos_with_dependabot\u0026#34; ]]; then echo \u0026#34;‚ö†Ô∏è No repositories found with Dependabot configuration\u0026#34; exit 0 fi repos=\u0026#34;$(echo \u0026#34;$repos_with_dependabot\u0026#34; | sed \u0026#34;s|^${ORG}/||\u0026#34;)\u0026#34; repo_count=$(echo \u0026#34;$repos\u0026#34; | grep -c . || echo \u0026#34;0\u0026#34;) echo \u0026#34;üìä Found $repo_count repositories with Dependabot\u0026#34; now=\u0026#34;$(date -u +%s)\u0026#34; cutoff=$(( CUTOFF_DAYS * 86400 )) warn_list=\u0026#34;\u0026#34; while IFS= read -r repo; do [[ -z \u0026#34;$repo\u0026#34; ]] \u0026amp;\u0026amp; continue should_include_repo \u0026#34;$repo\u0026#34; || { echo \u0026#34;‚è≠Ô∏è Skipping $repo (filtered)\u0026#34;; continue; } full=\u0026#34;$ORG/$repo\u0026#34; # Get last Dependabot PR activity last_pr_update=\u0026#34;$(gh pr list -R \u0026#34;$full\u0026#34; --author dependabot[bot] --state all --json updatedAt -q \u0026#39;.[].updatedAt\u0026#39; || true)\u0026#34; # Get latest \u0026#34;Dependabot Updates\u0026#34; workflow run wf_runs=\u0026#34;$(gh api -q \u0026#39;.workflow_runs[]?|select(.name==\u0026#34;Dependabot Updates\u0026#34;)|.updated_at\u0026#39; \u0026#34;/repos/$full/actions/runs?per_page=100\u0026#34; 2\u0026gt;/dev/null || true)\u0026#34; latest=\u0026#34;$(printf \u0026#34;%s\\n%s\\n\u0026#34; \u0026#34;$last_pr_update\u0026#34; \u0026#34;$wf_runs\u0026#34; | grep -E \u0026#39;.+\u0026#39; | sort -r | head -n1 || true)\u0026#34; if [[ -z \u0026#34;$latest\u0026#34; ]]; then warn_list+=\u0026#34;$full - no Dependabot activity found\u0026#34;$\u0026#39;\\n\u0026#39; continue fi latest_s=\u0026#34;$(date -u -d \u0026#34;$latest\u0026#34; +%s 2\u0026gt;/dev/null || echo 0)\u0026#34; age=$(( now - latest_s )) if [[ \u0026#34;$latest_s\u0026#34; -eq 0 || \u0026#34;$age\u0026#34; -ge \u0026#34;$cutoff\u0026#34; ]]; then days=$(( age / 86400 )) warn_list+=\u0026#34;$full - last activity: $latest (${days}d ago)\u0026#34;$\u0026#39;\\n\u0026#39; fi done \u0026lt;\u0026lt;\u0026lt; \u0026#34;$repos\u0026#34; { echo \u0026#34;warn_list\u0026lt;\u0026lt;EOF\u0026#34; printf \u0026#34;%s\u0026#34; \u0026#34;$warn_list\u0026#34; echo \u0026#34;EOF\u0026#34; } \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; - name: Dry-run - job summary if: ${{ github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs[\u0026#39;dry-run\u0026#39;] == true }} shell: bash run: | { printf \u0026#34;### Dependabot inactivity report (‚â•%s days) ‚Äî %s\\n\\n\u0026#34; \u0026#34;$CUTOFF_DAYS\u0026#34; \u0026#34;$TEAM_HANDLE\u0026#34; if [ -z \u0026#34;${{ steps.scan.outputs.warn_list }}\u0026#34; ]; then echo \u0026#34;No repos nearing the threshold üéâ\u0026#34; else echo \u0026#34;The following repos may pause soon:\u0026#34; echo \u0026#39;```\u0026#39; printf \u0026#34;%s\\n\u0026#34; \u0026#34;${{ steps.scan.outputs.warn_list }}\u0026#34; echo \u0026#39;```\u0026#39; fi } \u0026gt;\u0026gt; \u0026#34;$GITHUB_STEP_SUMMARY\u0026#34; - name: Post issue (real mode) if: ${{ github.event_name != \u0026#39;workflow_dispatch\u0026#39; || inputs[\u0026#39;dry-run\u0026#39;] != true }} env: GH_TOKEN: ${{ secrets.GITHUB_TOKEN }} shell: bash run: | if [ -n \u0026#34;${{ steps.scan.outputs.warn_list }}\u0026#34; ]; then title=\u0026#34;Dependabot inactivity report (‚â•${CUTOFF_DAYS} days) ‚Äî ${TEAM_HANDLE}\u0026#34; body=\u0026#34;${TEAM_HANDLE} The following repositories are nearing the ${CUTOFF_DAYS}-day Dependabot pause threshold: ${{ steps.scan.outputs.warn_list }}\u0026#34; existing=\u0026#34;$(gh issue list -R \u0026#34;$GITHUB_REPOSITORY\u0026#34; --search \u0026#34;$title in:title\u0026#34; --json number,title -q \u0026#39;.[] | select(.title==\u0026#34;\u0026#39;\u0026#34;$title\u0026#34;\u0026#39;\u0026#34;) | .number\u0026#39; | head -n1 || true)\u0026#34; if [ -n \u0026#34;${existing:-}\u0026#34; ]; then gh issue comment -R \u0026#34;$GITHUB_REPOSITORY\u0026#34; \u0026#34;$existing\u0026#34; --body \u0026#34;$body\u0026#34; else gh issue create -R \u0026#34;$GITHUB_REPOSITORY\u0026#34; --title \u0026#34;$title\u0026#34; --body \u0026#34;$body\u0026#34; --label \u0026#34;dependencies\u0026#34; fi fi Workflow #2: Dependabot Keep-Alive (Reusable Template) Purpose This workflow actively prevents Dependabot from pausing by creating activity in repositories before they reach the 90-day threshold.\nHow It Works Dual Operation Modes\nSingle Repository Mode: Processes only the current repository Organization Mode: Scans and processes all Dependabot-enabled repos Intelligent PR Nudging\nWhen it finds an open Dependabot PR:\ngh pr comment -R \u0026#34;$repo\u0026#34; \u0026#34;$pr_number\u0026#34; --body \u0026#34;@dependabot rebase\u0026#34; This triggers Dependabot to rebase the PR, which counts as activity and resets the inactivity timer.\nReminder Issue Creation\nFor repositories without open PRs:\n@your-team No open Dependabot PRs found in org/repo. Action: Press **Insights ‚Üí Dependency graph ‚Üí Dependabot ‚Üí Recent update jobs ‚Üí Check for updates** in the UI. Creates actionable issues that prompt manual intervention while still registering activity.\nSmart Duplicate Prevention\n# Searches for existing issues created by GitHub Actions existing=\u0026#34;$(gh issue list -R \u0026#34;$repo\u0026#34; \\ --search \u0026#34;Dependabot keep-alive in:title is:open author:app/github-actions\u0026#34; \\ --json number -q \u0026#39;.[0].number\u0026#39;)\u0026#34; Updates existing issues instead of creating duplicates, keeping repositories clean.\nGraceful Label Handling\n# Check if \u0026#39;dependencies\u0026#39; label exists before using it if gh api repos/$repo/labels/dependencies --silent \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then # Create with label else # Create without label (no failure) fi Works seamlessly across repositories with different label configurations.\nKey Technical Improvements Fixed Exit Code Issues\nThe ((var++)) syntax returns exit code 1 when incrementing to 1 with bash\u0026rsquo;s set -e:\n# ‚ùå Causes script failure ((total_issues++)) # ‚úÖ Works correctly total_issues=$((total_issues + 1)) Smart Token Selection\nGH_TOKEN: ${{ inputs.scan-mode == \u0026#39;org\u0026#39; \u0026amp;\u0026amp; secrets.ORG_READ_TOKEN || secrets.GITHUB_TOKEN }} Automatically uses the appropriate token based on operation scope.\nComplete Workflow File Here\u0026rsquo;s the complete dependabot-keep-alive.yml reusable workflow:\nClick to expand full workflow (190 lines) name: Dependabot Keep-Alive on: workflow_call: inputs: runs-on: description: \u0026#39;Runner labels as JSON array\u0026#39; required: false type: string default: \u0026#39;[\u0026#34;ubuntu-latest\u0026#34;]\u0026#39; team-handle: description: \u0026#39;Optional team mention\u0026#39; required: false type: string default: \u0026#39;@your-org/maintainers\u0026#39; org: description: \u0026#39;Organization to scan (if different from current repo owner)\u0026#39; required: false type: string default: \u0026#39;\u0026#39; scan-mode: description: \u0026#39;Mode: \u0026#34;single\u0026#34; (current repo only) or \u0026#34;org\u0026#34; (all org repos with dependabot)\u0026#39; required: false type: string default: \u0026#39;single\u0026#39; repo-includes: description: \u0026#39;Comma-separated repo patterns to include (org mode only)\u0026#39; required: false type: string default: \u0026#39;\u0026#39; repo-excludes: description: \u0026#39;Comma-separated repo patterns to exclude (org mode only)\u0026#39; required: false type: string default: \u0026#39;\u0026#39; permissions: contents: read pull-requests: write issues: write jobs: keepalive: runs-on: ${{ fromJSON(inputs.runs-on) }} steps: - name: Discover repositories id: discover env: GH_TOKEN: ${{ inputs.scan-mode == \u0026#39;org\u0026#39; \u0026amp;\u0026amp; secrets.ORG_READ_TOKEN || secrets.GITHUB_TOKEN }} SCAN_MODE: ${{ inputs.scan-mode }} ORG: ${{ inputs.org || github.repository_owner }} REPO_INCLUDES: ${{ inputs.repo-includes }} REPO_EXCLUDES: ${{ inputs.repo-excludes }} shell: bash run: | set -eo pipefail matches_pattern() { local repo=\u0026#34;$1\u0026#34; patterns=\u0026#34;$2\u0026#34; [[ -z \u0026#34;$patterns\u0026#34; ]] \u0026amp;\u0026amp; return 0 IFS=\u0026#39;,\u0026#39; read -ra PATTERN_ARRAY \u0026lt;\u0026lt;\u0026lt; \u0026#34;$patterns\u0026#34; for pattern in \u0026#34;${PATTERN_ARRAY[@]}\u0026#34;; do pattern=$(echo \u0026#34;$pattern\u0026#34; | xargs) [[ -z \u0026#34;$pattern\u0026#34; ]] \u0026amp;\u0026amp; continue [[ \u0026#34;$repo\u0026#34; == $pattern ]] \u0026amp;\u0026amp; return 0 done return 1 } should_include_repo() { local repo=\u0026#34;$1\u0026#34; if [[ -n \u0026#34;$REPO_INCLUDES\u0026#34; ]] \u0026amp;\u0026amp; ! matches_pattern \u0026#34;$repo\u0026#34; \u0026#34;$REPO_INCLUDES\u0026#34;; then return 1 fi if [[ -n \u0026#34;$REPO_EXCLUDES\u0026#34; ]] \u0026amp;\u0026amp; matches_pattern \u0026#34;$repo\u0026#34; \u0026#34;$REPO_EXCLUDES\u0026#34;; then return 1 fi return 0 } if [[ \u0026#34;$SCAN_MODE\u0026#34; == \u0026#34;single\u0026#34; ]]; then echo \u0026#34;üéØ Single repository mode: $GITHUB_REPOSITORY\u0026#34; echo \u0026#34;repos=$GITHUB_REPOSITORY\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; else echo \u0026#34;üîç Organization mode: Scanning $ORG for Dependabot-enabled repositories\u0026#34; search_query=\u0026#34;org:${ORG}+path:.github+filename:dependabot.yml\u0026#34; repos_with_dependabot=\u0026#34;$(gh api \u0026#34;search/code?q=${search_query}\u0026#34; --jq \u0026#39;.items[].repository.full_name\u0026#39; | sort -u || true)\u0026#34; if [[ -z \u0026#34;$repos_with_dependabot\u0026#34; ]]; then echo \u0026#34;‚ö†Ô∏è No repositories found with Dependabot configuration\u0026#34; echo \u0026#34;repos=\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; exit 0 fi filtered_repos=\u0026#34;\u0026#34; while IFS= read -r full_repo; do [[ -z \u0026#34;$full_repo\u0026#34; ]] \u0026amp;\u0026amp; continue repo_name=\u0026#34;${full_repo#*/}\u0026#34; if should_include_repo \u0026#34;$repo_name\u0026#34;; then filtered_repos+=\u0026#34;$full_repo \u0026#34; else echo \u0026#34;‚è≠Ô∏è Skipping $full_repo (filtered out)\u0026#34; fi done \u0026lt;\u0026lt;\u0026lt; \u0026#34;$repos_with_dependabot\u0026#34; repo_count=$(echo \u0026#34;$filtered_repos\u0026#34; | wc -w) echo \u0026#34;üìä Found $repo_count repositories to process\u0026#34; echo \u0026#34;repos=$filtered_repos\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; fi - name: Process repositories env: GH_TOKEN: ${{ inputs.scan-mode == \u0026#39;org\u0026#39; \u0026amp;\u0026amp; secrets.ORG_READ_TOKEN || secrets.GITHUB_TOKEN }} REPOS: ${{ steps.discover.outputs.repos }} shell: bash run: | set -eo pipefail [[ -z \u0026#34;$REPOS\u0026#34; ]] \u0026amp;\u0026amp; { echo \u0026#34;No repositories to process\u0026#34;; exit 0; } total_prs=0 total_issues=0 for repo in $REPOS; do echo \u0026#34;üîÑ Processing $repo\u0026#34; pr_number=\u0026#34;$(gh pr list -R \u0026#34;$repo\u0026#34; --author \u0026#34;dependabot[bot]\u0026#34; --state open --json number -q \u0026#39;.[0].number\u0026#39; || true)\u0026#34; if [ -n \u0026#34;${pr_number:-}\u0026#34; ]; then echo \u0026#34; üìå Found PR #$pr_number - nudging with rebase\u0026#34; gh pr comment -R \u0026#34;$repo\u0026#34; \u0026#34;$pr_number\u0026#34; --body \u0026#34;@dependabot rebase\u0026#34; || echo \u0026#34; ‚ö†Ô∏è Failed to comment on PR\u0026#34; total_prs=$((total_prs + 1)) else echo \u0026#34; üìù No open PRs - creating reminder issue\u0026#34; mention=\u0026#34;${{ inputs.team-handle }}\u0026#34; body=\u0026#34;$mention No open Dependabot PRs found in $repo. Action: Press **Insights ‚Üí Dependency graph ‚Üí Dependabot ‚Üí Recent update jobs ‚Üí Check for updates** in the UI.\u0026#34; existing=\u0026#34;$(gh issue list -R \u0026#34;$repo\u0026#34; --search \u0026#34;Dependabot keep-alive in:title is:open author:app/github-actions\u0026#34; --json number -q \u0026#39;.[0].number\u0026#39; 2\u0026gt;/dev/null || true)\u0026#34; if [ -n \u0026#34;${existing:-}\u0026#34; ]; then if gh issue comment -R \u0026#34;$repo\u0026#34; \u0026#34;$existing\u0026#34; --body \u0026#34;$body\u0026#34; 2\u0026gt;/dev/null; then echo \u0026#34; üí¨ Updated existing issue #$existing\u0026#34; else echo \u0026#34; ‚ö†Ô∏è Failed to comment on issue #$existing\u0026#34; fi else if gh api repos/$repo/labels/dependencies --silent \u0026gt;/dev/null 2\u0026gt;\u0026amp;1; then echo \u0026#34; üè∑Ô∏è Creating issue with \u0026#39;dependencies\u0026#39; label\u0026#34; issue_url=$(gh issue create -R \u0026#34;$repo\u0026#34; --title \u0026#34;Dependabot keep-alive\u0026#34; --body \u0026#34;$body\u0026#34; --label \u0026#34;dependencies\u0026#34; 2\u0026gt;\u0026amp;1) \u0026amp;\u0026amp; echo \u0026#34;$issue_url\u0026#34; || { echo \u0026#34; ‚ö†Ô∏è Failed with label, trying without: $issue_url\u0026#34; gh issue create -R \u0026#34;$repo\u0026#34; --title \u0026#34;Dependabot keep-alive\u0026#34; --body \u0026#34;$body\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34; ‚ùå Failed to create issue\u0026#34; } else echo \u0026#34; ‚ÑπÔ∏è Creating issue without \u0026#39;dependencies\u0026#39; label (not found)\u0026#34; gh issue create -R \u0026#34;$repo\u0026#34; --title \u0026#34;Dependabot keep-alive\u0026#34; --body \u0026#34;$body\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34; ‚ùå Failed to create issue\u0026#34; fi fi total_issues=$((total_issues + 1)) fi done echo \u0026#34;üìä Summary: Nudged $total_prs PRs, created/updated $total_issues issues\u0026#34; Workflow #3: Dependabot Keep-Alive Scheduled (Orchestration) Purpose Provides a scheduled trigger for organization-wide keep-alive operations.\nComplete Workflow File Here\u0026rsquo;s the complete dependabot-keep-alive-scheduled.yml workflow:\nname: Dependabot Keep-Alive (Scheduled) on: schedule: - cron: \u0026#34;0 0 1 */3 *\u0026#34; workflow_dispatch: inputs: mode: description: \u0026#39;runner mode: hosted or selfhosted\u0026#39; required: false default: \u0026#39;hosted\u0026#39; # Critical for org-wide operations permissions: contents: read pull-requests: write issues: write # Note: For org-wide scanning, ensure ORG_READ_TOKEN secret has: # - repo (full control of repositories) # - workflow (update GitHub Action workflows) # - read:org (read org and team membership, read org projects) jobs: call: uses: ./.github/workflows/dependabot-keep-alive.yml secrets: inherit with: runs-on: ${{ github.event_name == \u0026#39;workflow_dispatch\u0026#39; \u0026amp;\u0026amp; inputs.mode == \u0026#39;selfhosted\u0026#39; \u0026amp;\u0026amp; \u0026#39;[\u0026#34;self-hosted\u0026#34;,\u0026#34;dependabot\u0026#34;]\u0026#39; || \u0026#39;[\u0026#34;ubuntu-latest\u0026#34;]\u0026#39; }} team-handle: \u0026#34;@your-org/maintainers\u0026#34; scan-mode: \u0026#39;org\u0026#39; # Use organization mode to scan all repos with dependabot org: \u0026#39;your-org\u0026#39; # Specify the organization to scan Key Configuration Points:\nSchedule: Runs quarterly on the 1st of every 3rd month Manual Trigger: Supports workflow_dispatch for testing Runner Selection: Can use hosted or self-hosted runners Secrets: Inherits all secrets (including ORG_READ_TOKEN) Organization: Change org: 'your-org' to your organization name Team Handle: Update team-handle to your team\u0026rsquo;s GitHub handle Why Quarterly? Running every 3 months (well before the 90-day threshold) provides:\nSafety buffer: Catches repos even if one run fails Resource efficiency: Doesn\u0026rsquo;t overwhelm the API Manual flexibility: Allows manual triggers when needed Implementation Guide Prerequisites Organization Token (ORG_READ_TOKEN secret)\nRequired Scopes: repo - Full control of repositories workflow - Update GitHub Action workflows (comment on PRs, create issues) read:org - Read org and team membership, read org projects Used for: Cross-repository scanning, PR comments, and issue creation Repository Permissions\npermissions: contents: read pull-requests: write issues: write Setup Steps Create the Keep-Alive Reusable Workflow\nPlace in .github/workflows/dependabot-keep-alive.yml Configure inputs and permissions Create the Keep-Alive Scheduled Workflow\nPlace in .github/workflows/dependabot-keep-alive-scheduled.yml Set your organization name Configure schedule Create the Inactivity Report Workflow\nPlace in .github/workflows/dependabot-inactivity-report.yml Adjust cutoff days if needed Configure filtering patterns Add Organization Token\nCreate a PAT (Personal Access Token) with these scopes: ‚úÖ workflow - Update GitHub Action workflows ‚úÖ repo - Full control of repositories ‚úÖ read:org - Read org and team membership, read org projects Add as ORG_READ_TOKEN repository secret or organization secret The token will be used to scan repositories and create issues/comments Test with Dry-Run\n# Trigger inactivity report in dry-run mode gh workflow run dependabot-inactivity-report.yml \\ -f dry-run=true \\ -f org=your-org Real-World Benefits Before This Solution ‚ùå Manually checked 50+ repositories monthly ‚ùå Discovered paused Dependabot after the fact ‚ùå Spent hours clicking through UI to trigger updates ‚ùå Inconsistent dependency update practices After Implementation ‚úÖ Automated monitoring across entire organization ‚úÖ Early warnings 15 days before pause ‚úÖ Proactive prevention through automated nudges ‚úÖ Centralized visibility into dependency health ‚úÖ Zero manual intervention for most repositories Advanced Patterns Custom Filtering Strategy Target specific repository groups:\n# Keep-alive for production services only repo-includes: \u0026#39;prod-*,service-*\u0026#39; repo-excludes: \u0026#39;*-test,*-sandbox\u0026#39; Tiered Monitoring Create multiple inactivity report workflows with different thresholds:\nCritical repos: 60-day threshold (30-day buffer) Standard repos: 75-day threshold (15-day buffer) Low-priority repos: 85-day threshold (5-day buffer) Integration with Slack/Teams Extend the inactivity report to post reports to team channels:\n- name: Notify team uses: slackapi/slack-github-action@v1 with: payload: | { \u0026#34;text\u0026#34;: \u0026#34;Dependabot inactivity alert\u0026#34;, \u0026#34;blocks\u0026#34;: [...] } Monitoring \u0026amp; Metrics Track the system\u0026rsquo;s effectiveness:\nIssue Creation Rate: How many reminder issues are created PR Nudge Success: Percentage of successful rebase commands Prevented Pauses: Repos that would have paused without intervention Time Savings: Manual hours saved per quarter Lessons Learned Technical Gotchas Bash Arithmetic with set -e\nUse $((var + 1)) instead of ((var++)) GitHub Search API Efficiency\nMuch faster than paginating through all repos Has rate limits (consider for very large orgs) Label Handling\nNot all repos have standard labels Always provide fallback behavior Token Scopes\nGITHUB_TOKEN cannot access other repos in the organization Requires PAT with workflow and repo and read:org scopes for cross-repo operations The workflow scope allows commenting on PRs and creating issues The repo scope provides full control of repositories The read:org scope enables discovering repositories via the Search API Operational Insights Start with Dry-Run: Always test with dry-run mode first Monitor First Run: Watch the initial org-wide scan carefully Adjust Thresholds: Tune based on your team\u0026rsquo;s response time Document for Team: Make sure everyone understands the automation Cost Considerations GitHub Actions Minutes Inactivity Report (daily): ~2-5 minutes per run = ~75-150 min/month Keep-Alive (quarterly): ~10-30 minutes per run = ~40-120 min/quarter Total: ~195-270 minutes per quarter For organizations on GitHub Team or Enterprise, this is negligible compared to the free tier (2,000+ minutes/month).\nAPI Rate Limits Search API: 30 requests per minute (authenticated) Issues/PR operations: 5,000 requests per hour For 100 repositories: Well within limits Future Enhancements Potential additions to consider:\nMetrics Dashboard: Track Dependabot health over time Auto-Merge: Automatically merge certain types of updates Dependency Insights: Analyze which dependencies update most frequently Security Priority: Prioritize security updates in nudging logic Custom Schedules: Per-repository cadences based on criticality Conclusion Managing Dependabot at scale doesn\u0026rsquo;t have to be a manual nightmare. By combining proactive automation with early detection, this system:\nPrevents the 90-day pause before it happens Detects inactivity early with configurable thresholds Scales to hundreds of repositories effortlessly Requires minimal ongoing maintenance The complete solution is production-ready and has been battle-tested across multiple organizations. The workflows are modular, allowing you to adopt them incrementally or all at once.\nStart with the inactivity report for visibility, add the keep-alive workflows for prevention, and watch your dependency management transform from a time sink into a well-oiled machine.\nResources GitHub Dependabot Documentation GitHub Actions Workflow Syntax GitHub Search API Questions or improvements? Feel free to adapt these workflows to your organization\u0026rsquo;s needs. The modular design makes it easy to customize behavior while maintaining the core functionality.\nHappy automating! üöÄ\n","date":"2025-09-30T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/09/automating-dependabot-at-scale-github-actions-strategy/cover_hu_2af66fc3907066b0.jpg","image":"https://quintelier.dev/posts/2025/09/automating-dependabot-at-scale-github-actions-strategy/cover_hu_47bc383c5f236caa.jpg","permalink":"https://quintelier.dev/posts/2025/09/automating-dependabot-at-scale-github-actions-strategy/","title":"ü§ñ Automating Dependabot at Scale: GitHub Actions Strategy","webpImage":"https://quintelier.dev/posts/2025/09/automating-dependabot-at-scale-github-actions-strategy/cover_hu_fdad6e0747d51521.webp"},{"content":"Part 2: Deep Dive into async and await In Part 1, we saw why asynchronous programming matters and how async/await can make our apps more responsive. Now it‚Äôs time to go deeper into how it actually works.\nAnatomy of an async Method Let‚Äôs look at a simple method:\npublic async Task DoWorkAsync() { Console.WriteLine(\u0026#34;Step 1: Start work\u0026#34;); await Task.Delay(2000); Console.WriteLine(\u0026#34;Step 2: Work complete\u0026#34;); } Breaking it down:\nasync ‚Üí marks the method as asynchronous. Task ‚Üí return type that represents ongoing work. await Task.Delay(2000) ‚Üí pauses execution here until the delay finishes. Important: The method doesn‚Äôt block the thread. The runtime pauses at the await, then comes back to resume execution when the task completes.\nReturn Types of Async Methods There are three common return types for async methods:\nTask ‚Üí when you don‚Äôt return a value.\npublic async Task SaveAsync() { ... } Task\u0026lt;T\u0026gt; ‚Üí when you return a value.\npublic async Task\u0026lt;int\u0026gt; GetNumberAsync() { await Task.Delay(1000); return 42; } Usage:\nint result = await GetNumberAsync(); void ‚Üí should be avoided (fire-and-forget), except for event handlers.\nprivate async void Button_Click(object sender, EventArgs e) { await SaveAsync(); } Problem: You can‚Äôt await an async void, making error handling tricky.\nWhat Does await Really Do? At first glance, await looks like ‚Äújust wait here until it‚Äôs done.‚Äù\nBut that‚Äôs not quite right.\nHere‚Äôs what actually happens:\nThe method starts running. When it hits await someTask;, it checks if the task is already complete. If yes ‚Üí continues immediately. If no ‚Üí pauses execution. The compiler generates a state machine behind the scenes. It remembers: Where the method left off Local variables What should happen when the task completes When the task finishes, the method resumes right after the await. So await is really ‚Äúregister a continuation and resume later.‚Äù\nSequential vs. Asynchronous Execution Here‚Äôs an example that shows the difference:\npublic async Task SequentialAsync() { var client = new HttpClient(); // Sequential (slower) var page1 = await client.GetStringAsync(\u0026#34;https://example.com/page1\u0026#34;); var page2 = await client.GetStringAsync(\u0026#34;https://example.com/page2\u0026#34;); Console.WriteLine(\u0026#34;Sequential done\u0026#34;); } This runs page1, waits, then runs page2.\nNow let‚Äôs run them in parallel:\npublic async Task ParallelAsync() { var client = new HttpClient(); // Start both tasks immediately var task1 = client.GetStringAsync(\u0026#34;https://example.com/page1\u0026#34;); var task2 = client.GetStringAsync(\u0026#34;https://example.com/page2\u0026#34;); // Wait for both to complete await Task.WhenAll(task1, task2); Console.WriteLine(\u0026#34;Parallel done\u0026#34;); } Both requests are in-flight together ‚Üí much faster.\nKey Takeaways async makes a method awaitable. Return types matter: use Task or Task\u0026lt;T\u0026gt;, avoid void. await doesn‚Äôt block ‚Äî it sets up a continuation and resumes later. Sequential awaits can be slow; sometimes it‚Äôs better to run tasks in parallel with Task.WhenAll. üëâ In Part 3, we‚Äôll cover:\nThe dangers of async void Deadlocks and ConfigureAwait(false) Mixing sync and async code safely Exception handling in async methods Series Navigation Previous: Part 1 ‚Äì Introduction Series Index: Overview Next: Part 3 ‚Äì Pitfalls \u0026amp; Best Practices\n","date":"2025-09-24T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/09/csharp-async-await-part2/cover_hu_ca8544d580f857c6.jpg","image":"https://quintelier.dev/posts/2025/09/csharp-async-await-part2/cover_hu_49ebced5a1d81a1b.jpg","permalink":"https://quintelier.dev/posts/2025/09/csharp-async-await-part2/","title":"Mastering Asynchronous Programming with C# async/await - Part 2: Deep Dive","webpImage":"https://quintelier.dev/posts/2025/09/csharp-async-await-part2/cover_hu_787ea508cede484.webp"},{"content":"ü§ñ When to Use GitHub Models, Azure AI Foundry, and OpenAI Artificial intelligence is moving fast, and so are the platforms that deliver it.\nFor many teams, the question isn‚Äôt ‚ÄúCan we use AI?‚Äù but ‚ÄúWhere should we use it from?‚Äù\nBetween GitHub Models, Azure AI Foundry, and OpenAI, developers and enterprises alike face an abundance of choice. The reality is that there is no single ‚Äúbest‚Äù option ‚Äî the right one depends on your perspective and your stage in the AI journey.\nThis guide takes a neutral look at when to use each option, from both the developer‚Äôs lens (speed, simplicity, integration) and the enterprise lens (compliance, governance, scalability).\nüåê The Options at a Glance GitHub Models\nA selection of models available directly in GitHub. Perfect for prototyping, experimenting, and enhancing workflows inside repositories. Integrated with GitHub Copilot and Actions.\nAzure AI Foundry (formerly Azure AI Studio)\nA full-featured platform for building, evaluating, deploying, and governing AI solutions. Enterprise-grade, with compliance, monitoring, and integration into the wider Azure ecosystem. Supports multiple foundation models, both proprietary and open source.\nOpenAI\nDirect access to OpenAI‚Äôs cutting-edge models (e.g., GPT-4, o1, DALL¬∑E, Whisper). Typically the first place to see new features and capabilities. Lightweight to get started, but less focused on governance out of the box.\nüß≠ Visual Decision Map flowchart LR A[Start: What\u0026#39;s your immediate goal?] --\u0026gt; B{Prototype quickly in a repo?} B -- Yes --\u0026gt; GM[GitHub Models] B -- No --\u0026gt; C{Need enterprise governance, data privacy, and observability?} C -- Yes --\u0026gt; AF[Azure AI Foundry] C -- No --\u0026gt; D{Chasing cutting-edge features first?} D -- Yes --\u0026gt; OA[OpenAI] D -- No --\u0026gt; E{Multi-model mix + Azure integration helpful?} E -- Yes --\u0026gt; AF E -- No --\u0026gt; GM GM --\u0026gt; GM2[PoC validated?] GM2 -- Yes --\u0026gt; AF GM2 -- No --\u0026gt; GM OA --\u0026gt; OA2[Frontier feature stable?] OA2 -- Yes --\u0026gt; AF OA2 -- No --\u0026gt; OA Alt text: Start with your goal. If you need fast repo-native prototyping, choose GitHub Models. If governance and integration matter, choose Azure AI Foundry. If you need frontier features, use OpenAI ‚Äî and migrate to Foundry once stable.\n‚öñÔ∏è Comparison by Perspective AI platforms look different depending on whether you are a developer or an enterprise decision-maker.\nHere‚Äôs how the three compare across both views:\nCriteria GitHub Models Azure AI Foundry OpenAI Developer View Quick prototyping inside GitHub. Easy to switch models. More setup overhead, but powerful integrations with data and apps. Fastest access to the latest features and frontier models. Enterprise View Suitable for PoCs and hackathons, but limited governance. Enterprise-ready with compliance, monitoring, and Responsible AI tooling. Innovation-friendly, but weaker on data residency and enterprise controls. üßë‚Äçüíª Developer Perspective Priorities: speed, integration, flexibility.\nGitHub Models\nTest prompts and models directly in your repo. Zero infra to start; ideal for summarizing PRs, generating docs, or experimenting with RAG approaches on public data. Swap models easily without vendor lock-in during early exploration. Azure AI Foundry\nAdds evaluations, safety filters, and deployment endpoints. Integrates with Azure services (Search, Key Vault, Functions, Container Apps) to build full stacks. Useful when prototypes must touch enterprise data or scale behind private networks. OpenAI\nGreat for trying new reasoning/multimodal/agent features. Simple APIs and SDKs to build quick demos or SaaS features. Consider a path to bring workloads under enterprise control later. üè¢ Enterprise Perspective Priorities: security, compliance, cost governance, reliability.\nGitHub Models\nExcellent for hackathons and ideation, but not a production landing zone. Encourage developers to validate feasibility here before requesting infra. Azure AI Foundry\nStrongest option for regulated workloads. Regional hosting, network isolation, and observability out of the box. Built-in Responsible AI: monitoring, content filters, human-in-the-loop patterns, and auditability. Supports multi-model strategies and gradual model replacement. OpenAI\nPerfect for innovation labs and customer pilots where cutting-edge matters. Requires additional guardrails for data residency, privacy, and DLP if used for sensitive data. Keep a governance plan for model/version drift. üîÑ A Pragmatic Hybrid Journey Prototype with GitHub Models ‚Äî move fast, learn, de-risk ideas. Scale \u0026amp; Govern with Azure AI Foundry ‚Äî secure data, monitor, and control costs. Innovate with OpenAI ‚Äî scout frontier capabilities and feed learnings back into the roadmap. Why this works: it aligns developer speed with enterprise safety without stalling either side.\nüß© Architecture Patterns (High-Level) Repo-native PoC ‚Üí GitHub Actions triggers evaluation jobs; results posted to PRs. Enterprise RAG ‚Üí Azure AI Search + Azure AI Foundry endpoints + Key Vault for secrets + Private Link. Hybrid Innovation ‚Üí Feature toggles to call OpenAI for specific capabilities while the core runs on Foundry. flowchart TB subgraph PoC [Repo-native PoC] A1[GitHub Repo] --\u0026gt; A2[GitHub Models Playground] A1 --\u0026gt; A3[GitHub Actions Eval Job] A3 --\u0026gt; A4[PR Annotations] end subgraph Prod [Enterprise RAG on Azure] B1[Data Lake / Blob] --\u0026gt; B2[Indexer] B2 --\u0026gt; B3[Azure AI Search] B3 --\u0026gt; B4[Azure AI Foundry Endpoint] B4 --\u0026gt; B5[App Service / Functions] B5 --\u0026gt; B6[Observability / Cost] B4 -.-\u0026gt; B7[Key Vault / Private Link] end subgraph Frontier [Innovation Stream] C1[Feature Flags] --\u0026gt; C2[OpenAI API] C2 --\u0026gt; C3[Beta Features / A/B Tests] C3 --\u0026gt; B5 end üß™ Evaluation \u0026amp; Responsible AI Define success metrics early (latency, accuracy, refusal rate, cost per request). Run red-team tests for prompt injection, data leakage, and jailbreaks. Track model and prompt versions; compare with A/B or interleaving tests. Document known limitations and fallback behaviors. üí∞ Cost \u0026amp; Operations Shift-left on costing: estimate cost-per-call √ó expected volume; include embedding/search costs in RAG. Right-size deployments: autoscale endpoints; cache results; batch long-running jobs. Observability: centralize logs, traces, and evaluations; alert on anomaly spikes. FinOps: tag resources by team/project; review idle endpoints monthly. ‚úÖ Conclusion The question of whether to use GitHub Models, Azure AI Foundry, or OpenAI doesn‚Äôt have a one-size-fits-all answer. It depends on context, audience, and maturity.\nGitHub Models = Idea Lab ‚Üí fast, frictionless prototyping. Azure AI Foundry = AI Factory ‚Üí enterprise-grade deployment with governance. OpenAI = Innovation Hub ‚Üí cutting-edge experimentation. For developers, start small and iterate quickly.\nFor enterprises, invest in governance and scale responsibly.\nTogether, both views create a pathway from exploration ‚Üí production ‚Üí innovation\n","date":"2025-09-23T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/09/when-to-use-github-models-azure-ai-foundry-openai/cover_hu_18b139ed679f202a.jpg","image":"https://quintelier.dev/posts/2025/09/when-to-use-github-models-azure-ai-foundry-openai/cover_hu_3c02ed4bcdda22a4.jpg","permalink":"https://quintelier.dev/posts/2025/09/when-to-use-github-models-azure-ai-foundry-openai/","title":"ü§ñ When to Use GitHub Models, Azure AI Foundry, and OpenAI","webpImage":"https://quintelier.dev/posts/2025/09/when-to-use-github-models-azure-ai-foundry-openai/cover_hu_e5120b4e669dcbe0.webp"},{"content":"Complete 7-Part Series Series Navigation Part 1 ‚Äì Introduction Part 2 ‚Äì Deep Dive Part 3 ‚Äì Pitfalls \u0026amp; Best Practices Part 4 ‚Äì Patterns Part 5 ‚Äì Real-World Use Cases Part 6 ‚Äì Advanced Topics Part 7 ‚Äì Testing \u0026amp; Debugging Asynchronous programming is one of the most powerful features in modern C#. It keeps your applications responsive, scalable, and efficient ‚Äî but it can also be tricky to master.\nThis 7-part series takes you from the basics of async/await to advanced scenarios, pitfalls, and testing strategies. Each part builds on the previous one with clear explanations and practical code samples.\nüìö Table of Contents Part 1: Introduction to Asynchronous Programming Why async matters, the difference between blocking and non-blocking code, and your first async/await example.\nüëâ Read Part 1\nPart 2: Deep Dive into async and await The anatomy of an async method, return types (Task, Task\u0026lt;T\u0026gt;, void), and how await really works.\nüëâ Read Part 2\nPart 3: Common Pitfalls \u0026amp; Best Practices Avoiding async void, handling deadlocks, using ConfigureAwait(false), and safe exception handling.\nüëâ Read Part 3\nPart 4: Patterns with Async Running multiple tasks in parallel, cancelling with CancellationToken, using async streams, and implementing timeouts/retries.\nüëâ Read Part 4\nPart 5: Real-World Use Cases How async improves real apps: calling APIs with HttpClient, async file I/O, EF Core queries, responsive UIs, and background services.\nüëâ Read Part 5\nPart 6: Advanced Topics Performance with ValueTask, writing custom awaiters, using coordination primitives (SemaphoreSlim, Channel), and tuning async performance.\nüëâ Read Part 6\nPart 7: Testing and Debugging Async Code Writing async unit tests, mocking async methods, debugging with Visual Studio async tools, logging async flows, and handling unobserved exceptions.\nüëâ Read Part 7\n","date":"2025-09-17T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/09/csharp-async-await/cover_hu_ab580e340e52cd80.jpg","image":"https://quintelier.dev/posts/2025/09/csharp-async-await/cover_hu_31943f4e46dd9dd1.jpg","permalink":"https://quintelier.dev/posts/2025/09/csharp-async-await/","title":"Mastering Asynchronous Programming with C# async/await","webpImage":"https://quintelier.dev/posts/2025/09/csharp-async-await/cover_hu_222edc7e27f33ff0.webp"},{"content":"Part 1: Introduction to Asynchronous Programming When you build applications in C#, one of the biggest challenges is making them fast and responsive. Whether you‚Äôre writing a desktop app that must keep its UI smooth, a web API that needs to handle thousands of requests, or a service that integrates with multiple external systems, asynchronous programming is key.\nBut async/await can feel like magic ‚ú®. Let‚Äôs break it down step by step.\nWhy Asynchronous Programming? By default, C# code runs synchronously ‚Äî one line after another. If a method takes 5 seconds (e.g., downloading a file), everything else waits.\nExample:\nConsole.WriteLine(\u0026#34;Starting work...\u0026#34;); Thread.Sleep(5000); // Simulates long work Console.WriteLine(\u0026#34;Work complete!\u0026#34;); Output:\nStarting work... (5-second pause) Work complete! This is fine for quick operations but becomes a problem when:\nA UI freezes while waiting A web server is blocked and can‚Äôt serve other requests Long-running I/O (file, database, network) slows down everything That‚Äôs where async comes in.\nEnter async and await Instead of blocking threads, C# lets you write asynchronous code with async and await.\nRewriting the above example:\nConsole.WriteLine(\u0026#34;Starting work...\u0026#34;); await Task.Delay(5000); // Non-blocking wait Console.WriteLine(\u0026#34;Work complete!\u0026#34;); This looks almost identical ‚Äî but the difference is huge:\nThread.Sleep blocks the thread ‚Üí nothing else can run. Task.Delay yields control back ‚Üí freeing the thread for other work. How Does It Work? When you mark a method as async, you can use await inside it.\npublic async Task DoWorkAsync() { Console.WriteLine(\u0026#34;Starting work...\u0026#34;); await Task.Delay(2000); Console.WriteLine(\u0026#34;Work complete!\u0026#34;); } The compiler transforms this into a state machine:\nStart running until the first await. Pause execution while the awaited task runs. Resume where it left off once the task completes. The magic? Your code still looks synchronous ‚Äî no callbacks, no spaghetti.\nAsync in Action: A Real Example Imagine fetching data from two APIs.\nSynchronous version:\nvar client = new HttpClient(); var weather = client.GetStringAsync(\u0026#34;https://api.weather.com/data\u0026#34;).Result; var news = client.GetStringAsync(\u0026#34;https://api.news.com/headlines\u0026#34;).Result; Console.WriteLine($\u0026#34;Weather: {weather}\u0026#34;); Console.WriteLine($\u0026#34;News: {news}\u0026#34;); This blocks each call until it‚Äôs finished. Slow ‚è≥.\nAsynchronous version:\nvar client = new HttpClient(); var weatherTask = client.GetStringAsync(\u0026#34;https://api.weather.com/data\u0026#34;); var newsTask = client.GetStringAsync(\u0026#34;https://api.news.com/headlines\u0026#34;); var weather = await weatherTask; var news = await newsTask; Console.WriteLine($\u0026#34;Weather: {weather}\u0026#34;); Console.WriteLine($\u0026#34;News: {news}\u0026#34;); Here:\nBoth requests start immediately. await only pauses until each finishes. The result: faster execution ‚ö°. Key Takeaways Asynchronous programming keeps apps responsive and scalable. async enables await in a method. await pauses execution until the task completes ‚Äî without blocking the thread. Async code looks like sync code but is much more powerful. üëâ In Part 2, we‚Äôll go deeper:\nWhat exactly happens behind the scenes with async and await? The different return types (Task, Task\u0026lt;T\u0026gt;, void) How the compiler rewrites async methods Series Navigation Series Index: Overview Next: Part 2 ‚Äì Deep Dive\n","date":"2025-09-17T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/09/csharp-async-await-part1/cover_hu_57c6ecd0982c7ce0.jpg","image":"https://quintelier.dev/posts/2025/09/csharp-async-await-part1/cover_hu_eefeabe3a96a1996.jpg","permalink":"https://quintelier.dev/posts/2025/09/csharp-async-await-part1/","title":"Mastering Asynchronous Programming with C# async/await - Part 1: Introduction","webpImage":"https://quintelier.dev/posts/2025/09/csharp-async-await-part1/cover_hu_1e1b88dfa992e70b.webp"},{"content":"If you‚Äôve enabled Dependabot, you probably love the automated updates but not the PR storm and the merge conflicts that happen when several PRs touch the same lockfiles or project files.\nThe cure is Dependabot Groups: batch related updates into a single PR so you merge once and move on.\nWhy conflicts happen npm: multiple PRs each rewrite package-lock.json. NuGet: several PRs change the same .csproj / packages.lock.json. GitHub Actions: many small PRs bumping actions across several workflow files. Merge one PR ‚Üí the others go stale ‚Üí conflicts or repeated rebases.\nThe fix: group by ecosystem \u0026amp; update type Batch minor/patch together (usually safe). Keep majors separate (often need code changes). Optionally group security updates for quick handling. Limit concurrency with open-pull-requests-limit. Drop-in config: npm, NuGet \u0026amp; GitHub Actions Create or edit .github/dependabot.yml:\nversion: 2 updates: # npm (package.json / lockfile) - package-ecosystem: npm directory: \u0026#34;/\u0026#34; schedule: interval: weekly open-pull-requests-limit: 5 groups: npm-minor-patch: patterns: [\u0026#34;*\u0026#34;] update-types: [\u0026#34;minor\u0026#34;, \u0026#34;patch\u0026#34;] npm-majors: patterns: [\u0026#34;*\u0026#34;] update-types: [\u0026#34;major\u0026#34;] npm-security: applies-to: security-updates patterns: [\u0026#34;*\u0026#34;] # NuGet (.csproj, .props, .targets, packages.lock.json) - package-ecosystem: nuget directory: \u0026#34;/\u0026#34; schedule: interval: weekly open-pull-requests-limit: 5 groups: nuget-minor-patch: patterns: [\u0026#34;*\u0026#34;] update-types: [\u0026#34;minor\u0026#34;, \u0026#34;patch\u0026#34;] nuget-majors: patterns: [\u0026#34;*\u0026#34;] update-types: [\u0026#34;major\u0026#34;] nuget-security: applies-to: security-updates patterns: [\u0026#34;*\u0026#34;] # GitHub Actions (.github/workflows/*.yml) - package-ecosystem: github-actions directory: \u0026#34;/\u0026#34; schedule: interval: weekly open-pull-requests-limit: 5 groups: actions-all: patterns: [\u0026#34;*\u0026#34;] # one PR for all action bumps actions-security: applies-to: security-updates patterns: [\u0026#34;*\u0026#34;] Special notes for GitHub Actions One PR, many workflows: Grouping will update all matching workflows in one PR (e.g., bump actions/checkout across multiple files together). Version style: Dependabot follows tags you use in uses: (e.g., @v4 or @v4.1.0). Using major tags (@v4) gives you a steady cadence of safe bumps. Security posture: Many teams pin to major tags (easier updates) or to exact versions. If you pin to commit SHAs, you won‚Äôt receive normal semver/tag updates, review your policy and decide which approach fits your supply-chain requirements. Noise reduction: Toolchain bursts (e.g., actions/setup-node, setup-dotnet, checkout, upload-artifact) are where grouping shines, one tidy PR instead of five. Still stale? Comment @dependabot rebase on the PR to refresh after other merges. Tips that save time Cadence: Weekly works well-predictable and batch-friendly. Lockfile only (npm): For infra repos, consider versioning-strategy: lockfile-only. Per-folder repos: Add additional updates blocks with different directory values if you have multiple package roots. PR hygiene: Keep open-pull-requests-limit low to avoid ‚ÄúPR storms‚Äù. Troubleshooting quick hits ‚ÄúWhy didn‚Äôt X get grouped?‚Äù\nCheck the patterns and update-types. Majors won‚Äôt join a minor/patch group. ‚ÄúWhy so many Actions PRs?‚Äù\nEnsure the github-actions ecosystem has a groups: block (see above). ‚ÄúConflicts anyway?‚Äù\nIf another large change touched the same files, trigger a rebase (@dependabot rebase). Grouping minimizes, but can‚Äôt eliminate, all conflicts. Official docs Dependabot options reference ‚Üí Groups\nhttps://docs.github.com/en/code-security/dependabot/working-with-dependabot/dependabot-options-reference#groups Optimizing PR creation with groups\nhttps://docs.github.com/en/code-security/dependabot/dependabot-version-updates/optimizing-pr-creation-version-updates Configuring Dependabot security updates\nhttps://docs.github.com/en/code-security/dependabot/dependabot-security-updates/configuring-dependabot-security-updates Configuring multi-ecosystem updates\nhttps://docs.github.com/en/code-security/dependabot/working-with-dependabot/configuring-multi-ecosystem-updates ","date":"2025-09-16T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/09/taming-dependabot-bundle-nuget-npm-github-actions-updates-without-merge-conflicts/cover_hu_bee23148f7a39c0a.jpg","image":"https://quintelier.dev/posts/2025/09/taming-dependabot-bundle-nuget-npm-github-actions-updates-without-merge-conflicts/cover_hu_baf3c0b7ec4069ea.jpg","permalink":"https://quintelier.dev/posts/2025/09/taming-dependabot-bundle-nuget-npm-github-actions-updates-without-merge-conflicts/","title":"Taming Dependabot: Bundle NuGet, npm, and GitHub Actions Updates Without Merge Conflicts","webpImage":"https://quintelier.dev/posts/2025/09/taming-dependabot-bundle-nuget-npm-github-actions-updates-without-merge-conflicts/cover_hu_fcb8cc7483b1ad24.webp"},{"content":"Introduction Welcome to Part 4 of the GitHub Certification Journey! ü§ñ\nAfter mastering GitHub Foundations (GH-900), GitHub Administration (GH-100), and GitHub Actions (GH-200), you\u0026rsquo;re ready to embrace the future of software development with GitHub Copilot - the revolutionary AI pair programmer that transforms how developers write, review, and maintain code. The GH-300 GitHub Copilot certification validates your expertise in AI-powered development workflows, prompt engineering, and enterprise AI governance.\nThis comprehensive guide provides everything needed to pass the GH-300 exam and become a GitHub Copilot expert. Whether you\u0026rsquo;re a developer seeking to amplify productivity, a team lead implementing AI development practices, or an enterprise architect designing AI-powered development strategies, this preparation roadmap will take you from basic AI assistance to advanced enterprise Copilot mastery.\nCertification Overview About GH-300 GitHub Copilot Certification Name: GitHub Copilot Exam Code: GH-300 Duration: 150 minutes Question Count: ~75 questions Passing Score: 700/1000 (approximately 70%) Cost: $99 USD Validity: 3 years from certification date Prerequisites: GitHub Foundations (GH-900), GitHub Administration (GH-100), and GitHub Actions (GH-200) recommended Who Should Take This Exam Software developers using AI-powered development tools Technical leads implementing AI development workflows Product managers overseeing AI-enhanced development teams Enterprise architects designing AI development strategies DevOps engineers integrating AI tools into development pipelines Engineering managers optimising team productivity with AI Exam Domains Breakdown The GH-300 exam covers seven main domains with specific weightings:\nDomain 1: Getting Started with GitHub Copilot (23%) Core Competencies:\nUnderstanding GitHub Copilot ecosystem and capabilities Configuring Copilot for different development environments Managing Copilot subscriptions and licensing Implementing Copilot across development teams Understanding AI model capabilities and limitations Key Skills:\nCopilot installation and configuration across IDEs Subscription management and team deployment Initial setup and workspace optimization Understanding Copilot\u0026rsquo;s AI foundation models Privacy and data handling fundamentals Domain 2: Using GitHub Copilot Chat and Code Completion (15%) Core Competencies:\nMastering code completion and suggestion workflows Leveraging Copilot Chat for development assistance Understanding context-aware code generation Implementing AI-assisted debugging and optimization Using Copilot CLI for command-line productivity Key Skills:\nAdvanced code completion techniques Effective Copilot Chat interactions Context management and code understanding Debugging assistance and error resolution Command-line AI integration workflows Domain 3: How GitHub Copilot Works and Handles Data (15%) Core Competencies:\nUnderstanding Copilot\u0026rsquo;s AI architecture and data pipeline Comprehending code suggestion generation mechanisms Managing data privacy and security considerations Understanding model training and inference processes Implementing data governance for enterprise usage Key Skills:\nAI model architecture comprehension Data flow and processing understanding Privacy policy implementation Enterprise data governance Security and compliance considerations Domain 4: Prompt Crafting and Prompt Engineering (9%) Core Competencies:\nDesigning effective prompts for code generation Understanding context engineering and optimization Implementing advanced prompt patterns Measuring and improving prompt effectiveness Teaching prompt engineering to development teams Key Skills:\nPrompt design principles and best practices Context engineering and optimization techniques Advanced prompting patterns and strategies Prompt evaluation and iteration methods Team training and prompt standardisation Domain 5: Developer Use Cases for AI (14%) Core Competencies:\nImplementing AI-powered development workflows Enhancing developer productivity with AI assistance Using AI for code review and quality improvement Leveraging AI for documentation and learning Integrating AI into software development lifecycle Key Skills:\nProductivity optimization strategies AI-assisted code review processes Documentation generation and maintenance Learning and skill development with AI SDLC integration and workflow optimization Domain 6: Testing with GitHub Copilot (9%) Core Competencies:\nGenerating comprehensive test suites with AI assistance Implementing AI-powered test automation Using AI for test case design and edge case identification Enhancing test quality and coverage with AI Integrating AI testing into CI/CD pipelines Key Skills:\nAI-assisted test generation techniques Test automation and optimization Edge case identification and testing Quality assurance enhancement CI/CD integration for AI-generated tests Domain 7: Privacy Fundamentals and Context Exclusions (15%) Core Competencies:\nImplementing privacy-first AI development practices Configuring context exclusions and data protection Understanding enterprise privacy requirements Managing intellectual property and code confidentiality Implementing compliance frameworks for AI usage Key Skills:\nPrivacy policy configuration and management Context exclusion implementation Enterprise compliance and governance Intellectual property protection Data sovereignty and security frameworks Complete Study Plan Phase 1: AI Development Foundation (Weeks 1-2) Week 1: Copilot Fundamentals and Setup Understand GitHub Copilot architecture and capabilities Install and configure Copilot across development environments Explore subscription models and enterprise deployment Practice with basic code completion and suggestions Daily Tasks:\nInstall Copilot in multiple IDEs (VS Code, Visual Studio, JetBrains) Configure Copilot settings and preferences Explore different subscription tiers and features Practice basic code completion workflows Hands-On Labs:\n// Basic Copilot interaction patterns // Comment-driven development example function calculateCompoundInterest( principal: number, rate: number, time: number, compoundFrequency: number = 1 ): number { // Calculate compound interest using A = P(1 + r/n)^(nt) // where A = final amount, P = principal, r = rate, n = frequency, t = time // Copilot will suggest implementation based on this context } // Generate sorting algorithm with performance optimisation // Create a quick sort implementation that handles edge cases // and provides O(n log n) average case performance Week 2: Chat and CLI Mastery Master Copilot Chat for complex development tasks Learn advanced code completion techniques Practice with Copilot CLI for command-line productivity Understand context management and code understanding Advanced Chat Interactions:\n# Copilot CLI usage examples gh copilot explain \u0026#34;git rebase -i HEAD~3\u0026#34; gh copilot suggest \u0026#34;create a GitHub Actions workflow for Node.js testing\u0026#34; # Complex chat prompts for development # \u0026#34;Generate a React component with TypeScript that implements # a data table with sorting, filtering, and pagination. # Include proper error handling and accessibility features.\u0026#34; Phase 2: Prompt Engineering and AI Workflows (Weeks 3-4) Week 3: Advanced Prompt Engineering Learn prompt design principles and best practices Implement context engineering techniques Practice with different prompting strategies Measure and optimise prompt effectiveness Prompt Engineering Patterns:\n# Effective prompt patterns for code generation # Pattern 1: Context + Intent + Constraints \u0026#34;\u0026#34;\u0026#34; Context: Building a REST API for e-commerce platform Intent: Create user authentication middleware Constraints: - Use JWT tokens - Include rate limiting - Handle refresh tokens - Implement proper error responses - Follow security best practices \u0026#34;\u0026#34;\u0026#34; # Pattern 2: Step-by-step breakdown \u0026#34;\u0026#34;\u0026#34; Create a database migration system that: 1. Reads migration files from a directory 2. Tracks applied migrations in metadata table 3. Supports rollback functionality 4. Includes transaction safety 5. Provides detailed logging \u0026#34;\u0026#34;\u0026#34; # Pattern 3: Example-driven prompting \u0026#34;\u0026#34;\u0026#34; Following this pattern for API endpoints: GET /api/users -\u0026gt; UserController.getUsers() POST /api/users -\u0026gt; UserController.createUser() Create similar endpoints for orders with: - CRUD operations - Pagination support - Search functionality - Input validation \u0026#34;\u0026#34;\u0026#34; Week 4: Enterprise AI Integration Implement AI governance and policy frameworks Configure enterprise privacy and security settings Design team workflows with AI assistance Practice with large-scale AI development adoption Enterprise Configuration Examples:\n# Enterprise Copilot policy configuration copilot_enterprise_policy: suggestions: enabled: true public_code_matching: \u0026#34;blocked\u0026#34; chat: enabled: true enterprise_knowledge_base: true data_handling: transmission_to_github: false retention_period: \u0026#34;zero_retention\u0026#34; content_exclusions: file_patterns: - \u0026#34;*.env\u0026#34; - \u0026#34;*.key\u0026#34; - \u0026#34;*.pem\u0026#34; - \u0026#34;**/secrets/**\u0026#34; compliance: audit_logging: enabled usage_monitoring: enabled privacy_mode: strict Phase 3: Advanced Development Workflows (Weeks 5-6) Week 5: AI-Powered Development Lifecycle Integrate AI into complete development workflows Implement AI-assisted code review processes Use AI for documentation and knowledge management Practice with AI-enhanced debugging and optimization AI Development Workflow Integration:\n// AI-assisted development workflow example class AIEnhancedDevelopmentWorkflow { private copilot: GitHubCopilot; async generateFeature(requirements: string): Promise\u0026lt;FeatureImplementation\u0026gt; { // Use AI to break down requirements into tasks const tasks = await this.copilot.chat(` Break down these requirements into development tasks: ${requirements} Format as JSON with: description, complexity, dependencies, testing_requirements `); // Generate implementation with AI assistance const implementation = await this.generateImplementation(tasks); // AI-assisted code review const reviewSuggestions = await this.performAICodeReview(implementation); // Generate tests with AI const tests = await this.generateComprehensiveTests(implementation); return { implementation, tests, reviewSuggestions, documentation: await this.generateDocumentation(implementation) }; } async generateComprehensiveTests(code: string): Promise\u0026lt;TestSuite\u0026gt; { return await this.copilot.chat(` Generate comprehensive test suite for this code: ${code} Include: - Unit tests for all functions - Integration tests for workflows - Edge cases and error scenarios - Performance benchmarks - Security vulnerability tests `); } } Week 6: Testing and Quality Assurance with AI Master AI-assisted test generation and automation Implement quality assurance enhancement with AI Practice with CI/CD integration for AI-generated tests Learn advanced testing strategies with AI assistance AI Testing Framework:\n# AI-powered testing framework class AITestingFramework: def __init__(self, copilot_client): self.copilot = copilot_client async def generate_test_suite(self, source_code, test_requirements): \u0026#34;\u0026#34;\u0026#34;Generate comprehensive test suite using AI\u0026#34;\u0026#34;\u0026#34; prompt = f\u0026#34;\u0026#34;\u0026#34; Generate a comprehensive test suite for this code: {source_code} Requirements: - {test_requirements} - Include positive and negative test cases - Cover edge cases and boundary conditions - Include performance and security tests - Generate test data and mocks - Follow pytest best practices \u0026#34;\u0026#34;\u0026#34; return await self.copilot.generate_code(prompt) async def identify_edge_cases(self, function_signature, business_logic): \u0026#34;\u0026#34;\u0026#34;Use AI to identify potential edge cases\u0026#34;\u0026#34;\u0026#34; prompt = f\u0026#34;\u0026#34;\u0026#34; Analyze this function and identify potential edge cases: Function: {function_signature} Business Logic: {business_logic} Consider: - Input validation scenarios - Boundary conditions - Error states - Performance edge cases - Security vulnerabilities \u0026#34;\u0026#34;\u0026#34; return await self.copilot.analyze(prompt) Phase 4: Privacy, Security, and Enterprise Governance (Weeks 7-8) Week 7: Privacy and Security Implementation Configure comprehensive privacy and security settings Implement context exclusions and data protection Practice with enterprise compliance requirements Learn about intellectual property protection strategies Privacy and Security Configuration:\n{ \u0026#34;copilot_privacy_settings\u0026#34;: { \u0026#34;suggestions_enabled\u0026#34;: true, \u0026#34;public_code_suggestions\u0026#34;: \u0026#34;disabled\u0026#34;, \u0026#34;telemetry_collection\u0026#34;: \u0026#34;minimal\u0026#34;, \u0026#34;content_exclusions\u0026#34;: { \u0026#34;paths\u0026#34;: [ \u0026#34;src/secrets/\u0026#34;, \u0026#34;config/production/\u0026#34;, \u0026#34;*.env*\u0026#34;, \u0026#34;**/*.key\u0026#34;, \u0026#34;**/certificates/**\u0026#34; ], \u0026#34;file_types\u0026#34;: [\u0026#34;.pem\u0026#34;, \u0026#34;.p12\u0026#34;, \u0026#34;.jks\u0026#34;], \u0026#34;patterns\u0026#34;: [ \u0026#34;password.*=\u0026#34;, \u0026#34;secret.*=\u0026#34;, \u0026#34;api[_-]?key.*=\u0026#34;, \u0026#34;token.*=\u0026#34; ] }, \u0026#34;enterprise_features\u0026#34;: { \u0026#34;audit_logging\u0026#34;: true, \u0026#34;usage_analytics\u0026#34;: true, \u0026#34;content_scanning\u0026#34;: true, \u0026#34;policy_enforcement\u0026#34;: \u0026#34;strict\u0026#34; } } } Week 8: Advanced Enterprise Features and Governance Implement advanced enterprise Copilot features Design AI governance frameworks Practice with usage monitoring and analytics Learn about AI ethics and responsible development Enterprise Governance Framework:\n# Enterprise AI governance framework class CopilotGovernanceFramework: def __init__(self): self.policies = self.load_enterprise_policies() self.monitoring = UsageMonitoringService() def enforce_usage_policies(self, user_request): \u0026#34;\u0026#34;\u0026#34;Enforce enterprise AI usage policies\u0026#34;\u0026#34;\u0026#34; # Check compliance requirements compliance_check = self.validate_compliance(user_request) # Verify data classification data_classification = self.classify_request_data(user_request) # Apply appropriate restrictions restrictions = self.apply_policy_restrictions( user_request, compliance_check, data_classification ) return { \u0026#39;approved\u0026#39;: restrictions.get(\u0026#39;approved\u0026#39;, False), \u0026#39;conditions\u0026#39;: restrictions.get(\u0026#39;conditions\u0026#39;, []), \u0026#39;audit_trail\u0026#39;: self.create_audit_record(user_request, restrictions) } def monitor_ai_usage(self): \u0026#34;\u0026#34;\u0026#34;Monitor and analyze AI usage patterns\u0026#34;\u0026#34;\u0026#34; usage_metrics = { \u0026#39;total_requests\u0026#39;: self.monitoring.get_request_count(), \u0026#39;code_generation_requests\u0026#39;: self.monitoring.get_code_requests(), \u0026#39;chat_interactions\u0026#39;: self.monitoring.get_chat_interactions(), \u0026#39;policy_violations\u0026#39;: self.monitoring.get_violations(), \u0026#39;productivity_metrics\u0026#39;: self.calculate_productivity_impact() } return self.generate_governance_report(usage_metrics) Hands-On Laboratory Exercises Lab 1: Complete AI Development Workflow Implementation Implement a comprehensive AI-powered development workflow from requirements to deployment.\nObjectives:\nDesign AI-assisted feature development pipeline Implement prompt engineering best practices Create automated testing with AI assistance Build documentation generation workflows Implementation Steps:\nAI-Powered Feature Development Pipeline: // Complete AI development workflow import { GitHubCopilot } from \u0026#39;@github/copilot-sdk\u0026#39;; class AIFeatureDevelopmentPipeline { private copilot: GitHubCopilot; constructor() { this.copilot = new GitHubCopilot({ apiKey: process.env.COPILOT_API_KEY, organizationId: process.env.GITHUB_ORG_ID }); } async developFeature(requirements: FeatureRequirements): Promise\u0026lt;FeatureDeliverable\u0026gt; { // Step 1: AI-assisted requirements analysis const analysis = await this.analyzeRequirements(requirements); // Step 2: Generate architecture with AI const architecture = await this.generateArchitecture(analysis); // Step 3: AI-powered implementation const implementation = await this.generateImplementation(architecture); // Step 4: AI-generated comprehensive tests const testSuite = await this.generateTestSuite(implementation); // Step 5: AI-assisted documentation const documentation = await this.generateDocumentation(implementation); // Step 6: AI code review and optimization const optimizations = await this.performAICodeReview(implementation); return { code: implementation, tests: testSuite, documentation, optimizations, deploymentGuide: await this.generateDeploymentGuide(implementation) }; } private async analyzeRequirements(requirements: FeatureRequirements): Promise\u0026lt;RequirementsAnalysis\u0026gt; { const prompt = ` Analyze these feature requirements and provide detailed breakdown: Requirements: ${requirements.description} Acceptance Criteria: ${requirements.acceptanceCriteria} Technical Constraints: ${requirements.constraints} Provide: 1. Functional decomposition 2. Technical architecture recommendations 3. Risk assessment 4. Implementation complexity estimation 5. Testing strategy recommendations `; return await this.copilot.chat(prompt); } } Advanced Prompt Engineering Framework: # Advanced prompt engineering framework class AdvancedPromptEngineer: def __init__(self): self.prompt_patterns = self.load_prompt_patterns() self.context_strategies = self.load_context_strategies() def engineer_code_generation_prompt(self, task, context, constraints): \u0026#34;\u0026#34;\u0026#34;Engineer optimized prompts for code generation\u0026#34;\u0026#34;\u0026#34; base_prompt = self.build_base_prompt(task) # Add contextual information context_enhanced = self.enhance_with_context(base_prompt, context) # Apply constraints and requirements constrained_prompt = self.apply_constraints(context_enhanced, constraints) # Optimize for AI model understanding optimized_prompt = self.optimize_for_model(constrained_prompt) return { \u0026#39;prompt\u0026#39;: optimized_prompt, \u0026#39;expected_tokens\u0026#39;: self.estimate_token_usage(optimized_prompt), \u0026#39;quality_score\u0026#39;: self.evaluate_prompt_quality(optimized_prompt) } def build_chain_of_thought_prompt(self, problem, examples=None): \u0026#34;\u0026#34;\u0026#34;Build chain-of-thought prompts for complex problems\u0026#34;\u0026#34;\u0026#34; prompt_template = \u0026#34;\u0026#34;\u0026#34; Problem: {problem} Let\u0026#39;s solve this step by step: 1. Understanding the problem: - What are we trying to achieve? - What are the key components? - What are the constraints? 2. Planning the solution: - What approach should we take? - What are the major steps? - What potential issues might we encounter? 3. Implementation strategy: - How should we structure the code? - What patterns or libraries should we use? - How can we ensure quality and maintainability? 4. Testing and validation: - How can we verify the solution works? - What edge cases should we consider? - How can we measure success? {examples_section} Now, provide the complete solution with detailed explanation. \u0026#34;\u0026#34;\u0026#34; examples_section = \u0026#34;\u0026#34; if examples: examples_section = f\u0026#34;\u0026#34;\u0026#34; Examples for reference: {self.format_examples(examples)} \u0026#34;\u0026#34;\u0026#34; return prompt_template.format( problem=problem, examples_section=examples_section ) Lab 2: Enterprise AI Governance and Compliance Implement comprehensive enterprise AI governance with privacy, security, and compliance controls.\nObjectives:\nDesign enterprise AI governance framework Implement privacy and security controls Create compliance monitoring and reporting Build audit trails and usage analytics Enterprise Governance Implementation:\n# Enterprise AI governance configuration apiVersion: v1 kind: ConfigMap metadata: name: copilot-enterprise-governance data: governance-policy.yaml: | enterprise_ai_policy: version: \u0026#34;1.0\u0026#34; effective_date: \u0026#34;2025-11-01\u0026#34; access_controls: user_tiers: - name: \u0026#34;developer\u0026#34; permissions: - code_completion - chat_assistance - documentation_generation restrictions: - no_sensitive_data_processing - limited_enterprise_features - name: \u0026#34;senior_developer\u0026#34; permissions: - all_developer_permissions - advanced_prompt_engineering - ai_code_review restrictions: - audit_trail_required - name: \u0026#34;architect\u0026#34; permissions: - all_permissions - policy_configuration - usage_analytics restrictions: - full_audit_logging data_protection: classification_levels: - public: \u0026#34;no_restrictions\u0026#34; - internal: \u0026#34;basic_filtering\u0026#34; - confidential: \u0026#34;strict_exclusion\u0026#34; - restricted: \u0026#34;complete_prohibition\u0026#34; content_filtering: patterns: - \u0026#34;password.*=\u0026#34; - \u0026#34;secret.*=\u0026#34; - \u0026#34;api[_-]?key.*=\u0026#34; - \u0026#34;private[_-]?key.*=\u0026#34; file_exclusions: - \u0026#34;*.env*\u0026#34; - \u0026#34;*.key\u0026#34; - \u0026#34;*.pem\u0026#34; - \u0026#34;**/secrets/**\u0026#34; - \u0026#34;**/config/production/**\u0026#34; compliance_requirements: audit_logging: enabled: true retention_period: \u0026#34;7_years\u0026#34; detail_level: \u0026#34;comprehensive\u0026#34; usage_monitoring: real_time_alerts: true policy_violation_reporting: true performance_analytics: true data_sovereignty: geographic_restrictions: [\u0026#34;EU\u0026#34;, \u0026#34;US\u0026#34;] data_residency_requirements: true cross_border_data_transfer: \u0026#34;prohibited\u0026#34; Governance Implementation Code:\n# Enterprise AI governance implementation from typing import Dict, List, Optional from dataclasses import dataclass from enum import Enum import logging class DataClassification(Enum): PUBLIC = \u0026#34;public\u0026#34; INTERNAL = \u0026#34;internal\u0026#34; CONFIDENTIAL = \u0026#34;confidential\u0026#34; RESTRICTED = \u0026#34;restricted\u0026#34; @dataclass class GovernanceDecision: approved: bool restrictions: List[str] audit_required: bool reasoning: str class EnterpriseAIGovernance: def __init__(self, policy_config: Dict): self.policy = policy_config self.audit_logger = logging.getLogger(\u0026#39;ai_governance_audit\u0026#39;) self.usage_monitor = UsageMonitoringService() async def evaluate_ai_request(self, request: AIRequest) -\u0026gt; GovernanceDecision: \u0026#34;\u0026#34;\u0026#34;Evaluate AI request against enterprise governance policies\u0026#34;\u0026#34;\u0026#34; # Classify data sensitivity data_classification = await self.classify_request_data(request) # Check user permissions user_permissions = await self.get_user_permissions(request.user_id) # Evaluate content for policy violations content_analysis = await self.analyze_content(request.prompt) # Apply governance rules decision = self.apply_governance_rules( data_classification, user_permissions, content_analysis ) # Log decision for audit trail await self.log_governance_decision(request, decision) return decision async def classify_request_data(self, request: AIRequest) -\u0026gt; DataClassification: \u0026#34;\u0026#34;\u0026#34;Classify the sensitivity level of the request data\u0026#34;\u0026#34;\u0026#34; content = request.prompt + \u0026#34; \u0026#34; + request.context # Check for restricted patterns if self.contains_restricted_patterns(content): return DataClassification.RESTRICTED # Check for confidential indicators if self.contains_confidential_indicators(content): return DataClassification.CONFIDENTIAL # Check for internal-only content if self.contains_internal_indicators(content): return DataClassification.INTERNAL return DataClassification.PUBLIC def apply_governance_rules(self, classification: DataClassification, permissions: UserPermissions, content_analysis: ContentAnalysis) -\u0026gt; GovernanceDecision: \u0026#34;\u0026#34;\u0026#34;Apply enterprise governance rules to make access decision\u0026#34;\u0026#34;\u0026#34; restrictions = [] approved = True audit_required = False reasoning = [] # Check data classification restrictions if classification == DataClassification.RESTRICTED: approved = False reasoning.append(\u0026#34;Request contains restricted data\u0026#34;) elif classification == DataClassification.CONFIDENTIAL: if not permissions.can_access_confidential: approved = False reasoning.append(\u0026#34;User lacks confidential data access\u0026#34;) else: restrictions.append(\u0026#34;confidential_data_handling\u0026#34;) audit_required = True # Check content violations if content_analysis.policy_violations: approved = False reasoning.extend(content_analysis.policy_violations) # Apply additional restrictions based on user tier if permissions.tier == \u0026#34;developer\u0026#34;: restrictions.extend([ \u0026#34;basic_features_only\u0026#34;, \u0026#34;no_enterprise_data_access\u0026#34; ]) return GovernanceDecision( approved=approved, restrictions=restrictions, audit_required=audit_required, reasoning=\u0026#34;; \u0026#34;.join(reasoning) ) Lab 3: Advanced Prompt Engineering and AI Optimization Create sophisticated prompt engineering frameworks and AI optimization strategies.\nObjectives:\nDevelop advanced prompt engineering techniques Implement AI model optimization strategies Create prompt evaluation and improvement systems Build team prompt engineering guidelines Advanced Prompt Engineering System:\n// Advanced prompt engineering and optimization system interface PromptEngineering { pattern: string; context: ContextStrategy; optimization: OptimizationTechnique; evaluation: EvaluationMetrics; } class AdvancedPromptEngineeringSystem { private promptPatterns: Map\u0026lt;string, PromptPattern\u0026gt;; private contextStrategies: ContextStrategy[]; private evaluationMetrics: EvaluationFramework; constructor() { this.initializePromptPatterns(); this.setupEvaluationFramework(); } async engineerOptimalPrompt(task: DevelopmentTask): Promise\u0026lt;OptimizedPrompt\u0026gt; { // Analyze task requirements const taskAnalysis = await this.analyzeTask(task); // Select appropriate prompt pattern const pattern = this.selectPromptPattern(taskAnalysis); // Build context-rich prompt const contextualPrompt = await this.buildContextualPrompt(pattern, task); // Optimize for AI model performance const optimizedPrompt = await this.optimizeForModel(contextualPrompt); // Evaluate prompt quality const qualityScore = await this.evaluatePromptQuality(optimizedPrompt); return { prompt: optimizedPrompt, pattern: pattern.name, qualityScore, estimatedTokens: this.estimateTokenUsage(optimizedPrompt), expectedOutput: this.predictOutput(optimizedPrompt) }; } private async buildContextualPrompt(pattern: PromptPattern, task: DevelopmentTask): Promise\u0026lt;string\u0026gt; { const contextBuilder = new ContextBuilder(); // Add technical context contextBuilder.addTechnicalContext({ language: task.programmingLanguage, framework: task.framework, architecture: task.architecturalPattern, constraints: task.technicalConstraints }); // Add business context contextBuilder.addBusinessContext({ domain: task.businessDomain, requirements: task.functionalRequirements, goals: task.businessGoals }); // Add quality context contextBuilder.addQualityContext({ performanceRequirements: task.performanceTargets, securityRequirements: task.securityConstraints, maintainabilityGoals: task.maintainabilityTargets }); return pattern.template.render(contextBuilder.build()); } } // Prompt pattern library const PROMPT_PATTERNS = { CHAIN_OF_THOUGHT: { name: \u0026#34;Chain of Thought\u0026#34;, template: ` Task: {task_description} Let\u0026#39;s approach this systematically: 1. Problem Analysis: - What exactly needs to be accomplished? - What are the key challenges? - What constraints must we consider? 2. Solution Design: - What approach will work best? - How should we structure the solution? - What patterns or best practices apply? 3. Implementation Plan: - What are the key components? - How will they interact? - What libraries or frameworks should we use? 4. Quality Assurance: - How will we ensure correctness? - What tests are needed? - How will we handle edge cases? Context: {technical_context} Requirements: {requirements} Constraints: {constraints} Please provide a complete, well-documented solution. `, useCases: [\u0026#34;complex_algorithms\u0026#34;, \u0026#34;system_design\u0026#34;, \u0026#34;architecture_decisions\u0026#34;] }, EXAMPLE_DRIVEN: { name: \u0026#34;Example-Driven Development\u0026#34;, template: ` Create {target_description} following these patterns: Example 1: {example_1} Example 2: {example_2} Key patterns to follow: - {pattern_1} - {pattern_2} - {pattern_3} Now create: {specific_requirements} Ensure the solution: - Follows the established patterns - Maintains consistency with examples - Includes proper error handling - Has comprehensive documentation `, useCases: [\u0026#34;api_development\u0026#34;, \u0026#34;component_libraries\u0026#34;, \u0026#34;design_systems\u0026#34;] }, CONSTRAINT_DRIVEN: { name: \u0026#34;Constraint-Driven Development\u0026#34;, template: ` Develop {solution_description} with these specific constraints: Technical Constraints: {technical_constraints} Performance Constraints: {performance_constraints} Security Constraints: {security_constraints} Business Constraints: {business_constraints} The solution must: 1. Satisfy all constraints explicitly 2. Provide reasoning for design decisions 3. Include fallback strategies for constraint violations 4. Demonstrate constraint validation Context: {context} `, useCases: [\u0026#34;enterprise_systems\u0026#34;, \u0026#34;regulated_industries\u0026#34;, \u0026#34;performance_critical\u0026#34;] } }; Enterprise AI Best Practices AI Development Governance 1. AI Usage Policy Framework:\n# Enterprise AI development policy ai_development_policy: principles: - human_oversight_required - transparency_in_ai_decisions - privacy_by_design - security_first_approach - continuous_monitoring usage_guidelines: approved_use_cases: - code_completion_assistance - documentation_generation - test_case_creation - code_review_assistance - learning_and_skill_development restricted_use_cases: - autonomous_deployment_decisions - security_critical_code_generation - personal_data_processing - regulatory_compliance_decisions quality_standards: code_review: \u0026#34;ai_generated_code_requires_human_review\u0026#34; testing: \u0026#34;comprehensive_testing_mandatory\u0026#34; documentation: \u0026#34;ai_rationale_must_be_documented\u0026#34; monitoring: \u0026#34;usage_patterns_continuously_monitored\u0026#34; 2. Prompt Engineering Standards:\n# Enterprise prompt engineering standards class EnterprisePromptStandards: PROMPT_TEMPLATES = { \u0026#39;code_generation\u0026#39;: { \u0026#39;structure\u0026#39;: [ \u0026#39;context_establishment\u0026#39;, \u0026#39;requirements_specification\u0026#39;, \u0026#39;constraint_definition\u0026#39;, \u0026#39;quality_criteria\u0026#39;, \u0026#39;output_format\u0026#39; ], \u0026#39;required_elements\u0026#39;: [ \u0026#39;programming_language\u0026#39;, \u0026#39;framework_specification\u0026#39;, \u0026#39;error_handling_requirements\u0026#39;, \u0026#39;testing_expectations\u0026#39; ] }, \u0026#39;code_review\u0026#39;: { \u0026#39;focus_areas\u0026#39;: [ \u0026#39;security_vulnerabilities\u0026#39;, \u0026#39;performance_optimizations\u0026#39;, \u0026#39;maintainability_improvements\u0026#39;, \u0026#39;best_practices_compliance\u0026#39; ], \u0026#39;output_format\u0026#39;: \u0026#39;structured_feedback_with_priorities\u0026#39; }, \u0026#39;documentation\u0026#39;: { \u0026#39;requirements\u0026#39;: [ \u0026#39;api_documentation_standards\u0026#39;, \u0026#39;code_comment_guidelines\u0026#39;, \u0026#39;architecture_decision_records\u0026#39;, \u0026#39;user_guide_specifications\u0026#39; ] } } @staticmethod def validate_prompt(prompt: str, template_type: str) -\u0026gt; ValidationResult: \u0026#34;\u0026#34;\u0026#34;Validate prompt against enterprise standards\u0026#34;\u0026#34;\u0026#34; template = EnterprisePromptStandards.PROMPT_TEMPLATES[template_type] validation_results = [] # Check required elements for element in template[\u0026#39;required_elements\u0026#39;]: if not EnterprisePromptStandards._contains_element(prompt, element): validation_results.append(f\u0026#34;Missing required element: {element}\u0026#34;) # Check structure compliance structure_score = EnterprisePromptStandards._evaluate_structure( prompt, template[\u0026#39;structure\u0026#39;] ) return ValidationResult( is_valid=len(validation_results) == 0 and structure_score \u0026gt; 0.8, issues=validation_results, quality_score=structure_score, recommendations=EnterprisePromptStandards._generate_recommendations( prompt, template ) ) Productivity Optimization Strategies AI-Enhanced Development Metrics:\n# AI productivity optimization and measurement class AIProductivityOptimizer: def __init__(self): self.metrics_collector = ProductivityMetricsCollector() self.optimization_engine = OptimizationEngine() async def measure_ai_impact(self, team_id: str, time_period: str) -\u0026gt; ProductivityReport: \u0026#34;\u0026#34;\u0026#34;Measure AI impact on development productivity\u0026#34;\u0026#34;\u0026#34; baseline_metrics = await self.get_baseline_metrics(team_id, time_period) ai_enhanced_metrics = await self.get_ai_enhanced_metrics(team_id, time_period) productivity_gains = self.calculate_productivity_gains( baseline_metrics, ai_enhanced_metrics ) return ProductivityReport( code_generation_speed=productivity_gains.get(\u0026#39;code_generation\u0026#39;, 0), bug_reduction_rate=productivity_gains.get(\u0026#39;bug_reduction\u0026#39;, 0), test_coverage_improvement=productivity_gains.get(\u0026#39;test_coverage\u0026#39;, 0), documentation_quality=productivity_gains.get(\u0026#39;documentation\u0026#39;, 0), learning_acceleration=productivity_gains.get(\u0026#39;skill_development\u0026#39;, 0), overall_satisfaction=productivity_gains.get(\u0026#39;developer_satisfaction\u0026#39;, 0) ) async def optimize_ai_workflows(self, team_metrics: ProductivityMetrics) -\u0026gt; OptimizationPlan: \u0026#34;\u0026#34;\u0026#34;Generate optimization plan for AI-enhanced workflows\u0026#34;\u0026#34;\u0026#34; bottlenecks = self.identify_productivity_bottlenecks(team_metrics) opportunities = self.identify_optimization_opportunities(team_metrics) optimization_plan = OptimizationPlan() for bottleneck in bottlenecks: optimization_plan.add_recommendation( await self.generate_optimization_recommendation(bottleneck) ) for opportunity in opportunities: optimization_plan.add_enhancement( await self.generate_enhancement_recommendation(opportunity) ) return optimization_plan Exam Tips and Strategies Technical Preparation Focus Areas Copilot Mastery:\nCode completion optimization and context management Advanced Chat interactions and prompt engineering CLI integration and command-line productivity Enterprise feature configuration and management Privacy and security policy implementation AI Development Expertise:\nPrompt design principles and optimization techniques Context engineering and model understanding AI-assisted testing and quality assurance Development workflow integration Team adoption and change management strategies Enterprise Governance Proficiency:\nPrivacy policy configuration and data protection Compliance framework implementation Usage monitoring and analytics Security and intellectual property protection Enterprise deployment and administration Exam Day Strategy Time Management:\nAllocate 2 minutes per question average Focus on practical AI integration scenarios Use elimination techniques for complex prompting questions Reserve time for enterprise governance review Question Approach:\nIdentify the AI development domain being tested Consider enterprise vs individual developer implications Evaluate privacy and security considerations Apply hands-on Copilot experience Select the most productive and secure solution Common Exam Topics:\nPrompt engineering patterns and optimization Enterprise privacy and security configuration AI development workflow integration Testing strategies with AI assistance Governance and compliance requirements Official Study Resources GitHub Documentation GitHub Copilot Documentation Copilot Chat Guide Copilot CLI Documentation Enterprise Copilot Administration Microsoft Learn Paths Introduction to GitHub Copilot Using GitHub Copilot with JavaScript GitHub Copilot Fundamentals - Understand the AI pair programmer Practice Platforms GitHub Copilot Free Trial GitHub Codespaces GitHub Skills - AI Development Additional Practice Resources AI Development Communities GitHub Copilot Community AI-Powered Development Forums Copilot Best Practices Hands-On Practice Use Copilot for personal and professional projects Practice with different programming languages and frameworks Experiment with advanced prompt engineering techniques Build AI-enhanced development workflows Final Preparation Checklist Technical Skills Validation Can effectively use Copilot across multiple IDEs and environments Understand prompt engineering principles and advanced techniques Master Copilot Chat for complex development assistance Know enterprise privacy and security configuration Can integrate AI tools into complete development workflows Understand AI model capabilities and limitations Know testing strategies with AI assistance Can implement enterprise governance and compliance Understand intellectual property and data protection Know usage monitoring and optimization techniques Can train teams on AI development best practices Understand ethical AI development principles Exam Readiness Assessment Completed all study plan phases Finished hands-on AI development laboratories Practiced with enterprise AI scenarios Reviewed official documentation thoroughly Taken practice assessments consistently Confident in all AI development domains Registered for exam date Prepared exam day logistics Your Complete GitHub Certification Journey Congratulations! You\u0026rsquo;ve reached the end of the comprehensive GitHub Certification Journey series. Let\u0026rsquo;s celebrate what you\u0026rsquo;ve accomplished:\nYour Certification Pathway üéØ ‚úÖ Part 1: GitHub Foundations (GH-900) - Mastered Git and GitHub collaboration\n‚úÖ Part 2: GitHub Actions (GH-200) - Conquered CI/CD automation\n‚úÖ Part 3: GitHub Advanced Security (GH-500) - Secured the software supply chain\n‚úÖ Part 4: GitHub Administration (GH-100) - Governed GitHub at enterprise scale\n‚úÖ Part 5: GitHub Copilot (GH-500) - Embraced AI-powered development\nCareer Advancement Opportunities With all five GitHub certifications, you\u0026rsquo;re positioned for exciting career opportunities:\nüöÄ Technical Leadership Roles:\nSenior Platform Engineer: Lead enterprise GitHub implementations DevSecOps Architect: Design secure, automated development pipelines AI Development Lead: Champion AI-enhanced development practices Technical Consultant: Provide GitHub expertise to enterprises üíº Strategic Positions:\nDeveloper Experience Manager: Optimize developer productivity and tooling Cloud Architecture Director: Design scalable, secure cloud-native solutions Digital Transformation Lead: Guide organisations through DevOps modernisation Technology Evangelist: Share knowledge and best practices across the industry Continuing Your Learning Journey Advanced Certifications:\nMicrosoft Azure DevOps Engineer Expert AWS DevOps Engineer Professional Certified Kubernetes Administrator (CKA) HashiCorp Terraform Associate Emerging Technologies:\nMachine Learning Engineering Cloud Security Specialisation Platform Engineering Excellence Sustainable Software Development Conclusion The GH-300 GitHub Copilot certification represents the cutting edge of software development, validating your expertise in AI-powered development workflows and positioning you at the forefront of the industry\u0026rsquo;s transformation. By following this comprehensive study guide, completing the hands-on AI development laboratories, and practising with real-world scenarios, you\u0026rsquo;ll be well-prepared to pass the exam and excel in AI-enhanced development.\nRemember that AI-powered development is rapidly evolving, with new capabilities, tools, and best practices emerging regularly. Continue learning about prompt engineering techniques, enterprise AI governance, and responsible AI development practices. The skills you develop preparing for this certification will serve you throughout your career as AI becomes increasingly integrated into software development.\nReady to embrace the future of development? ü§ñ\nStart your GitHub Copilot mastery journey today, and join the ranks of certified AI-enhanced developers who are shaping the future of software engineering worldwide.\nThis guide concludes the GitHub Certification Journey series. Previous: GitHub Administration (GH-100)\nHave questions about GitHub Copilot certification or the complete certification journey? Connect with me on LinkedIn or GitHub for guidance and support.\nüéâ Congratulations on completing the entire GitHub Certification Journey! You now have the knowledge and skills to excel across all aspects of modern GitHub usage, from foundations to AI-powered development. Best of luck with your certifications and career advancement!\n","date":"2025-08-29T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-300/cover_hu_8a7824488436583.jpg","image":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-300/cover_hu_7a75bab8fecfc47d.jpg","permalink":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-300/","title":"ü§ñ GitHub Copilot Certification Guide (GH-300) - AI Development Mastery","webpImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-300/cover_hu_c1ca874e4a5dee15.webp"},{"content":"Dependabot is the easiest way to keep dependencies current and secure in your GitHub repositories. It can: alert on vulnerabilities, open PRs to fix them, and keep versions fresh with scheduled updates. This guide shows how to enable Dependabot, add a robust dependabot.yml, and include small optimisations for specific ecosystems.\nWhat you‚Äôll set up Enable Dependabot alerts, security updates, and version updates. Create and tune a .github/dependabot.yml that covers your package managers. Add optional Java metadata to improve PR quality. Enable Dependabot in your repository Use the repository UI:\nGo to Settings. Under Security, open Advanced Security. Enable: Dependabot alerts, Dependabot security updates, and Dependabot version updates. If you enable version updates from the UI, GitHub adds a default .github/dependabot.yml you can edit.\nCreate dependabot.yml Dependabot looks for a .github/dependabot.yml at the repo root. Here‚Äôs a practical starting file that covers common ecosystems. Adjust directories and schedules to match your repo layout.\nversion: 2 updates: # JavaScript/TypeScript (npm or pnpm) - package-ecosystem: \u0026#34;npm\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; # daily | weekly | monthly open-pull-requests-limit: 10 labels: [\u0026#34;dependencies\u0026#34;] # GitHub Actions (workflow uses action@version syntax only) - package-ecosystem: \u0026#34;github-actions\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; # Docker (Dockerfiles and image tags in k8s manifests/Helm charts) - package-ecosystem: \u0026#34;docker\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; # Maven (Java) - package-ecosystem: \u0026#34;maven\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; # Gradle (Java/Kotlin) - package-ecosystem: \u0026#34;gradle\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; # Terraform - package-ecosystem: \u0026#34;terraform\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; info For pnpm and some others, you still use package-ecosystem: \u0026quot;npm\u0026quot; (see Supported ecosystems). Poetry and pipenv use the pip YAML value (see Supported ecosystems). If you have multiple package roots (for example, monorepos), add one updates entry per directory. Useful options to consider ignore: ignore specific dependencies or version ranges. allow: restrict updates to a list of dependencies. commit-message: add a prefix such as deps:. target-branch: direct PRs to a non-default branch. rebase-strategy: control how rebases are handled. See the Dependabot options reference for the full list.\nEcosystem specifics and caveats GitHub Actions Dependabot only updates actions referenced with the GitHub repository syntax: owner/repo@vX (for example, actions/checkout@v5). Local action references like ./.github/actions/foo and container actions via docker:// are ignored. Docker and Kubernetes/Helm Dependabot can add metadata (release notes/changelogs) to Docker PRs when images include the org.opencontainers.image.source label in their Dockerfile and matching tags. It can update image tags inside Kubernetes manifests and Helm charts when you configure a docker entry for those directories. Java (Maven and Gradle) Dependabot uses dependency metadata to enrich PRs. For libraries you publish, add these to your pom.xml so Dependabot can link to release notes and issues.\n\u0026lt;project\u0026gt; \u0026lt;url\u0026gt;https://github.com/OWNER/REPOSITORY\u0026lt;/url\u0026gt; \u0026lt;scm\u0026gt; \u0026lt;url\u0026gt;https://github.com/OWNER/REPOSITORY\u0026lt;/url\u0026gt; \u0026lt;/scm\u0026gt; \u0026lt;issueManagement\u0026gt; \u0026lt;url\u0026gt;https://github.com/OWNER/REPOSITORY/issues\u0026lt;/url\u0026gt; \u0026lt;/issueManagement\u0026gt; \u0026lt;/project\u0026gt; Gradle: Dependabot updates build.gradle, build.gradle.kts, and standard version catalogs (gradle/libs.versions.toml). Maven: Dependabot updates pom.xml files. If metadata is missing, PRs are still created, but without rich links.\nTerraform Dependabot updates providers and modules (including OCI/registry sources). Private registries are supported with proper configuration. Dev containers Use package-ecosystem: \u0026quot;devcontainers\u0026quot; to keep Features up to date in devcontainer.json and lockfiles. Private registries and private dependencies Dependabot can access private package registries and private GitHub repositories, but you must configure credentials in dependabot.yml and/or grant org access. See ‚ÄúConfiguring access to private registries for Dependabot‚Äù and ‚ÄúManaging security and analysis settings for your organisation‚Äù in the official docs.\nTip: Keep credentials in GitHub secrets and reference them from dependabot.yml rather than using plaintext.\nHow Dependabot runs Alerts: appear on the repository Security tab when the dependency graph detects known vulnerabilities. Security updates: PRs that bump vulnerable versions to a patched release. Version updates: PRs on a schedule to keep you current. After you commit dependabot.yml, Dependabot will scan and begin creating PRs based on your schedule. You can merge, close, or tweak the config and rerun.\nTroubleshooting essentials Dependabot must be able to resolve all dependencies. If manifests reference private sources, provide access. For Actions, only owner/repo@version references are updatable. For Docker metadata in PRs, ensure images include the org.opencontainers.image.source label and matching tags. Some ecosystems (for example, Gradle security updates) may rely on dependency submission; check the ecosystem notes in Supported ecosystems. Advanced configuration and operations Managing PRs for dependency updates: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/managing-pull-requests-for-dependency-updates Automating Dependabot with GitHub Actions: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/automating-dependabot-with-github-actions Keeping your actions up to date with Dependabot: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/keeping-your-actions-up-to-date-with-dependabot Configuring access to private registries: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/configuring-access-to-private-registries-for-dependabot Guidance for private registry configuration: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/guidance-for-the-configuration-of-private-registries-for-dependabot Configuring multi-ecosystem updates: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/configuring-multi-ecosystem-updates About Dependabot on GitHub Actions runners: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/about-dependabot-on-github-actions-runners Running on self-hosted runners with ARC: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/setting-dependabot-to-run-on-self-hosted-runners-using-arc Running on GitHub-hosted runners with Azure VNET: https://docs.github.com/en/code-security/dependabot/working-with-dependabot/setting-dependabot-to-run-on-github-hosted-runners-using-vnet Recap Enable alerts, security updates, and version updates. Add a single .github/dependabot.yml and include one updates entry per ecosystem and directory. For Java, add project metadata in pom.xml to improve PRs. Review supported ecosystems for caveats, private registries, and special cases. Happy patching and stay secure.\nReferences Dependabot quickstart guide Dependabot supported ecosystems and repositories Optimising Java packages for Dependabot updates ","date":"2025-08-25T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/dependabot-getting-started/cover_hu_de39d59c19d5bea9.jpg","image":"https://quintelier.dev/posts/2025/08/dependabot-getting-started/cover_hu_b788900d6e1c3484.jpg","permalink":"https://quintelier.dev/posts/2025/08/dependabot-getting-started/","title":"Getting started with Dependabot","webpImage":"https://quintelier.dev/posts/2025/08/dependabot-getting-started/cover_hu_f520ba33d3a7c2f3.webp"},{"content":"Mastering Platform Engineering: From Golden Paths to Selling Your MVP Platform engineering is no longer a buzzword: it‚Äôs a proven practice that bridges the gap between developers and operations by building Internal Developer Platforms (IDPs). As Gartner predicts, by 2026, 80% of large software engineering organisations will establish platform engineering teams as internal providers of reusable services, components, and tools for application delivery.\nThe Platform Engineering Certified Practitioner course equips engineers, architects, and technology leaders with the knowledge and hands-on frameworks to design, build, and scale successful platforms inside their organisations.\nWhy This Course Matters Modern DevOps setups often struggle with:\nHigh cognitive load for developers Long waiting times for infrastructure Low productivity and shadow operations Lack of standardisation and automation Platform engineering addresses these pain points by creating golden paths, standardising infrastructure, and enabling self-service, while treating the platform as a product (CNCF Maturity Model).\nBy the end of the course, you‚Äôll be able to:\nBuild an Internal Developer Platform from scratch Apply best practices such as abstraction layers, golden paths, and everything-as-code Launch a Minimum Viable Platform (MVP) in weeks, not years Secure buy-in from executives and stakeholders by selling your MVP Inside the Course: Module Highlights 1. Introduction to Platform Engineering Understand the evolution from DevOps to platform engineering, the importance of treating platforms as products, and how IDPs enable ‚Äútrue DevOps‚Äù.\n2. Building an Internal Developer Platform Explore platform team composition, maturity models, and reference architectures for multi-cloud environments. Learn why platforms should start from the backend, not the portal.\n3. Platform Tooling 101 Dive into the five planes of platform architecture: developer control, integration and delivery, resource, monitoring and logging, and security. Get hands-on with frameworks such as CNOE and Pocket IDP.\n4. The Art of Building Golden Paths Learn how to design end-to-end developer workflows that reduce cognitive load, waiting times, and ticket operations. Golden paths automate manual tasks and embed security by design.\n5. Finding the Right Abstractions Discover how frontend (portals, declarative configs) and backend (orchestrators, infrastructure as code) abstractions balance usability with flexibility. Avoid golden cages by shielding complexity without removing context.\n6. Infrastructure Platform Engineering Deep dive into infrastructure orchestration, vending machine-style provisioning, and applying ‚Äúeverything as code‚Äù for scalable platforms.\n7. How to Build Your Minimum Viable Platform (MVP) Practical framework for building an MVP in 8 weeks. Learn to measure onboarding time, service creation time, and complexity reduction as success metrics.\n8. How to Sell Your MVP to Stakeholders Winning stakeholder buy-in is crucial. This module shows how to frame value for:\nDevelopers: self-service, golden paths I\u0026amp;O teams: standardisation, cost reduction Security: guardrails, automation Executives: ROI, business case Who Should Take This Course? Platform Engineers: Build advanced knowledge and frameworks to succeed in your role. DevOps \u0026amp; SREs: Learn how to evolve from fragmented toolchains to structured platforms. Engineering Leaders \u0026amp; Architects: Understand how to fund, scale, and secure stakeholder buy-in for your platform initiative. Certification and Beyond The course concludes with a certification exam (60 minutes, multiple choice, two attempts). Passing it validates you as a Platform Engineering Certified Practitioner: a credential increasingly in demand as organisations scale their digital capabilities.\nGraduates are also invited to join the community, attend PlatformCon, and continue their journey into the Certified Professional track.\nRecap and Next Steps The Platform Engineering Certified Practitioner course is more than theory: it‚Äôs a playbook for real-world platform success. Whether you‚Äôre just starting with platform engineering or looking to scale your MVP into a production-ready platform, this programme will give you the tools, frameworks, and credibility to make it happen.\nüëâ Ready to shape the future of software delivery? Start your journey with the Platform Engineering Certified Practitioner course today.\n","date":"2025-08-22T09:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/platform-engineering-certified-practitioner/cover_hu_f847a3269b824ac0.jpg","image":"https://quintelier.dev/posts/2025/08/platform-engineering-certified-practitioner/cover_hu_3a838b7a487d48c6.jpg","permalink":"https://quintelier.dev/posts/2025/08/platform-engineering-certified-practitioner/","title":"Platform Engineering Course Guide üöÄ","webpImage":"https://quintelier.dev/posts/2025/08/platform-engineering-certified-practitioner/cover_hu_f24338c079f62be.webp"},{"content":"Introduction Welcome to Part 3 of the GitHub Certification Journey! üéØ\nAfter mastering GitHub Foundations (GH-900) and GitHub Administration (GH-100), you\u0026rsquo;re ready to dive deep into GitHub Actions - the powerhouse automation platform that transforms how teams build, test, and deploy software. The GH-200 GitHub Actions certification validates your ability to design, implement, and manage CI/CD pipelines at enterprise scale.\nThis comprehensive guide provides everything needed to pass the GH-200 exam and become a GitHub Actions expert. Whether you\u0026rsquo;re a DevOps engineer, platform team member, or developer looking to automate workflows, this preparation roadmap will take you from YAML basics to advanced enterprise automation patterns.\nCertification Overview About GH-200 GitHub Actions Certification Name: GitHub Actions Exam Code: GH-200 Duration: 150 minutes Question Count: ~65 questions Passing Score: 700/1000 (approximately 70%) Cost: $99 USD Validity: 3 years from certification date Prerequisites: GitHub Foundations (GH-900) and GitHub Administration (GH-100) recommended Who Should Take This Exam DevOps engineers implementing CI/CD pipelines Platform engineering teams building developer experiences Software architects designing automation strategies Release managers optimising deployment workflows Security engineers implementing DevSecOps practices Technical leads scaling development processes Exam Domains Breakdown The GH-200 exam covers five main domains with specific weightings:\nDomain 1: Author and Maintain Workflows (40%) Core Competencies:\nCreating workflow files and understanding triggers Implementing jobs, steps, and actions Managing workflow execution and debugging Using expressions, contexts, and functions Implementing matrix strategies and conditional logic Key Skills:\nYAML syntax mastery for GitHub Actions Event-driven workflow design Action selection and configuration Workflow optimisation for performance Error handling and retry mechanisms Domain 2: Consume Actions (20%) Core Competencies:\nFinding and evaluating actions from GitHub Marketplace Using official GitHub-maintained actions Implementing third-party actions securely Creating composite actions for reusability Managing action versions and dependencies Key Skills:\nAction discovery and selection criteria Semantic versioning for actions Security assessment of third-party actions Action composition and abstraction Dependency management strategies Domain 3: Author and Maintain Actions (25%) Core Competencies:\nCreating JavaScript actions Building Docker container actions Developing composite actions Publishing actions to GitHub Marketplace Implementing action metadata and documentation Key Skills:\nAction development lifecycle JavaScript/TypeScript for actions Docker containerisation for actions Action inputs, outputs, and branding Testing and validation strategies Domain 4: Manage GitHub Actions for the Enterprise (15%) Core Competencies:\nConfiguring self-hosted runners Managing runner groups and security Implementing enterprise-wide policies Monitoring usage and performance Scaling runner infrastructure Key Skills:\nEnterprise runner architecture Security policies and compliance Cost management and optimisation Monitoring and observability Governance and control frameworks Complete Study Plan Phase 1: Foundation Building (Weeks 1-2) Week 1: YAML and Workflow Basics Master YAML syntax and GitHub Actions structure Understand workflow triggers and event types Practice basic job and step configurations Learn about runners and execution environments Daily Tasks:\nCreate simple workflows with different triggers Practice YAML syntax and validation Experiment with job dependencies and conditions Explore GitHub-hosted vs self-hosted runners Hands-On Labs:\n# Basic workflow structure name: CI Pipeline on: push: branches: [main, develop] pull_request: branches: [main] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Setup Node.js uses: actions/setup-node@v4 with: node-version: \u0026#39;18\u0026#39; - name: Install dependencies run: npm install - name: Run tests run: npm test Week 2: Actions and Marketplace Explore GitHub Marketplace for actions Learn to evaluate action quality and security Practice using official GitHub actions Understand action versioning strategies Practice Projects:\nBuild a multi-language test pipeline Implement code quality checks Set up automated dependency updates Create deployment workflows Phase 2: Intermediate Automation (Weeks 3-4) Week 3: Advanced Workflow Patterns Master matrix strategies for parallel execution Implement conditional workflows and job dependencies Learn about workflow environments and protection rules Practice with secrets and environment variables Advanced Examples:\n# Matrix strategy example strategy: matrix: os: [ubuntu-latest, windows-latest, macos-latest] node-version: [16, 18, 20] exclude: - os: windows-latest node-version: 16 # Conditional job execution if: github.event_name == \u0026#39;push\u0026#39; \u0026amp;\u0026amp; contains(github.ref, \u0026#39;refs/tags/\u0026#39;) Week 4: Custom Actions Development Learn JavaScript action development Practice Docker container actions Create composite actions for reusability Understand action testing and publishing Action Development Labs:\nBuild a custom notification action Create a deployment status action Develop a security scanning action Publish an action to the marketplace Phase 3: Enterprise Mastery (Weeks 5-6) Week 5: Self-Hosted Runners Set up and configure self-hosted runners Implement runner groups and access controls Practice scaling and load balancing Learn monitoring and maintenance Infrastructure Labs:\nDeploy runners on different cloud platforms Configure auto-scaling runner pools Implement security hardening Set up monitoring and alerting Week 6: Enterprise Governance Master organisation-wide policies Implement workflow approval processes Practice cost monitoring and optimisation Learn compliance and audit features Governance Projects:\nCreate organisation workflow templates Implement security policies Set up usage monitoring dashboards Configure approval workflows Phase 4: Exam Preparation (Week 7) Final Review Topics:\nWorkflow debugging and troubleshooting Performance optimisation techniques Security best practices Enterprise scaling patterns Mock Exam Practice:\nTake practice tests daily Review GitHub Actions documentation Practice hands-on scenarios Focus on weak areas Hands-On Laboratory Exercises Lab 1: Multi-Environment CI/CD Pipeline Create a comprehensive pipeline that builds, tests, and deploys across multiple environments.\nObjectives:\nImplement branch-based deployment strategies Use environment protection rules Practice secret management Configure approval workflows Implementation:\nname: Multi-Environment Pipeline on: push: branches: [main, develop, feature/*] pull_request: branches: [main, develop] env: REGISTRY: ghcr.io IMAGE_NAME: ${{ github.repository }} jobs: test: runs-on: ubuntu-latest strategy: matrix: node-version: [16, 18, 20] steps: - uses: actions/checkout@v4 - name: Setup Node.js ${{ matrix.node-version }} uses: actions/setup-node@v4 with: node-version: ${{ matrix.node-version }} cache: \u0026#39;npm\u0026#39; - name: Install dependencies run: npm ci - name: Run tests run: npm run test:coverage - name: Upload coverage reports uses: codecov/codecov-action@v3 with: token: ${{ secrets.CODECOV_TOKEN }} build: needs: test runs-on: ubuntu-latest outputs: image-tag: ${{ steps.meta.outputs.tags }} image-digest: ${{ steps.build.outputs.digest }} steps: - uses: actions/checkout@v4 - name: Log in to Container Registry uses: docker/login-action@v3 with: registry: ${{ env.REGISTRY }} username: ${{ github.actor }} password: ${{ secrets.GITHUB_TOKEN }} - name: Extract metadata id: meta uses: docker/metadata-action@v5 with: images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }} tags: | type=ref,event=branch type=ref,event=pr type=sha,prefix={{branch}}- - name: Build and push Docker image id: build uses: docker/build-push-action@v5 with: context: . push: true tags: ${{ steps.meta.outputs.tags }} labels: ${{ steps.meta.outputs.labels }} deploy-dev: if: github.ref == \u0026#39;refs/heads/develop\u0026#39; needs: build runs-on: ubuntu-latest environment: development steps: - name: Deploy to Development run: | echo \u0026#34;Deploying ${{ needs.build.outputs.image-tag }} to development\u0026#34; # Add your deployment logic here deploy-staging: if: github.ref == \u0026#39;refs/heads/main\u0026#39; needs: build runs-on: ubuntu-latest environment: staging steps: - name: Deploy to Staging run: | echo \u0026#34;Deploying ${{ needs.build.outputs.image-tag }} to staging\u0026#34; # Add your staging deployment logic here deploy-production: if: startsWith(github.ref, \u0026#39;refs/tags/v\u0026#39;) needs: build runs-on: ubuntu-latest environment: production steps: - name: Deploy to Production run: | echo \u0026#34;Deploying ${{ needs.build.outputs.image-tag }} to production\u0026#34; # Add your production deployment logic here Lab 2: Custom JavaScript Action Build a reusable action that posts deployment status to Slack.\nObjectives:\nLearn JavaScript action structure Practice input/output handling Implement external API integration Understand action packaging Action Structure:\n// action.yml name: \u0026#39;Deployment Notifier\u0026#39; description: \u0026#39;Send deployment notifications to Slack\u0026#39; inputs: slack-webhook-url: description: \u0026#39;Slack webhook URL\u0026#39; required: true deployment-status: description: \u0026#39;Deployment status (success, failure, pending)\u0026#39; required: true environment: description: \u0026#39;Target environment\u0026#39; required: true service-name: description: \u0026#39;Name of the deployed service\u0026#39; required: true outputs: message-ts: description: \u0026#39;Slack message timestamp\u0026#39; runs: using: \u0026#39;node20\u0026#39; main: \u0026#39;dist/index.js\u0026#39; branding: icon: \u0026#39;bell\u0026#39; color: \u0026#39;blue\u0026#39; // src/main.js const core = require(\u0026#39;@actions/core\u0026#39;); const github = require(\u0026#39;@actions/github\u0026#39;); const { IncomingWebhook } = require(\u0026#39;@slack/webhook\u0026#39;); async function run() { try { // Get inputs const webhookUrl = core.getInput(\u0026#39;slack-webhook-url\u0026#39;); const status = core.getInput(\u0026#39;deployment-status\u0026#39;); const environment = core.getInput(\u0026#39;environment\u0026#39;); const serviceName = core.getInput(\u0026#39;service-name\u0026#39;); // Create Slack webhook client const webhook = new IncomingWebhook(webhookUrl); // Determine status color and emoji const statusConfig = { success: { color: \u0026#39;#36a64f\u0026#39;, emoji: \u0026#39;‚úÖ\u0026#39; }, failure: { color: \u0026#39;#ff0000\u0026#39;, emoji: \u0026#39;‚ùå\u0026#39; }, pending: { color: \u0026#39;#ffaa00\u0026#39;, emoji: \u0026#39;‚è≥\u0026#39; } }; const config = statusConfig[status] || statusConfig.pending; // Prepare message const message = { attachments: [{ color: config.color, title: `${config.emoji} Deployment ${status.toUpperCase()}`, fields: [ { title: \u0026#39;Service\u0026#39;, value: serviceName, short: true }, { title: \u0026#39;Environment\u0026#39;, value: environment, short: true }, { title: \u0026#39;Repository\u0026#39;, value: github.context.repo.repo, short: true }, { title: \u0026#39;Commit\u0026#39;, value: github.context.sha.substring(0, 8), short: true } ], footer: \u0026#39;GitHub Actions\u0026#39;, ts: Math.floor(Date.now() / 1000) }] }; // Send notification const result = await webhook.send(message); // Set output core.setOutput(\u0026#39;message-ts\u0026#39;, result.ts); core.info(`Notification sent successfully`); } catch (error) { core.setFailed(error.message); } } run(); Lab 3: Self-Hosted Runner Setup Deploy and configure self-hosted runners for enterprise scenarios.\nObjectives:\nSet up runners on different platforms Configure runner groups and permissions Implement auto-scaling Practice security hardening Runner Configuration Script:\n#!/bin/bash # Setup script for Ubuntu self-hosted runner set -e # Configuration RUNNER_VERSION=\u0026#34;2.311.0\u0026#34; RUNNER_USER=\u0026#34;actions-runner\u0026#34; RUNNER_HOME=\u0026#34;/home/$RUNNER_USER\u0026#34; ORGANIZATION=\u0026#34;your-org\u0026#34; RUNNER_GROUP=\u0026#34;production\u0026#34; # Create runner user sudo useradd -m -s /bin/bash $RUNNER_USER # Download and extract runner cd $RUNNER_HOME curl -o actions-runner-linux-x64.tar.gz \\ -L https://github.com/actions/runner/releases/download/v${RUNNER_VERSION}/actions-runner-linux-x64-${RUNNER_VERSION}.tar.gz tar xzf actions-runner-linux-x64.tar.gz sudo chown -R $RUNNER_USER:$RUNNER_USER $RUNNER_HOME # Install dependencies sudo $RUNNER_HOME/bin/installdependencies.sh # Configure runner (requires registration token) sudo -u $RUNNER_USER $RUNNER_HOME/config.sh \\ --url https://github.com/$ORGANIZATION \\ --token $RUNNER_TOKEN \\ --runnergroup $RUNNER_GROUP \\ --name $(hostname) \\ --labels production,linux,x64 \\ --work _work \\ --unattended # Install as service sudo $RUNNER_HOME/svc.sh install $RUNNER_USER sudo $RUNNER_HOME/svc.sh start # Configure log rotation sudo tee /etc/logrotate.d/github-runner \u0026gt; /dev/null \u0026lt;\u0026lt;EOF $RUNNER_HOME/_diag/*.log { daily rotate 30 compress delaycompress missingok notifempty create 0644 $RUNNER_USER $RUNNER_USER } EOF echo \u0026#34;Self-hosted runner configured successfully\u0026#34; Exam Tips and Strategies Technical Preparation Master These YAML Patterns:\nWorkflow triggers and event filtering Job dependencies and conditional execution Matrix strategies and exclusions Environment variables and contexts Secrets and security configurations Action Development Focus:\nJavaScript action architecture Input validation and error handling Output generation and consumption Docker action best practices Composite action design patterns Enterprise Scenarios:\nRunner group configuration Organisation-wide policy implementation Cost optimisation strategies Security compliance patterns Monitoring and observability Exam Day Strategy Time Management:\nAllocate 1.5 minutes per question average Flag difficult questions for review Complete all questions before detailed review Use remaining time for flagged items Question Approach:\nRead the entire question carefully Identify key requirements and constraints Eliminate obviously incorrect answers Apply practical GitHub Actions knowledge Choose the most appropriate solution Common Pitfalls to Avoid:\nConfusing workflow and action syntax Misunderstanding trigger event details Overlooking security implications Ignoring performance considerations Missing enterprise-specific features Official Study Resources Microsoft Learn Paths GitHub Actions Fundamentals Build and Deploy Applications with GitHub Actions Secure DevOps with GitHub GitHub Documentation GitHub Actions Documentation Workflow Syntax Reference GitHub Actions Marketplace Self-hosted Runners Guide Practice Platforms GitHub Skills GitHub Actions Examples Repository GitHub Community Discussions Additional Practice Resources Community Examples Awesome Actions GitHub Actions Toolkit Security Best Practices Real-World Projects Contribute to open-source projects using GitHub Actions Build CI/CD pipelines for personal projects Create and publish custom actions Set up monitoring and alerting workflows Final Preparation Checklist Technical Skills Validation Can create workflows with multiple triggers Understand job dependencies and conditions Master matrix strategies and parallel execution Know how to use official GitHub actions Can evaluate and use marketplace actions Understand action versioning and security Can create JavaScript actions from scratch Know Docker action development process Can build composite actions for reusability Understand self-hosted runner setup Know enterprise governance features Can implement security policies Understand cost optimisation strategies Know monitoring and observability practices Exam Readiness Assessment Completed all study plan phases Finished hands-on laboratory exercises Practiced with real-world scenarios Reviewed official documentation thoroughly Taken practice tests consistently Confident in all exam domains Registered for exam date Prepared exam day logistics Next Steps in Your Journey After achieving GH-200 certification, consider these advanced paths:\nAdvanced Certifications GitHub Advanced Security (GH-300): Security-focused GitHub expertise GitHub Enterprise (GH-400): Large-scale GitHub administration Partner Program: GitHub consultant certification Career Advancement Platform Engineering: Build developer experience platforms DevOps Architecture: Design enterprise automation strategies Site Reliability Engineering: Implement operational excellence Security Engineering: Lead DevSecOps transformations Conclusion The GH-200 GitHub Actions certification validates your expertise in modern CI/CD automation and positions you as a key contributor to any development team. By following this comprehensive study guide, completing the hands-on laboratories, and practising with real-world scenarios, you\u0026rsquo;ll be well-prepared to pass the exam and excel in your DevOps career.\nRemember that GitHub Actions is rapidly evolving, so continue learning and experimenting with new features. The skills you develop preparing for this certification will serve you throughout your career in modern software development and platform engineering.\nReady to automate everything? üöÄ\nStart your GitHub Actions mastery journey today, and join the ranks of certified DevOps professionals who are transforming how software is built, tested, and deployed worldwide.\nThis guide is part of the GitHub Certification Journey series. Previous: GitHub Foundations (GH-900) | Next: GitHub Advanced Security (GH-300) - Coming Soon\nHave questions about GitHub Actions certification? Connect with me on LinkedIn or GitHub for guidance and support.\n","date":"2025-08-22T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-200/cover_hu_43fccf439a2cf5d4.jpg","image":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-200/cover_hu_44b75630919c09d.jpg","permalink":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-200/","title":"üöÄ GitHub Actions Certification Guide (GH-200) - Complete Prep","webpImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-200/cover_hu_af8e7c8cea979a68.webp"},{"content":"Introduction Welcome to Part 2 of the GitHub Certification Journey! üè¢\nAfter mastering GitHub Foundations (GH-900), you\u0026rsquo;re ready to tackle GitHub Administration - the comprehensive enterprise administration certification that validates your ability to manage, govern, and administer GitHub at organisational scale. The GH-100 GitHub Administration certification demonstrates your expertise in enterprise GitHub management, user governance, and organisational administration.\nThis comprehensive guide provides everything needed to pass the GH-100 exam and become a GitHub Enterprise administrator. Whether you\u0026rsquo;re an IT administrator, enterprise architect, or platform engineer responsible for GitHub governance, this preparation roadmap will take you from basic administrative knowledge to advanced enterprise GitHub management.\nCertification Overview About GH-100 GitHub Administration Certification Name: GitHub Administration Exam Code: GH-100 Duration: 150 minutes Question Count: ~65 questions Passing Score: 700/1000 (approximately 70%) Cost: $99 USD Validity: 3 years from certification date Prerequisites: GitHub Foundations (GH-900) and enterprise administration experience recommended Who Should Take This Exam Enterprise GitHub administrators IT administrators managing developer platforms Platform engineers implementing GitHub governance DevOps engineers responsible for GitHub enterprise features Security administrators implementing GitHub policies Technical leads managing GitHub at organisational scale Exam Domains Breakdown The GH-100 exam covers six main domains with specific weightings:\nDomain 1: Support Enterprise Administration (20%) Core Competencies:\nManaging GitHub Enterprise Server and GitHub Enterprise Cloud Understanding enterprise billing and licensing models Implementing enterprise-wide policies and configurations Managing enterprise accounts and organisations Troubleshooting enterprise-level issues Key Skills:\nEnterprise account hierarchy management Billing administration and cost optimisation Enterprise policy implementation Multi-organisation governance Enterprise support and escalation procedures Domain 2: Manage User Identities and Access (25%) Core Competencies:\nImplementing SAML/SCIM authentication and provisioning Managing user lifecycle and access controls Configuring external identity providers Implementing role-based access control (RBAC) Managing team and organisation memberships Key Skills:\nSAML SSO configuration and troubleshooting SCIM user provisioning automation Identity provider integration (Azure AD, Okta, etc.) User access reviews and compliance Team and permission management Domain 3: Enable Secure Software Development (20%) Core Competencies:\nImplementing organisation-wide security policies Managing GitHub Advanced Security features Configuring branch protection and repository policies Implementing compliance and audit requirements Managing security reporting and monitoring Key Skills:\nSecurity policy enforcement across organisations Branch protection rule management Compliance framework implementation Security audit and reporting Vulnerability management processes Domain 4: Facilitate Collaboration and Communication (15%) Core Competencies:\nManaging GitHub communication features Implementing project management and collaboration tools Configuring notification and integration systems Managing GitHub Apps and marketplace applications Optimising developer workflow and experience Key Skills:\nProject and issue management configuration Integration and webhook management GitHub Apps administration Developer experience optimisation Communication policy implementation Domain 5: Manage GitHub Actions (15%) Core Competencies:\nAdministering GitHub Actions at enterprise scale Managing self-hosted runners and runner groups Implementing Actions policies and security controls Managing workflow permissions and secrets Monitoring Actions usage and costs Key Skills:\nEnterprise Actions policy configuration Self-hosted runner administration Workflow security and compliance Usage monitoring and cost management Runner scaling and optimisation Domain 6: Manage GitHub Packages (5%) Core Competencies:\nAdministering GitHub Packages across organisations Managing package visibility and access controls Implementing package security and compliance Managing package storage and billing Integrating with external package registries Key Skills:\nPackage registry administration Access control and visibility management Package security policy implementation Storage optimisation and cost management External registry integration Complete Study Plan Phase 1: Enterprise Foundation Building (Weeks 1-2) Week 1: GitHub Enterprise Architecture Understand GitHub Enterprise Cloud vs Server Learn enterprise account and organisation hierarchy Explore billing models and cost optimisation Practice with enterprise-level configurations Daily Tasks:\nExplore GitHub Enterprise features and architecture Understand billing and licensing models Practice with enterprise account management Review organisation and team structures Hands-On Labs:\n# GitHub CLI enterprise administration gh api /enterprises/ENTERPRISE/settings \\ --method PATCH \\ --field default_repository_permission=\u0026#34;read\u0026#34; \\ --field members_can_create_repositories=false # Organisation management gh api /orgs/ORG/members \\ --jq \u0026#39;.[] | {login: .login, role: .role, type: .type}\u0026#39; Week 2: Identity and Access Management Foundation Understand SAML SSO configuration and management Learn SCIM provisioning and user lifecycle Practice with identity provider integrations Explore access control and permission models Practice Projects:\nSet up SAML SSO with test identity provider Configure SCIM provisioning workflows Implement team-based access controls Create user access review processes Phase 2: User and Identity Management (Weeks 3-4) Week 3: SAML and SSO Implementation Configure SAML authentication for organisations Implement identity provider integrations Practice with SSO troubleshooting and diagnostics Learn about Just-in-Time (JIT) provisioning SAML Configuration Labs:\n\u0026lt;!-- SAML assertion example for GitHub --\u0026gt; \u0026lt;saml2:Assertion\u0026gt; \u0026lt;saml2:Subject\u0026gt; \u0026lt;saml2:NameID Format=\u0026#34;urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress\u0026#34;\u0026gt; user@company.com \u0026lt;/saml2:NameID\u0026gt; \u0026lt;/saml2:Subject\u0026gt; \u0026lt;saml2:AttributeStatement\u0026gt; \u0026lt;saml2:Attribute Name=\u0026#34;login\u0026#34;\u0026gt; \u0026lt;saml2:AttributeValue\u0026gt;user.name\u0026lt;/saml2:AttributeValue\u0026gt; \u0026lt;/saml2:Attribute\u0026gt; \u0026lt;saml2:Attribute Name=\u0026#34;email\u0026#34;\u0026gt; \u0026lt;saml2:AttributeValue\u0026gt;user@company.com\u0026lt;/saml2:AttributeValue\u0026gt; \u0026lt;/saml2:Attribute\u0026gt; \u0026lt;saml2:Attribute Name=\u0026#34;full_name\u0026#34;\u0026gt; \u0026lt;saml2:AttributeValue\u0026gt;User Full Name\u0026lt;/saml2:AttributeValue\u0026gt; \u0026lt;/saml2:Attribute\u0026gt; \u0026lt;/saml2:AttributeStatement\u0026gt; \u0026lt;/saml2:Assertion\u0026gt; Week 4: SCIM Provisioning and User Lifecycle Implement SCIM user provisioning Configure automated user lifecycle management Practice with user group synchronisation Learn about user access reviews and compliance SCIM Implementation Example:\n{ \u0026#34;schemas\u0026#34;: [\u0026#34;urn:ietf:params:scim:schemas:core:2.0:User\u0026#34;], \u0026#34;userName\u0026#34;: \u0026#34;user.name\u0026#34;, \u0026#34;name\u0026#34;: { \u0026#34;givenName\u0026#34;: \u0026#34;User\u0026#34;, \u0026#34;familyName\u0026#34;: \u0026#34;Name\u0026#34; }, \u0026#34;emails\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;user@company.com\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;work\u0026#34;, \u0026#34;primary\u0026#34;: true } ], \u0026#34;groups\u0026#34;: [ { \u0026#34;value\u0026#34;: \u0026#34;developers\u0026#34;, \u0026#34;display\u0026#34;: \u0026#34;Developers\u0026#34; } ], \u0026#34;active\u0026#34;: true } Phase 3: Security and Governance (Weeks 5-6) Week 5: Security Policy Implementation Configure organisation-wide security policies Implement branch protection and repository policies Set up compliance monitoring and reporting Practice with security policy enforcement Enterprise Security Policy Configuration:\n# Enterprise security policy template enterprise_security_policy: default_repository_settings: private_vulnerability_reporting: enabled dependency_graph: enabled dependabot_alerts: enabled dependabot_security_updates: enabled secret_scanning: enabled secret_scanning_push_protection: enabled branch_protection_defaults: required_status_checks: strict: true contexts: [\u0026#34;ci/build\u0026#34;, \u0026#34;security/scan\u0026#34;] enforce_admins: true required_pull_request_reviews: required_approving_review_count: 2 dismiss_stale_reviews: true require_code_owner_reviews: true restrictions: users: [] teams: [\u0026#34;platform-team\u0026#34;] Week 6: Compliance and Audit Management Implement compliance frameworks and controls Set up audit logging and monitoring Practice with compliance reporting Learn about regulatory requirements (SOC2, FedRAMP, etc.) Audit and Compliance Monitoring:\n# GitHub audit log analysis import requests import json from datetime import datetime, timedelta class GitHubAuditAnalyzer: def __init__(self, enterprise, token): self.enterprise = enterprise self.headers = { \u0026#39;Authorization\u0026#39;: f\u0026#39;token {token}\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/vnd.github.v3+json\u0026#39; } def get_audit_events(self, phrase=None, include=None): \u0026#34;\u0026#34;\u0026#34;Retrieve enterprise audit events\u0026#34;\u0026#34;\u0026#34; url = f\u0026#39;https://api.github.com/enterprises/{self.enterprise}/audit-log\u0026#39; params = {} if phrase: params[\u0026#39;phrase\u0026#39;] = phrase if include: params[\u0026#39;include\u0026#39;] = include response = requests.get(url, headers=self.headers, params=params) return response.json() def analyze_admin_activities(self): \u0026#34;\u0026#34;\u0026#34;Analyze administrative activities\u0026#34;\u0026#34;\u0026#34; admin_events = self.get_audit_events(phrase=\u0026#39;action:org\u0026#39;) activities = {} for event in admin_events: action = event.get(\u0026#39;action\u0026#39;, \u0026#39;unknown\u0026#39;) if action not in activities: activities[action] = 0 activities[action] += 1 return activities Phase 4: Actions and Automation Administration (Weeks 7-8) Week 7: GitHub Actions Enterprise Administration Configure GitHub Actions policies at enterprise level Implement self-hosted runner administration Set up runner groups and access controls Practice with Actions usage monitoring Enterprise Actions Configuration:\n# GitHub Actions enterprise policy actions_policy: enabled_organizations: \u0026#34;all\u0026#34; allowed_actions: \u0026#34;selected\u0026#34; allowed_actions_config: github_owned_allowed: true verified_allowed: true patterns_allowed: - \u0026#34;actions/*\u0026#34; - \u0026#34;company-org/*\u0026#34; default_workflow_permissions: \u0026#34;read\u0026#34; can_approve_pull_request_reviews: false runner_groups: - name: \u0026#34;production-runners\u0026#34; visibility: \u0026#34;selected\u0026#34; organizations: [\u0026#34;prod-org\u0026#34;] runners: [\u0026#34;runner-1\u0026#34;, \u0026#34;runner-2\u0026#34;] - name: \u0026#34;development-runners\u0026#34; visibility: \u0026#34;all\u0026#34; allow_public_repositories: false Week 8: Package Management and Integration Administer GitHub Packages across organisations Configure package visibility and access controls Implement package security policies Practice with external registry integrations Package Administration Scripts:\n#!/bin/bash # GitHub Packages administration ORG=\u0026#34;your-org\u0026#34; TOKEN=\u0026#34;your-token\u0026#34; # List all packages in organisation gh api \u0026#34;/orgs/$ORG/packages\u0026#34; \\ --jq \u0026#39;.[] | {name: .name, visibility: .visibility, package_type: .package_type}\u0026#39; # Configure package visibility gh api \u0026#34;/orgs/$ORG/packages/npm/PACKAGE_NAME\u0026#34; \\ --method PATCH \\ --field visibility=\u0026#34;private\u0026#34; # Delete package version gh api \u0026#34;/orgs/$ORG/packages/npm/PACKAGE_NAME/versions/VERSION_ID\u0026#34; \\ --method DELETE Hands-On Laboratory Exercises Lab 1: Enterprise SAML SSO Implementation Implement comprehensive SAML SSO across an enterprise GitHub organisation with automated provisioning.\nObjectives:\nConfigure SAML SSO with identity provider Implement user attribute mapping Set up automated user provisioning Test SSO functionality and troubleshooting Implementation Steps:\nSAML Configuration: \u0026lt;!-- SAML configuration for GitHub Enterprise --\u0026gt; \u0026lt;EntityDescriptor entityID=\u0026#34;https://github.com/orgs/YOUR_ORG\u0026#34;\u0026gt; \u0026lt;SPSSODescriptor\u0026gt; \u0026lt;AssertionConsumerService Binding=\u0026#34;urn:oasis:names:tc:SAML:2.0:bindings:HTTP-POST\u0026#34; Location=\u0026#34;https://github.com/orgs/YOUR_ORG/saml/consume\u0026#34; index=\u0026#34;0\u0026#34; /\u0026gt; \u0026lt;AttributeConsumingService index=\u0026#34;0\u0026#34;\u0026gt; \u0026lt;ServiceName xml:lang=\u0026#34;en\u0026#34;\u0026gt;GitHub\u0026lt;/ServiceName\u0026gt; \u0026lt;RequestedAttribute Name=\u0026#34;login\u0026#34; isRequired=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;RequestedAttribute Name=\u0026#34;email\u0026#34; isRequired=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;RequestedAttribute Name=\u0026#34;full_name\u0026#34; isRequired=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;/AttributeConsumingService\u0026gt; \u0026lt;/SPSSODescriptor\u0026gt; \u0026lt;/EntityDescriptor\u0026gt; User Provisioning Automation: # Automated user provisioning script import requests import json class GitHubUserProvisioner: def __init__(self, org, token): self.org = org self.headers = { \u0026#39;Authorization\u0026#39;: f\u0026#39;token {token}\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/vnd.github.v3+json\u0026#39; } def provision_user(self, username, email, teams=None): \u0026#34;\u0026#34;\u0026#34;Provision user and add to teams\u0026#34;\u0026#34;\u0026#34; # Invite user to organisation invite_data = { \u0026#39;email\u0026#39;: email, \u0026#39;role\u0026#39;: \u0026#39;direct_member\u0026#39; } if teams: invite_data[\u0026#39;team_ids\u0026#39;] = [self.get_team_id(team) for team in teams] response = requests.post( f\u0026#39;https://api.github.com/orgs/{self.org}/invitations\u0026#39;, headers=self.headers, json=invite_data ) return response.json() def get_team_id(self, team_slug): \u0026#34;\u0026#34;\u0026#34;Get team ID by slug\u0026#34;\u0026#34;\u0026#34; response = requests.get( f\u0026#39;https://api.github.com/orgs/{self.org}/teams/{team_slug}\u0026#39;, headers=self.headers ) return response.json().get(\u0026#39;id\u0026#39;) Lab 2: Comprehensive Security Policy Implementation Create and implement organisation-wide security policies with automated enforcement.\nObjectives:\nDesign comprehensive security policy framework Implement automated policy enforcement Set up compliance monitoring and reporting Create security incident response procedures Security Policy Framework:\n# Comprehensive security policy configuration security_framework: repository_policies: default_branch_protection: required_status_checks: strict: true contexts: - \u0026#34;continuous-integration\u0026#34; - \u0026#34;security-scan\u0026#34; - \u0026#34;dependency-check\u0026#34; enforce_admins: true required_pull_request_reviews: required_approving_review_count: 2 dismiss_stale_reviews: true require_code_owner_reviews: true required_approving_review_count_for_admins: 1 restrictions: users: [] teams: [\u0026#34;security-team\u0026#34;, \u0026#34;platform-team\u0026#34;] apps: [] allow_force_pushes: false allow_deletions: false security_features: dependency_graph: \u0026#34;enabled\u0026#34; dependabot_alerts: \u0026#34;enabled\u0026#34; dependabot_security_updates: \u0026#34;enabled\u0026#34; secret_scanning: \u0026#34;enabled\u0026#34; secret_scanning_push_protection: \u0026#34;enabled\u0026#34; private_vulnerability_reporting: \u0026#34;enabled\u0026#34; access_controls: default_repository_permission: \u0026#34;read\u0026#34; members_can_create_repositories: false members_can_create_internal_repositories: true members_can_create_pages: false members_can_fork_private_repositories: false third_party_access: saml_single_sign_on: \u0026#34;enabled\u0026#34; two_factor_requirement: \u0026#34;enabled\u0026#34; oauth_app_restrictions: \u0026#34;enabled\u0026#34; Policy Enforcement Automation:\n# Security policy enforcement automation class SecurityPolicyEnforcer: def __init__(self, org, token): self.org = org self.headers = { \u0026#39;Authorization\u0026#39;: f\u0026#39;token {token}\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/vnd.github.v3+json\u0026#39; } def enforce_branch_protection(self, repo, branch=\u0026#34;main\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Enforce branch protection rules\u0026#34;\u0026#34;\u0026#34; protection_config = { \u0026#34;required_status_checks\u0026#34;: { \u0026#34;strict\u0026#34;: True, \u0026#34;contexts\u0026#34;: [\u0026#34;ci/build\u0026#34;, \u0026#34;security/scan\u0026#34;] }, \u0026#34;enforce_admins\u0026#34;: True, \u0026#34;required_pull_request_reviews\u0026#34;: { \u0026#34;required_approving_review_count\u0026#34;: 2, \u0026#34;dismiss_stale_reviews\u0026#34;: True, \u0026#34;require_code_owner_reviews\u0026#34;: True }, \u0026#34;restrictions\u0026#34;: None, \u0026#34;allow_force_pushes\u0026#34;: False, \u0026#34;allow_deletions\u0026#34;: False } response = requests.put( f\u0026#39;https://api.github.com/repos/{self.org}/{repo}/branches/{branch}/protection\u0026#39;, headers=self.headers, json=protection_config ) return response.status_code == 200 def audit_repository_compliance(self): \u0026#34;\u0026#34;\u0026#34;Audit repository compliance against policies\u0026#34;\u0026#34;\u0026#34; repos = self.get_repositories() compliance_report = [] for repo in repos: compliance = self.check_repository_compliance(repo[\u0026#39;name\u0026#39;]) compliance_report.append({ \u0026#39;repository\u0026#39;: repo[\u0026#39;name\u0026#39;], \u0026#39;compliance_score\u0026#39;: compliance[\u0026#39;score\u0026#39;], \u0026#39;issues\u0026#39;: compliance[\u0026#39;issues\u0026#39;] }) return compliance_report Lab 3: GitHub Actions Enterprise Administration Implement comprehensive GitHub Actions administration with self-hosted runners and enterprise policies.\nObjectives:\nSet up enterprise GitHub Actions policies Deploy and manage self-hosted runner infrastructure Implement runner groups and access controls Monitor Actions usage and costs Self-Hosted Runner Deployment:\n# Docker Compose for self-hosted runners version: \u0026#39;3.8\u0026#39; services: github-runner-1: image: myoung34/github-runner:latest environment: - ORG_NAME=your-org - ACCESS_TOKEN=${GITHUB_TOKEN} - RUNNER_NAME=enterprise-runner-1 - RUNNER_WORKDIR=/tmp/runner/work - LABELS=self-hosted,production,linux,x64 volumes: - /var/run/docker.sock:/var/run/docker.sock - /tmp/runner:/tmp/runner restart: unless-stopped github-runner-2: image: myoung34/github-runner:latest environment: - ORG_NAME=your-org - ACCESS_TOKEN=${GITHUB_TOKEN} - RUNNER_NAME=enterprise-runner-2 - RUNNER_WORKDIR=/tmp/runner/work - LABELS=self-hosted,production,linux,x64 volumes: - /var/run/docker.sock:/var/run/docker.sock - /tmp/runner:/tmp/runner restart: unless-stopped Actions Administration Scripts:\n#!/bin/bash # GitHub Actions enterprise administration ORG=\u0026#34;your-org\u0026#34; TOKEN=\u0026#34;your-token\u0026#34; # Create runner group gh api /orgs/$ORG/actions/runner-groups \\ --method POST \\ --field name=\u0026#34;production-runners\u0026#34; \\ --field visibility=\u0026#34;selected\u0026#34; \\ --field selected_repository_ids:=\u0026#39;[123, 456]\u0026#39; # List runner groups gh api /orgs/$ORG/actions/runner-groups \\ --jq \u0026#39;.runner_groups[] | {id: .id, name: .name, visibility: .visibility}\u0026#39; # Get Actions usage statistics gh api /orgs/$ORG/actions/billing/usage \\ --jq \u0026#39;{total_minutes_used: .total_minutes_used, total_paid_minutes_used: .total_paid_minutes_used}\u0026#39; Enterprise Administration Best Practices Governance Framework 1. Administrative Hierarchy:\n# Enterprise governance structure governance_hierarchy: enterprise_administrators: responsibilities: - Enterprise billing and licensing - Organisation creation and management - Enterprise-wide policy enforcement - Compliance and audit oversight organisation_owners: responsibilities: - Organisation security policies - Team and repository management - User access control - Organisation billing oversight team_maintainers: responsibilities: - Team membership management - Repository access control - Team-specific policies - Developer onboarding 2. Access Control Framework:\n# Role-based access control matrix rbac_matrix: repository_permissions: read: [\u0026#34;all_members\u0026#34;] write: [\u0026#34;team_members\u0026#34;, \u0026#34;collaborators\u0026#34;] admin: [\u0026#34;team_maintainers\u0026#34;, \u0026#34;org_owners\u0026#34;] organisation_permissions: member: [\u0026#34;basic_access\u0026#34;] moderator: [\u0026#34;issue_management\u0026#34;] billing_manager: [\u0026#34;billing_access\u0026#34;] owner: [\u0026#34;full_access\u0026#34;] enterprise_permissions: member: [\u0026#34;org_access\u0026#34;] billing_manager: [\u0026#34;enterprise_billing\u0026#34;] owner: [\u0026#34;enterprise_admin\u0026#34;] Operational Excellence Monitoring and Alerting:\n# Enterprise monitoring and alerting system class GitHubEnterpriseMonitor: def __init__(self, enterprise, token): self.enterprise = enterprise self.client = GitHubClient(token) def monitor_user_activity(self): \u0026#34;\u0026#34;\u0026#34;Monitor user login and activity patterns\u0026#34;\u0026#34;\u0026#34; audit_events = self.client.get_audit_events( phrase=\u0026#39;action:oauth_access_token\u0026#39; ) activity_metrics = { \u0026#39;total_logins\u0026#39;: len(audit_events), \u0026#39;unique_users\u0026#39;: len(set(event[\u0026#39;actor\u0026#39;] for event in audit_events)), \u0026#39;failed_attempts\u0026#39;: len([e for e in audit_events if e.get(\u0026#39;failure\u0026#39;)]) } return activity_metrics def monitor_security_compliance(self): \u0026#34;\u0026#34;\u0026#34;Monitor security compliance across organisations\u0026#34;\u0026#34;\u0026#34; orgs = self.client.get_enterprise_orgs() compliance_summary = {} for org in orgs: compliance_summary[org[\u0026#39;login\u0026#39;]] = { \u0026#39;saml_enabled\u0026#39;: org.get(\u0026#39;saml_enabled\u0026#39;, False), \u0026#39;2fa_required\u0026#39;: org.get(\u0026#39;two_factor_requirement_enabled\u0026#39;, False), \u0026#39;private_repos\u0026#39;: org.get(\u0026#39;private_repos\u0026#39;, 0), \u0026#39;total_repos\u0026#39;: org.get(\u0026#39;total_private_repos\u0026#39;, 0) } return compliance_summary Cost Optimisation Strategies License Management:\n# GitHub license optimisation class GitHubLicenseOptimizer: def __init__(self, enterprise, token): self.enterprise = enterprise self.client = GitHubClient(token) def analyze_license_usage(self): \u0026#34;\u0026#34;\u0026#34;Analyze license usage and identify optimisations\u0026#34;\u0026#34;\u0026#34; consumed_licenses = self.client.get_license_consumption() analysis = { \u0026#39;total_licenses\u0026#39;: consumed_licenses[\u0026#39;total_seats_consumed\u0026#39;], \u0026#39;active_users\u0026#39;: self.count_active_users(), \u0026#39;inactive_users\u0026#39;: self.identify_inactive_users(), \u0026#39;potential_savings\u0026#39;: 0 } inactive_count = len(analysis[\u0026#39;inactive_users\u0026#39;]) analysis[\u0026#39;potential_savings\u0026#39;] = inactive_count * self.get_license_cost() return analysis def recommend_optimisations(self): \u0026#34;\u0026#34;\u0026#34;Recommend license optimisations\u0026#34;\u0026#34;\u0026#34; analysis = self.analyze_license_usage() recommendations = [] if analysis[\u0026#39;inactive_users\u0026#39;]: recommendations.append({ \u0026#39;type\u0026#39;: \u0026#39;remove_inactive_users\u0026#39;, \u0026#39;description\u0026#39;: f\u0026#34;Remove {len(analysis[\u0026#39;inactive_users\u0026#39;])} inactive users\u0026#34;, \u0026#39;potential_savings\u0026#39;: analysis[\u0026#39;potential_savings\u0026#39;] }) return recommendations Exam Tips and Strategies Technical Preparation Focus Areas Enterprise Administration Mastery:\nMulti-organisation management and hierarchy Billing administration and cost optimisation Enterprise policy implementation and enforcement Compliance frameworks and audit requirements Support escalation and troubleshooting procedures Identity Management Expertise:\nSAML SSO configuration and troubleshooting SCIM provisioning and user lifecycle automation Identity provider integration patterns Access review and compliance processes Just-in-Time provisioning workflows Security Governance Proficiency:\nOrganisation-wide security policy design Branch protection and repository governance Compliance monitoring and reporting Security incident response procedures Risk assessment and mitigation strategies Exam Day Strategy Time Management:\nAllocate 1.8 minutes per question average Focus on enterprise-scale scenarios Use process of elimination for complex questions Reserve time for configuration review Question Approach:\nIdentify the administrative domain being tested Consider enterprise vs organisation-level implications Evaluate compliance and governance requirements Apply hands-on administrative experience Select the most scalable and secure solution Common Exam Topics:\nSAML/SCIM configuration syntax and troubleshooting Enterprise policy implementation patterns Access control and permission management Actions administration and runner management Billing and license optimisation strategies Official Study Resources GitHub Documentation GitHub Enterprise Administration Managing SAML SSO SCIM Provisioning GitHub Actions Administration Microsoft Learn Paths Administer GitHub Enterprise Implement GitHub administration and security Manage GitHub Actions in enterprise Practice Platforms GitHub Enterprise Trial GitHub Learning Lab GitHub Skills Additional Practice Resources Administration Communities GitHub Community Forum GitHub Enterprise Support GitHub Administrator Documentation Hands-On Practice Set up GitHub Enterprise trial organisation Practice with identity provider integrations Implement comprehensive security policies Build administrative automation scripts Final Preparation Checklist Technical Skills Validation Can configure enterprise GitHub settings and policies Understand SAML SSO setup and troubleshooting Master SCIM provisioning and user lifecycle management Know security policy implementation across organisations Can administer GitHub Actions at enterprise scale Understand GitHub Packages administration Know billing and license management Can implement compliance monitoring and reporting Understand audit logging and analysis Know enterprise support and escalation procedures Can optimise costs and resource usage Understand integration with external systems Exam Readiness Assessment Completed all study plan phases Finished hands-on administration laboratories Practiced with enterprise scenarios Reviewed official documentation thoroughly Taken practice assessments consistently Confident in all administration domains Registered for exam date Prepared exam day logistics Next Steps in Your Journey After achieving GH-100 certification, consider these advanced paths:\nAdvanced Certifications GitHub Copilot (GH-500): AI-powered development and enterprise Copilot administration Microsoft Certifications: Azure DevOps and cloud platform certifications Security Specialisation: Advanced security and compliance certifications Career Advancement Enterprise Architect: Design large-scale GitHub implementations Platform Engineering Lead: Build and manage developer platforms DevOps Director: Lead enterprise DevOps transformation Technical Consultant: Provide GitHub expertise to enterprises Conclusion The GH-100 GitHub Administration certification validates your expertise in managing GitHub at enterprise scale and positions you as a crucial leader in organisational DevOps transformation. By following this comprehensive study guide, completing the hands-on administration laboratories, and practising with real-world enterprise scenarios, you\u0026rsquo;ll be well-prepared to pass the exam and excel in enterprise GitHub administration.\nRemember that enterprise administration is constantly evolving with new features, security requirements, and compliance needs. Continue learning about emerging GitHub capabilities, industry best practices, and enterprise governance frameworks. The skills you develop preparing for this certification will serve you throughout your career in platform engineering and enterprise administration.\nReady to administer at scale? üè¢\nStart your GitHub Administration mastery journey today, and join the ranks of certified enterprise administrators who are enabling developer productivity and organisational success worldwide.\nThis guide is part of the GitHub Certification Journey series. Previous: GitHub Advanced Security (GH-300) | Next: GitHub Copilot (GH-500) - Coming Soon\nHave questions about GitHub Administration certification? Connect with me on LinkedIn or GitHub for guidance and support.\n","date":"2025-08-15T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-100/cover_hu_fd6d6c2046354e31.jpg","image":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-100/cover_hu_116b25e365c49b5c.jpg","permalink":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-100/","title":"üè¢ GitHub Administration Certification Guide (GH-100) - Enterprise Mastery","webpImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-100/cover_hu_ab1727929c3730d7.webp"},{"content":"Why this pipeline exists Manually curating weekly platform updates (Azure service changes, GitHub changelog entries, Terraform provider releases) is repetitive, time‚Äësensitive, and error‚Äëprone. This pipeline automates the whole path from data acquisition ‚Üí AI summarisation ‚Üí Hugo markdown generation ‚Üí commit, delivering consistent, timestamped update posts under content/updates.\nHigh‚Äëlevel architecture ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê cron 05:15 UTC ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê grouped items ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ GitHub Action ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u0026gt; ‚îÇ PowerShell Script ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u0026gt; ‚îÇ AI Summarisation ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ commit (if changed) ‚îÇ filtered items ‚îÇ summaries V V V Repo content \u0026lt;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ write markdown posts \u0026lt;‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ assemble front matter Scheduling \u0026amp; cadence The workflow runs daily at 05:15 UTC but per‚Äësource publication cadence is controlled inside the script using a frequency map (Azure=weekly,GitHub=weekly,Terraform=weekly by default). The script:\nComputes a base window (week vs rolling period). Derives per‚Äësource windows (weekly / biweekly / monthly) without duplicating logic. Caps items per source (MaxAzure, MaxGitHub, MaxTerraform) to keep posts readable. Window calculation Compute-BaseWindow returns UTC and local (Europe/Brussels) boundaries. Compute-PerSourceWindows adjusts start dates biweekly (‚àí7 days) or monthly (month start) per source.\nData acquisition layer Fetcher functions isolate network concerns:\nGet-AzureUpdates parses an RSS feed with defensive XML handling. Get-GitHubChangelog mirrors the pattern for GitHub\u0026rsquo;s changelog. Get-TerraformReleases loops repositories (hashicorp/terraform, hashicorp/terraform-provider-azurerm) and filters releases in the window. Each returns plain objects with a common shape (title, url, publishedAt, raw, source), simplifying later aggregation.\nSummarisation engine Summarize-Items:\nExpands a model pool (defaults: openai/gpt-4.1, openai/gpt-4o, openai/gpt-5, openai/o1, openai/o3 plus -mini variants) or uses caller override. Normalises + deduplicates model identifiers. Iteratively attempts summarisation per item, promoting the first successful model to the front (adaptive ordering). Implements exponential backoff with jitter (SummaryRetryBaseSeconds) and bounded attempts (MaxSummaryRetries). Falls back to truncated raw title when all models fail. Output objects include optional bullet points (bullets) and a condensed factual summary.\nResilience patterns Retry loop with model deactivation on repeated failure (429 / transient). Cache placeholder (pattern enables future caching without refactor). Graceful degradation when token absent (-DisableSummaries). Markdown emission Write-PerTypePost builds a clean front matter block every run, avoiding op_Addition issues seen when concatenating PSCustomObjects. It:\nCreates a dated folder (content/updates/\u0026lt;slug\u0026gt;/index.md). Generates deterministic title + description from window and type. Uses lastmod refresh for diff hygiene. Ensures UTF‚Äë8 encoding. Example (trimmed) output block:\n+++ title = \u0026#39;Azure Weekly ‚Äì 2025 Week 33\u0026#39; date = 2025-08-13T12:50:21Z lastmod = 2025-08-13T12:50:21Z draft = false tags = [\u0026#39;updates\u0026#39;, \u0026#39;weekly\u0026#39;, \u0026#39;azure\u0026#39;] description = \u0026#39;Highlights from Azure between 2025-08-11 and 2025-08-17.\u0026#39; [params] author = \u0026#39;sujith\u0026#39; +++ Body lines are a bullet list with escaped markdown characters and optional detail indents per item.\nWorkflow integration (update.yml) Key steps:\nCheckout (full history for potential diff logic). Inject a fine‚Äëgrained Personal Access Token (PAT) with Models: read permission via secrets.WEEKLY and expose it as the environment variable GITHUB_TOKEN (the script expects that name). This PAT is required because the default ephemeral Actions token may not always grant Models: read on all repositories / org policies. Run script (PowerShell) which uses that token for release API calls and AI summarisation. Commit changes only when the working tree differs: Prevents empty commits. Enables predictable downstream indexing (e.g., search, sitemap). Snippet (core commit logic):\n- name: Commit \u0026amp; push (if changed) run: | git config user.name \u0026#34;github-actions[bot]\u0026#34; git config user.email \u0026#34;41898282+github-actions[bot]@users.noreply.github.com\u0026#34; git add -A if git diff --staged --quiet; then echo \u0026#34;No changes\u0026#34; exit 0 fi git commit -m \u0026#34;chore(updates): daily refresh of per-type posts\u0026#34; git push Full workflow file Below is the complete workflow (.github/workflows/update.yml) for reference so you can copy/adapt without switching contexts:\nname: Updates (Daily Refresh) on: schedule: # Run daily at 05:15 UTC (~07:15 Europe/Brussels during DST) - cron: \u0026#39;15 5 * * *\u0026#39; workflow_dispatch: permissions: contents: write models: read jobs: build-weekly: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 with: fetch-depth: 0 - name: Run tracker shell: pwsh env: GITHUB_TOKEN: ${{ secrets.WEEKLY }} run: | pwsh ./.github/scripts/generate-updates.ps1 -RepoRoot . -ContentDir content/updates -MaxAzure 20 -MaxGitHub 12 -MaxTerraform 8 -ShowApiUrls -Frequencies Azure=weekly,GitHub=weekly,Terraform=weekly -ModelPool \u0026#39;openai/gpt-5\u0026#39;,\u0026#39;openai/gpt-5-mini\u0026#39;,\u0026#39;openai/gpt-4.1\u0026#39;,\u0026#39;openai/gpt-4.1-mini\u0026#39;,\u0026#39;openai/gpt-4o\u0026#39;,\u0026#39;openai/gpt-4o-mini\u0026#39; - name: Commit \u0026amp; push (if changed) run: | git config user.name \u0026#34;github-actions[bot]\u0026#34; git config user.email \u0026#34;41898282+github-actions[bot]@users.noreply.github.com\u0026#34; git add -A if git diff --staged --quiet; then echo \u0026#34;No changes\u0026#34; exit 0 fi git commit -m \u0026#34;chore(updates): daily refresh of per-type posts\u0026#34; git push Script source (generate-updates.ps1) The full PowerShell script is lengthy (~800 lines) and modular. Expand the section below to view it inline; canonical source lives at .github/scripts/generate-updates.ps1.\nAbbreviated listing (top section):\n#!/usr/bin/env pwsh [CmdletBinding()] param( [string]$RepoRoot = (Resolve-Path -LiteralPath .).Path, [string]$ContentDir = \u0026#39;content/updates\u0026#39;, [ValidateRange(0,200)][int]$MaxAzure = 20, [ValidateRange(0,200)][int]$MaxGitHub = 12, [ValidateRange(0,200)][int]$MaxTerraform = 8, [string]$Frequencies = \u0026#39;Azure=weekly,GitHub=weekly,Terraform=weekly\u0026#39; , [ValidateSet(\u0026#39;week\u0026#39;,\u0026#39;rolling\u0026#39;)][string]$WindowType = \u0026#39;week\u0026#39;, [int]$RollingDays = 7 , [switch]$DisableSummaries, [int]$MaxSummaryRetries = 4, [int]$SummaryRetryBaseSeconds = 2 , [switch]$ShowApiUrls, [switch]$DumpSummaries, [switch]$CleanOutput, [string[]]$ModelPool = @() ) # (Truncated commentary for brevity in article) [Console]::OutputEncoding = [System.Text.Encoding]::UTF8 $ErrorActionPreference = \u0026#39;Stop\u0026#39; function Log { param([string]$Message) Write-Host \u0026#34;[$(Get-Date -Format \u0026#39;HH:mm:ss\u0026#39;)] $Message\u0026#34; } function Initialize-Environment { param([string]$Token,[switch]$AllowMissingToken) if(-not $Token){ if($AllowMissingToken){ Write-Warning \u0026#39;GITHUB_TOKEN missing\u0026#39;; return @{ HeadersGitHub = @{} } } else { throw \u0026#39;GITHUB_TOKEN is required (with models:read, contents:write).\u0026#39; } } return @{ HeadersGitHub = @{ \u0026#39;Authorization\u0026#39; = \u0026#34;Bearer $Token\u0026#34;; \u0026#39;Accept\u0026#39;=\u0026#39;application/vnd.github+json\u0026#39;; \u0026#39;X-GitHub-Api-Version\u0026#39;=\u0026#39;2022-11-28\u0026#39; } } } # ... (All helper + fetch + summarisation + write functions unchanged; see repository file for full context) function Invoke-WeeklyUpdates { \u0026lt;# full body preserved in repo #\u0026gt; } # Entry point invoking Invoke-WeeklyUpdates with normalisation logic. # (Full body intentionally omitted here to keep article scannable.) For the complete, unabridged script (including all functions such as Compute-BaseWindow, Summarize-Items, Write-PerTypePost), view the source directly in the repository to benefit from future updates.\nNote: Full script (including all helper functions) is in .github/scripts/generate-updates.ps1 in the repository.\nSecurity considerations A fine‚Äëgrained PAT stored as secrets.WEEKLY (minimum scope: models: read; optionally contents: write if not already inherited) is mapped to GITHUB_TOKEN only for the summarisation step. This tightly scopes access and keeps broader org tokens out of the workflow. The default ephemeral GITHUB_TOKEN usually works for releases + (in many cases) model inference, but: (a) rate limits for models can be lower, (b) org policy might restrict Models: read, and (c) token rotation / revocation granularity is coarser. Using a fine‚Äëgrained PAT improves reliability under higher summarisation volume and gives explicit auditability. Principle of least privilege: exclude unneeded scopes (no admin, no workflow write, no repo deletion). Rotate on a regular cadence; label the PAT (e.g. \u0026ldquo;weekly-updates-pipeline\u0026rdquo;). Throttling resilience: higher model rate limits reduce retry pressure and lower the chance of exhausting attempts during summarisation bursts. No external secret leakage; feed and release endpoints are public; only summary requests hit the GitHub Models API. Summarisation prompt sanitises whitespace, reducing injection surface. Consider future allow‚Äëlist validation for model names and a deny‚Äëlist for unexpected outbound URLs in model output. Performance \u0026amp; efficiency Applied optimisation patterns:\nBounded item counts to cap summarisation cost. Early exit when limits \u0026lt; 1. Single pass aggregation into an array (avoids + which can cast to string inadvertently). Stopwatch instrumentation around summarisation and Terraform fetch for future telemetry. Extensibility roadmap Enhancement Rationale Add caching layer (file or repo issues) Avoid re-summarising unchanged releases. Structured JSON artifact output Enables dashboards / trend analysis. Model latency metrics + ranking Optimise pool ordering adaptively. Additional sources (Azure DevOps, .NET release notes) Broader ecosystem coverage. Semantic diff of prior period Highlight new vs previously reported changes. Optional Slack / Teams notification Push updates to collaboration channels. Local testing # Run with summaries disabled (fast) pwsh .\\.github\\scripts\\generate-updates.ps1 -DisableSummaries -ShowApiUrls -ContentDir content/updates -MaxAzure 5 -MaxGitHub 5 -MaxTerraform 3 # Custom rolling 3‚Äëday window pwsh .\\.github\\scripts\\generate-updates.ps1 -WindowType rolling -RollingDays 3 -DisableSummaries # Specify custom model pool pwsh .\\.github\\scripts\\generate-updates.ps1 -ModelPool \u0026#39;openai/gpt-4.1-mini\u0026#39;,\u0026#39;openai/gpt-4o-mini\u0026#39; Validate output under content/updates then run a local Hugo server to inspect rendering.\nTroubleshooting Symptom Cause Fix Empty post folder No items in time window Adjust RollingDays or verify feeds. Summaries are just titles All model attempts failed Check GITHUB_TOKEN scopes; reduce model list. Terraform section missing No releases in window Increase window or confirm repos. RSS parse warning Transient or format change Re-run; add defensive logging if persistent. Timezone mismatch TZ ID not resolved on runner Fallback logs warn; acceptable; optionally pin to UTC. Design decisions (WHY, not WHAT) Function modularity: Facilitates unit testing and future module extraction. Front matter rebuild: Avoids mutation complexity and ensures determinism. Adaptive model ordering: Reduces average latency by reusing the last successful model first. Explicit limits: Keeps markdown lightweight and scannable for weekly cadence. Window abstraction: Allows switching between weekly calendar semantics and rolling ranges. When to fork vs reuse Reuse the script if you only need periodic feed aggregation + AI summaries + Hugo output. Fork (or parameterise) if you require auth-protected APIs, structured JSON export, or per-item diffing.\nRelated concepts Platform engineering content automation. Continuous content delivery patterns. AI-assisted summarisation pipelines. Next steps Add model performance telemetry. Produce combined weekly digest (across sources) for newsletter distribution. Expose a JSON feed for downstream automation. Integrate Slack notification on new commits. References Azure Updates RSS: https://aztty.azurewebsites.net/rss/updates GitHub Changelog RSS: https://github.blog/changelog/ Terraform Releases: https://github.com/hashicorp/terraform/releases Terraform AzureRM Provider: https://github.com/hashicorp/terraform-provider-azurerm/releases Hugo Content Organisation: https://gohugo.io/content-management/organization/ GitHub Actions Scheduling: https://docs.github.com/actions/using-workflows/events-that-trigger-workflows#schedule Conclusion This pipeline is a compact, extensible pattern for continuous knowledge harvesting. By combining deterministic windowing, modular fetchers, adaptive summarisation, and idempotent markdown generation, it eliminates manual toil and delivers a reliable weekly knowledge artefact for the platform engineering estate.\n","date":"2025-08-13T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/automated-updates-pipeline/cover_hu_58de1a84b59a1225.jpg","image":"https://quintelier.dev/posts/2025/08/automated-updates-pipeline/cover_hu_d219dc3bf708723f.jpg","permalink":"https://quintelier.dev/posts/2025/08/automated-updates-pipeline/","title":"‚öôÔ∏è Automated Updates Pipeline","webpImage":"https://quintelier.dev/posts/2025/08/automated-updates-pipeline/cover_hu_c5f6c0df66a1a421.webp"},{"content":"Failures happen: the key is shortening time-to-understanding. In this guide, you‚Äôll automate triage when a GitHub Actions workflow fails: pull logs and artifacts, get an AI summary from GitHub Models, open an issue with concrete next steps, and optionally assign it to the Copilot coding agent.\nWhat you‚Äôll build Trigger on failed workflow runs you care about Collect logs and artifacts for context Call GitHub Models for a concise summary and probable fix File a GitHub issue with reproduction hints and links Optionally assign the issue to the Copilot coding agent Prerequisites GitHub Actions enabled in your repo GitHub Models access and a token for inference: If you‚Äôre using a fine-grained PAT: add models:read scope Or use secrets.GITHUB_TOKEN in public repos where Models are enabled for the org Optional: Copilot coding agent enabled at the org or user level to assign issues to Workflow: AI-assisted failure triage Below is a complete workflow you can drop into .github/workflows/ai-triage.yml. It triggers when selected workflows complete and only proceeds if they failed.\nname: AI triage failed runs on: workflow_run: workflows: [\u0026#34;*\u0026#34;] types: [completed] permissions: actions: read contents: read issues: write checks: write models: read jobs: triage: if: ${{ github.event.workflow_run.conclusion == \u0026#39;failure\u0026#39; }} runs-on: ubuntu-latest env: PAT_WITH_ISSUES_WRITE: ${{ secrets.PAT_WITH_ISSUES_WRITE }} steps: - name: Set variables id: vars shell: bash run: | echo \u0026#34;run_id=${{ github.event.workflow_run.id }}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; echo \u0026#34;run_url=${{ github.event.workflow_run.html_url }}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; echo \u0026#34;repo=${{ github.repository }}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; echo \u0026#34;branch=${{ github.event.workflow_run.head_branch }}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; echo \u0026#34;sha=${{ github.event.workflow_run.head_sha }}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$GITHUB_OUTPUT\u0026#34; - name: Download workflow logs (zip) shell: bash env: GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} run: | set -euo pipefail mkdir -p logs # Follow redirect to the actual ZIP download URL curl -sSL -H \u0026#34;Authorization: Bearer $GITHUB_TOKEN\u0026#34; \\ \u0026#34;https://api.github.com/repos/${{ steps.vars.outputs.repo }}/actions/runs/${{ steps.vars.outputs.run_id }}/logs\u0026#34; \\ -o logs/run-logs.zip || true unzip -q logs/run-logs.zip -d logs || echo \u0026#34;No logs found.\u0026#34; - name: Download test-results artifact uses: dawidd6/action-download-artifact@v11 with: github_token: ${{ secrets.GITHUB_TOKEN }} run_id: ${{ steps.vars.outputs.run_id }} repo: ${{ steps.vars.outputs.repo }} name: test-results path: artifacts/test-results if_no_artifact_found: ignore - name: Summarize TRX test results (check run + outputs) id: report if: always() uses: dorny/test-reporter@v2 with: name: TestSummary artifact: test-results path: \u0026#34;**/*.trx\u0026#34; reporter: dotnet-trx only-summary: true use-actions-summary: true fail-on-error: false fail-on-empty: false - name: Build AI prompt (TRX summary, artifacts, logs) shell: bash run: | { echo \u0026#34;Failure in ${{ steps.vars.outputs.repo }} on branch ${{ steps.vars.outputs.branch }} at ${{ steps.vars.outputs.sha }}\u0026#34; echo \u0026#34;Run: ${{ steps.vars.outputs.run_url }}\u0026#34; echo echo \u0026#34;Goal: Summarize failure cause and propose next steps. Be concise and actionable.\u0026#34; echo echo \u0026#34;=== Test summary (TRX via test-reporter) ===\u0026#34; echo \u0026#34;- Passed: ${{ steps.report.outputs.passed || \u0026#39;0\u0026#39; }}\u0026#34; echo \u0026#34;- Failed: ${{ steps.report.outputs.failed || \u0026#39;0\u0026#39; }}\u0026#34; echo \u0026#34;- Skipped: ${{ steps.report.outputs.skipped || \u0026#39;0\u0026#39; }}\u0026#34; if [ -n \u0026#34;${{ steps.report.outputs.url_html }}\u0026#34; ]; then echo \u0026#34;- Report: ${{ steps.report.outputs.url_html }}\u0026#34; fi echo echo \u0026#34;=== Artifacts ===\u0026#34; find artifacts -type f -maxdepth 3 | sed \u0026#39;s/^/- /\u0026#39; || true echo echo \u0026#34;=== Logs (head) ===\u0026#34; find logs -type f -name \u0026#39;*.txt\u0026#39; | head -n 3 | xargs -r -I{} sh -c \u0026#39;echo \u0026#34;--- {} ---\u0026#34;; head -n 60 \u0026#34;{}\u0026#34;\u0026#39; echo echo \u0026#34;=== Logs (tail) ===\u0026#34; find logs -type f -name \u0026#39;*.txt\u0026#39; | head -n 3 | xargs -r -I{} sh -c \u0026#39;echo \u0026#34;--- {} ---\u0026#34;; tail -n 60 \u0026#34;{}\u0026#34;\u0026#39; } \u0026gt; prompt.txt - name: AI triage summary id: ai uses: actions/ai-inference@v1 with: model: openai/gpt-4o system-prompt: You are a senior CI engineer. Be concise. prompt-file: ./prompt.txt - name: Ensure labels exist uses: actions/github-script@v7 with: script: | const owner = context.repo.owner; const repo = context.repo.repo; const labels = [ { name: \u0026#39;ci-failure\u0026#39;, color: \u0026#39;B60205\u0026#39;, description: \u0026#39;CI pipeline failure\u0026#39; }, { name: \u0026#39;needs-triage\u0026#39;, color: \u0026#39;D4C5F9\u0026#39;, description: \u0026#39;Requires triage\u0026#39; }, ]; for (const l of labels) { try { await github.rest.issues.getLabel({ owner, repo, name: l.name }); } catch { await github.rest.issues.createLabel({ owner, repo, name: l.name, color: l.color, description: l.description }); } } - name: Create issue id: issue uses: actions/github-script@v7 env: BRANCH: ${{ steps.vars.outputs.branch }} RUN_URL: ${{ steps.vars.outputs.run_url }} SHA: ${{ steps.vars.outputs.sha }} AI: ${{ steps.ai.outputs.response }} with: script: | const owner = context.repo.owner; const repo = context.repo.repo; const title = `CI failure: ${(context.payload.workflow_run?.name ?? \u0026#39;CI\u0026#39;)} on ${process.env.BRANCH}`; const ai = process.env.AI || \u0026#39;\u0026#39;; const summary = ai.trim() ? ai : \u0026#39;No AI summary produced\u0026#39;; const body = `### AI summary\\n\\n${summary}\\n\\n---\\n\\nRun: ${process.env.RUN_URL}\\nCommit: ${process.env.SHA}`; const res = await github.rest.issues.create({ owner, repo, title, body, labels: [\u0026#39;ci-failure\u0026#39;,\u0026#39;needs-triage\u0026#39;] }); core.setOutput(\u0026#39;number\u0026#39;, res.data.number.toString()); # Optional: assign to Copilot coding agent with a user PAT that can assign issues - name: Assign to Copilot coding agent (optional) if: ${{ env.PAT_WITH_ISSUES_WRITE != \u0026#39;\u0026#39; }} uses: actions/github-script@v7 env: USER_PAT: ${{ secrets.PAT_WITH_ISSUES_WRITE }} ISSUE_NUMBER: ${{ steps.issue.outputs.number }} with: github-token: ${{ env.USER_PAT }} script: | const owner = context.repo.owner; const repo = context.repo.repo; const issue_number = Number(process.env.ISSUE_NUMBER); // GraphQL to get suggested actors const actorsRes = await github.graphql( `query($owner:String!,$name:String!){ repository(owner:$owner, name:$name) { suggestedActors(capabilities:[CAN_BE_ASSIGNED], first:100) { nodes { login __typename ... on Bot { id } ... on User { id } } } } }`, { owner, name: repo } ); const candidates = [\u0026#39;copilot-swe-agent\u0026#39;,\u0026#39;copilot-agent\u0026#39;,\u0026#39;copilot\u0026#39;]; const nodes = actorsRes?.repository?.suggestedActors?.nodes || []; const match = nodes.find(n =\u0026gt; candidates.includes(n.login)); if (!match) { core.info(\u0026#39;Copilot agent not suggested. Skipping assignment.\u0026#39;); return; } // Get issue node id const issueRes = await github.graphql( `query($owner:String!,$name:String!,$num:Int!){ repository(owner:$owner, name:$name) { issue(number:$num) { id } } }`, { owner, name: repo, num: issue_number } ); const issueId = issueRes?.repository?.issue?.id; if (!issueId) { core.warning(\u0026#39;Issue ID not found\u0026#39;); return; } // Assign via GraphQL await github.graphql( `mutation($id:ID!,$actor:ID!){ replaceActorsForAssignable(input:{assignableId:$id, actorIds:[$actor]}) { assignable { __typename } } }`, { id: issueId, actor: match.id } ); Notes:\nAPI host: models.github.ai supports org-scoped calls at /orgs/{org}/inference/chat/completions if you need attribution. Headers: include Authorization: Bearer \u0026lt;token\u0026gt;, Accept: application/json, Content-Type: application/json, and X-GitHub-Api-Version. Model IDs are publisher/name (e.g., openai/gpt-4.1). Switch models freely in the body. Logs endpoint returns a 302 to a temporary zip URL; using curl -L or -sSL follows it. gh run download extracts artifacts by name into subfolders. Use --name to filter if needed. Troubleshooting 401/403 from Models: ensure your token has access to GitHub Models. For fine-grained PATs, add models:read. Empty AI response: check .choices[0].message.content; if null, inspect response.json for errors. Copilot coding agent assignment fails: verify your plan (Pro+ or Enterprise) and that the agent is enabled for the repo. Logs too large: adjust head/tail counts or filter to failing job logs only. Next steps Enrich the prompt with structured test failures: parse TRX or JUnit XML and include a concise table of failing tests. Attach summary.md, the prompt, and key logs as issue attachments or comments. Enable the Copilot coding agent for your repo and flip the assignment step to true to let Copilot propose a fix. References GitHub Models: API announcement (chat completions): GitHub Models API now available GitHub Models: product page and catalog: GitHub Models GitHub Actions: Download workflow run logs endpoint: REST API: Download workflow run logs GitHub Actions: Using workflow run logs: Using workflow run logs GitHub CLI: gh run download manual: gh run download Copilot coding agent: Using Copilot to work on an issue: Using Copilot to work on an issue Copilot coding agent: Troubleshooting: Troubleshooting Copilot coding agent ","date":"2025-08-11T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/ai-triage-github-actions-with-copilot/cover_hu_f7f192096777d5fb.jpg","image":"https://quintelier.dev/posts/2025/08/ai-triage-github-actions-with-copilot/cover_hu_1d6f197b8ace3b89.jpg","permalink":"https://quintelier.dev/posts/2025/08/ai-triage-github-actions-with-copilot/","title":"Auto-triage CI failures with Copilot","webpImage":"https://quintelier.dev/posts/2025/08/ai-triage-github-actions-with-copilot/cover_hu_fb3bdd983ca25bb7.webp"},{"content":"Welcome to the GitHub Certification Journey! This comprehensive guide covers everything you need to master the GitHub Foundations (GH-900) certification. Whether you\u0026rsquo;re new to Git and GitHub or looking to validate your foundational knowledge, this post provides complete coverage of all exam domains with practical examples and hands-on exercises.\nWhy GitHub Foundations Certification Matters The GitHub Foundations certification validates your understanding of Git version control and GitHub collaboration fundamentals. As the entry point to GitHub\u0026rsquo;s certification pathway, it demonstrates your ability to:\nUse Git for version control effectively Collaborate on projects using GitHub workflows Manage repositories and understand GitHub\u0026rsquo;s ecosystem Apply basic DevOps principles with GitHub tools Career Impact: This certification opens doors to development, DevOps, and technical roles where Git and GitHub knowledge is essential. It\u0026rsquo;s particularly valuable for developers, project managers, and anyone working in modern software development environments.\nExam Overview Exam Details Duration: 100 minutes Questions: Approximately 75 questions Question Types: Multiple choice and scenario-based questions Passing Score: 700 out of 1000 points Cost: ¬£99 (may vary by region) Delivery: Online proctored or at testing centres Prerequisites: None (foundational level) What to Expect The exam tests practical knowledge rather than memorisation. You\u0026rsquo;ll encounter scenarios about:\nChoosing appropriate Git commands for specific situations Understanding GitHub workflow implications Solving collaboration challenges Configuring repository settings appropriately Skills Measured: Complete Domain Breakdown Let\u0026rsquo;s dive deep into each domain, covering every skill you need for exam success.\nDomain 1: Introduction to Git and GitHub (22%) This is the largest domain, focusing on fundamental Git concepts and GitHub navigation.\nUnderstanding Git Basics What Git Is Git is a distributed version control system that tracks changes in files and coordinates work among multiple people. Key concepts:\nRepository (Repo): A directory containing your project files and Git metadata Commit: A snapshot of your project at a specific point in time Branch: A parallel version of your repository Merge: Combining changes from different branches Basic Git Workflow The fundamental Git workflow follows this pattern:\nModify files in your working directory Stage changes using git add Commit staged changes using git commit Push commits to remote repository using git push # Example workflow git add filename.txt # Stage specific file git add . # Stage all changes git commit -m \u0026#34;Add new feature\u0026#34; # Commit with message git push origin main # Push to remote repository Local vs Remote Repositories Local Repository: Exists on your computer, contains complete project history Remote Repository: Hosted on GitHub (or other platforms), enables collaboration Clone: Creates local copy of remote repository Fork: Creates your own copy of someone else\u0026rsquo;s repository Essential Git Commands Repository Initialisation and Cloning # Initialise new repository git init # Clone existing repository git clone https://github.com/username/repository.git # Clone to specific directory git clone https://github.com/username/repository.git my-project Basic File Operations # Check repository status git status # Add files to staging area git add filename.txt # Specific file git add *.js # All JavaScript files git add . # All changes # Commit changes git commit -m \u0026#34;Descriptive commit message\u0026#34; git commit -am \u0026#34;Add and commit in one step\u0026#34; # For tracked files # View commit history git log git log --oneline # Compact format git log --graph # Visual representation Remote Repository Operations # Add remote repository git remote add origin https://github.com/username/repository.git # View remote repositories git remote -v # Push changes to remote git push origin main # Push to main branch git push -u origin main # Set upstream tracking # Pull changes from remote git pull origin main # Fetch and merge git fetch origin # Fetch without merging Branching and Merging # Create new branch git branch feature-branch git checkout -b feature-branch # Create and switch # Switch branches git checkout main git switch feature-branch # Modern alternative # List branches git branch # Local branches git branch -r # Remote branches git branch -a # All branches # Merge branches git checkout main git merge feature-branch # Delete branch git branch -d feature-branch # Delete merged branch git branch -D feature-branch # Force delete Navigating GitHub Account Creation and Setup Understanding GitHub account types and initial setup:\nPersonal Account: Free tier includes unlimited public/private repositories Organization Account: For teams and businesses GitHub Enterprise: Advanced features for large organisations Essential Profile Setup:\n# Example README.md for profile ## Hi there üëã I\u0026#39;m a developer passionate about: - Cloud technologies - Open source contributions - DevOps practices ### Current Projects - [Project Name](link) - Brief description Repository Management Key repository concepts for the exam:\nRepository Types: Public (visible to everyone) vs Private (restricted access) Repository Structure: README.md, LICENSE, .gitignore, code organisation Repository Settings: Access permissions, branch protection, webhooks GitHub Interface Navigation Essential interface elements:\nRepository tabs: Code, Issues, Pull requests, Actions, Projects, Wiki, Settings File browser: Navigate code, view history, edit files online Search functionality: Code search, repository search, user search Issues and Pull Requests Fundamentals Issues are used for:\nBug reports and feature requests Task tracking and project planning Discussions and questions Pull Requests enable:\nCode review processes Collaborative development Merge approval workflows Domain 2: Working with GitHub Repositories (8%) This domain focuses on practical repository management skills.\nManaging Repository Settings Repository Configuration Essential settings you need to understand:\n# .github/dependabot.yml example version: 2 updates: - package-ecosystem: \u0026#34;npm\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;weekly\u0026#34; Access and Permissions Repository permission levels:\nRead: View and clone repository Triage: Manage issues and pull requests Write: Push changes and manage some settings Maintain: Manage repository without access to sensitive actions Admin: Full repository access Repository Templates Creating reusable repository structures:\nTemplate repositories for consistent project setup Including standard files: README, LICENSE, .gitignore Automated repository creation from templates Working with Files Adding and Editing Files Multiple methods for file management:\nVia GitHub Web Interface:\nNavigate to repository Click \u0026ldquo;Add file\u0026rdquo; or \u0026ldquo;Create new file\u0026rdquo; Edit content using built-in editor Commit changes directly Via Git Commands:\n# Create new file touch newfile.txt echo \u0026#34;Content\u0026#34; \u0026gt; newfile.txt # Edit existing file (using editor) nano README.md vim config.json # Stage and commit git add newfile.txt git commit -m \u0026#34;Add new configuration file\u0026#34; File Versioning Understanding file history and versioning:\nBlame view: See who changed each line and when History view: Complete change history for files Diff view: Compare different versions GitHub Desktop Integration Key features of GitHub Desktop:\nVisual diff representation Simplified branching and merging Sync with GitHub repositories Conflict resolution interface Domain 3: Collaboration Features (30%) This is the most substantial domain, covering GitHub\u0026rsquo;s collaborative capabilities.\nGitHub Collaboration Workflows Forking Repositories The fork workflow is essential for open source contribution:\nFork the original repository to your account Clone your fork locally Create a feature branch Make changes and commit Push to your fork Create pull request to original repository # Complete fork workflow example git clone https://github.com/yourusername/forked-repo.git cd forked-repo git checkout -b feature-improvement # Make changes git add . git commit -m \u0026#34;Implement feature improvement\u0026#34; git push origin feature-improvement # Create PR via GitHub interface Pull Request Lifecycle Creating Pull Requests:\nClear title and description Reference related issues Include testing information Request specific reviewers Pull Request Template Example:\n## Description Brief description of changes ## Type of Change - [ ] Bug fix - [ ] New feature - [ ] Breaking change - [ ] Documentation update ## Testing - [ ] Tests pass locally - [ ] Added new tests - [ ] Manual testing completed ## Checklist - [ ] Code follows style guidelines - [ ] Self-review completed - [ ] Documentation updated Managing Pull Requests:\nDraft Pull Requests: Work-in-progress marker Review Requests: Assign specific reviewers Status Checks: Automated testing integration Merge Options: Merge commit, squash and merge, rebase and merge Reviewing Pull Requests:\nLine-by-line comments: Specific feedback on code changes General comments: Overall feedback and suggestions Review states: Approve, request changes, or comment only Suggested changes: Propose specific code modifications Merging Pull Requests:\nMerge Commit: Preserves complete history Squash and Merge: Combines commits into single commit Rebase and Merge: Replays commits without merge commit GitHub Actions for CI/CD Basic GitHub Actions Concepts GitHub Actions automates software workflows:\nWorkflow: Automated process triggered by events Job: Set of steps executed on same runner Step: Individual task within job Action: Reusable unit of code Simple Workflow Example:\n# .github/workflows/ci.yml name: Continuous Integration on: push: branches: [ main ] pull_request: branches: [ main ] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Set up Node.js uses: actions/setup-node@v3 with: node-version: \u0026#39;18\u0026#39; - name: Install dependencies run: npm install - name: Run tests run: npm test Common Workflow Triggers:\npush: Code pushed to repository pull_request: Pull request created or updated schedule: Time-based triggers workflow_dispatch: Manual trigger Project Management with GitHub Issues Management Creating Effective Issues:\n# Bug Report Template ## Bug Description Clear description of the bug ## Steps to Reproduce 1. Go to... 2. Click on... 3. See error ## Expected Behavior What should happen ## Actual Behavior What actually happened ## Environment - OS: [e.g. Windows 10] - Browser: [e.g. Chrome 91] - Version: [e.g. 1.0.0] Issue Labels Standard label categories:\nType: bug, enhancement, documentation Priority: high, medium, low Status: in progress, needs review, blocked Area: frontend, backend, testing Milestones Organising issues and PRs by:\nVersion releases: v1.0.0, v1.1.0 Sprint goals: Sprint 1, Sprint 2 Feature sets: User authentication, Payment system GitHub Projects Project Boards:\nBasic Kanban: To Do, In Progress, Done Advanced Workflows: Multiple swimlanes and automation Custom Fields: Priority, assignee, labels integration Project Automation:\n# Example project automation - Move issues to \u0026#34;In Progress\u0026#34; when assigned - Move PRs to \u0026#34;Review\u0026#34; when opened - Move items to \u0026#34;Done\u0026#34; when closed Tracking Progress:\nBurndown charts: Visual progress representation Velocity tracking: Team performance metrics Cross-repository projects: Multi-repo coordination Domain 4: Modern Development (13%) This domain covers DevOps practices and code review processes.\nImplementing DevOps Practices DevOps Principles in GitHub Core principles supported by GitHub:\nCollaboration: Issues, PRs, and team features Automation: GitHub Actions and workflow automation Continuous Integration: Automated testing and building Continuous Delivery: Automated deployment pipelines Monitoring: Insights and analytics CI/CD Pipeline Implementation Continuous Integration Example:\nname: CI Pipeline on: push: branches: [ main, develop ] pull_request: branches: [ main ] jobs: lint: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Lint code run: | npm install npm run lint test: runs-on: ubuntu-latest needs: lint steps: - uses: actions/checkout@v3 - name: Run tests run: | npm install npm test build: runs-on: ubuntu-latest needs: [lint, test] steps: - uses: actions/checkout@v3 - name: Build application run: | npm install npm run build Continuous Deployment Example:\nname: Deploy to Production on: push: branches: [ main ] jobs: deploy: runs-on: ubuntu-latest if: github.ref == \u0026#39;refs/heads/main\u0026#39; steps: - uses: actions/checkout@v3 - name: Deploy to Azure uses: azure/webapps-deploy@v2 with: app-name: \u0026#39;my-app\u0026#39; publish-profile: ${{ secrets.AZURE_WEBAPP_PUBLISH_PROFILE }} GitHub Actions Automation Common automation patterns:\nAutomated testing: Run test suites on every commit Code quality checks: ESLint, SonarQube integration Security scanning: Dependency vulnerability checks Documentation generation: Auto-update docs from code Code Review Best Practices Effective Code Reviews Review Checklist:\nFunctionality: Does the code do what it\u0026rsquo;s supposed to do? Performance: Are there any performance concerns? Security: Are there security vulnerabilities? Maintainability: Is the code readable and well-documented? Testing: Are there adequate tests? Review Tools in GitHub:\nDiff view: Side-by-side or unified diff options Comment threads: Discussions on specific lines Review summary: Overall approval or change requests Suggested changes: Propose specific code modifications Code Review Workflow:\n# Reviewer workflow git fetch origin git checkout pr-branch-name git pull origin pr-branch-name # Review changes locally # Test functionality # Leave review comments via GitHub Review Response Best Practices:\nBe constructive: Focus on code improvement Be specific: Point out exact issues and solutions Be timely: Respond to reviews promptly Be open: Accept feedback gracefully Domain 5: Project Management (7%) Focus on GitHub\u0026rsquo;s project management capabilities.\nGitHub Projects Deep Dive Project Types Understanding different project approaches:\nRepository Projects: Scoped to single repository Organization Projects: Cross-repository coordination User Projects: Personal project management Project Views Different ways to visualise work:\nTable View: Spreadsheet-like data organisation Board View: Kanban-style workflow boards Roadmap View: Timeline-based planning Custom Fields Enhancing projects with metadata:\n# Example custom fields - Priority: High, Medium, Low - Story Points: 1, 2, 3, 5, 8 - Component: Frontend, Backend, Database - Reviewer: Team member assignments Project Automation Automated workflow examples:\n# Move items based on status - When PR is opened ‚Üí Move to \u0026#34;In Review\u0026#34; - When issue is closed ‚Üí Move to \u0026#34;Done\u0026#34; - When issue is labeled \u0026#34;bug\u0026#34; ‚Üí Set priority to \u0026#34;High\u0026#34; Integration with Issues and Pull Requests Linking Work Items Connecting projects to development work:\n# In issue or PR description Closes #123 Fixes #456 Resolves #789 # Multiple issues Closes #123, #456, #789 Project Board Integration:\nAutomatic addition: New issues automatically added Status synchronisation: Board status reflects issue state Cross-repository tracking: Issues from multiple repos Progress Tracking:\nCompletion percentage: Visual progress indicators Milestone integration: Align projects with release milestones Reporting: Progress reports and team analytics Domain 6: Privacy, Security, and Administration (10%) Critical security and administrative concepts.\nRepository Security Branch Protection Rules Essential protections for important branches:\n{ \u0026#34;required_status_checks\u0026#34;: { \u0026#34;strict\u0026#34;: true, \u0026#34;contexts\u0026#34;: [\u0026#34;ci/test\u0026#34;, \u0026#34;ci/lint\u0026#34;] }, \u0026#34;enforce_admins\u0026#34;: true, \u0026#34;required_pull_request_reviews\u0026#34;: { \u0026#34;required_approving_review_count\u0026#34;: 2, \u0026#34;dismiss_stale_reviews\u0026#34;: true, \u0026#34;require_code_owner_reviews\u0026#34;: true }, \u0026#34;restrictions\u0026#34;: null } Key Protection Features:\nRequire pull request reviews: Mandatory code review Require status checks: CI must pass before merge Require branches to be up to date: Prevent stale merges Include administrators: Apply rules to all users Dependabot Security Automated dependency management:\n# .github/dependabot.yml version: 2 updates: - package-ecosystem: \u0026#34;npm\u0026#34; directory: \u0026#34;/\u0026#34; schedule: interval: \u0026#34;daily\u0026#34; open-pull-requests-limit: 10 reviewers: - \u0026#34;security-team\u0026#34; assignees: - \u0026#34;lead-developer\u0026#34; Security Features:\nVulnerability alerts: Automatic security notifications Security updates: Automated dependency updates Security advisories: Community vulnerability reports Access Control and Permissions Repository Permissions:\nPublic repositories: Visible to everyone, configurable contributions Private repositories: Restricted access, team-based permissions Internal repositories: Visible to organization members only Permission Levels:\nRead: Clone and fetch, create issues and comments Triage: Manage issues and pull requests without write access Write: Push to repository, manage issues and PRs Maintain: Manage repository settings except sensitive actions Admin: Full repository access including deletion GitHub Organizations Organization Structure Understanding organizational hierarchy:\nOrganization: Top-level container for repositories and teams Teams: Groups of users with specific permissions Repositories: Individual project containers Members: Users with organization access Creating and Managing Organizations:\n# Organization setup considerations - Organization name and description - Billing and plan selection - Member invitation and permissions - Team structure planning - Repository organization strategy Organization Security Settings:\nTwo-factor authentication: Require 2FA for all members SSH key management: Organization-wide SSH policies OAuth app restrictions: Control third-party app access SAML SSO: Enterprise identity integration Team Management:\nTeam creation: Organize members by project or function Team permissions: Assign repository access levels Team mentions: Use @team-name for group communication Team discussions: Private team communication channels Domain 7: Benefits of the GitHub Community (10%) Understanding GitHub\u0026rsquo;s community aspects and contribution culture.\nOpen Source Participation Contributing to Open Source Steps for meaningful contribution:\nFind projects: Use GitHub search and trending repositories Understand contribution guidelines: Read CONTRIBUTING.md Start small: Fix typos, improve documentation Follow project conventions: Code style, commit messages Engage with maintainers: Ask questions, seek guidance Contribution Types:\nCode contributions: Bug fixes, feature implementations Documentation: README improvements, API documentation Testing: Test case additions, bug reports Translation: Localisation and internationalisation Design: UI/UX improvements, graphics Good First Issue Practice:\n# Example good first issue ## Title: Add dark mode toggle to settings page ## Description Users have requested a dark mode option. We need to add a toggle switch to the settings page that switches between light and dark themes. ## Technical Details - Add toggle component to settings.js - Update CSS with dark theme variables - Store preference in localStorage ## Acceptance Criteria - [ ] Toggle appears in settings - [ ] Theme switches immediately - [ ] Preference persists across sessions Labels: good first issue, enhancement, frontend GitHub Discussions Community Engagement Using Discussions for community building:\nQ\u0026amp;A: Community support and knowledge sharing Ideas: Feature requests and project direction General: Open-ended community conversations Show and tell: Community showcases and demos Discussion Best Practices:\nClear titles: Make discussions discoverable Detailed descriptions: Provide context and background Engage actively: Respond to community input Moderate effectively: Maintain productive conversations Community Guidelines:\n# Example community guidelines ## Our Standards - Be respectful and inclusive - Stay on topic and relevant - Help others learn and grow - Follow the code of conduct ## Reporting Issues - Use issue templates - Provide reproducible examples - Search existing issues first - Be patient with responses Community Contribution Culture Open Source Etiquette Professional contribution practices:\nRead documentation: Understand project goals and conventions Start with issues: Don\u0026rsquo;t submit unsolicited major changes Follow coding standards: Match existing code style Write clear commit messages: Explain what and why Be patient: Maintainers are often volunteers Building Reputation:\nConsistent contributions: Regular, quality contributions Helpful reviews: Constructive feedback on others\u0026rsquo; work Documentation improvements: Often overlooked but valuable Community support: Helping other contributors Maintainer Perspective: Understanding the maintainer\u0026rsquo;s role:\nProject vision: Maintaining project direction and quality Community management: Fostering positive contributor environment Release management: Planning and executing releases Issue triage: Prioritising and categorising community input Hands-On Practice Exercises To solidify your understanding, complete these practical exercises:\nExercise 1: Repository Setup and Basic Git Operations # Create a new repository git init my-practice-repo cd my-practice-repo # Create initial files echo \u0026#34;# My Practice Repository\u0026#34; \u0026gt; README.md echo \u0026#34;node_modules/\u0026#34; \u0026gt; .gitignore echo \u0026#34;*.log\u0026#34; \u0026gt;\u0026gt; .gitignore # Stage and commit git add . git commit -m \u0026#34;Initial commit with README and gitignore\u0026#34; # Create and work with branches git checkout -b feature-branch echo \u0026#34;console.log(\u0026#39;Hello, GitHub!\u0026#39;);\u0026#34; \u0026gt; app.js git add app.js git commit -m \u0026#34;Add basic JavaScript file\u0026#34; # Switch back to main and merge git checkout main git merge feature-branch git branch -d feature-branch Exercise 2: GitHub Workflow Practice Create a GitHub repository with proper README, LICENSE, and .gitignore Set up branch protection for the main branch Create an issue using a template Create a pull request with proper description and linking to the issue Review and merge the pull request using different merge strategies Exercise 3: GitHub Actions Setup Create a simple CI workflow:\n# .github/workflows/practice.yml name: Practice Workflow on: push: branches: [ main ] pull_request: branches: [ main ] jobs: practice: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v3 - name: Print repository info run: | echo \u0026#34;Repository: ${{ github.repository }}\u0026#34; echo \u0026#34;Branch: ${{ github.ref }}\u0026#34; echo \u0026#34;Event: ${{ github.event_name }}\u0026#34; - name: List files run: ls -la Study Strategy and Timeline 4-Week Study Plan Week 1: Git Fundamentals Day 1-2: Git concepts and basic commands Day 3-4: Branching and merging Day 5-7: Practice exercises and command practice Week 2: GitHub Features Day 1-2: Repository management and navigation Day 3-4: Issues and pull requests Day 5-7: Collaboration workflows and practice Week 3: Advanced Features Day 1-2: GitHub Actions basics Day 3-4: Project management features Day 5-7: Security and administration Week 4: Integration and Review Day 1-2: Community features and open source Day 3-4: Practice exams and weak area review Day 5-7: Final review and exam preparation Study Resources Official GitHub Resources:\nGitHub Skills - Interactive learning paths GitHub Docs - Comprehensive documentation Git Handbook - Git fundamentals Practice Platforms:\nGitHub Learning Lab - Hands-on courses Git Immersion - Git tutorial walkthrough Learn Git Branching - Interactive Git visualization Community Resources:\nGitHub Community Forum - Community support GitHub YouTube Channel - Official tutorials Git Documentation - Official Git documentation Exam Preparation Tips Effective Study Techniques Active Learning:\nPractice regularly: Use Git and GitHub daily Teach others: Explain concepts to reinforce understanding Build projects: Apply skills in real scenarios Join communities: Engage with other learners Mock Exams and Practice:\nScenario-based practice: Work through realistic situations Time management: Practice under exam time constraints Weak area focus: Identify and strengthen weak points Official practice tests: Use Microsoft Learn practice assessments Common Exam Pitfalls Areas of Confusion:\nGit vs GitHub: Understanding the distinction and relationship Merge strategies: When to use different merge options Permission levels: Understanding granular access controls Workflow triggers: GitHub Actions trigger conditions Time Management:\nRead carefully: Understand what each question is asking Eliminate options: Rule out obviously incorrect answers Flag and return: Don\u0026rsquo;t spend too much time on difficult questions Review answers: Use remaining time to review flagged questions Career Benefits and Next Steps Job Opportunities Roles Enhanced by GitHub Foundations:\nSoftware Developer: Version control and collaboration skills DevOps Engineer: Understanding of CI/CD and automation Project Manager: Technical project management capabilities Technical Writer: Documentation and collaboration workflows Salary Impact: According to industry surveys, professionals with Git and GitHub skills command 15-25% higher salaries than those without version control experience.\nCertification Pathway Next Recommended Certifications:\nFor Developers: GitHub Actions (GH-200) or GitHub Copilot (GH-300) For DevOps Engineers: GitHub Actions (GH-200) then GitHub Administration (GH-100) For Administrators: GitHub Administration (GH-100) For Security Professionals: GitHub Administration (GH-100) then GitHub Advanced Security (GH-500) Continuing Education Stay Current:\nGitHub Blog: Follow product updates and new features GitHub Universe: Annual conference with latest announcements Release Notes: Stay updated with new GitHub capabilities Community Discussions: Engage with other professionals Conclusion The GitHub Foundations (GH-900) certification validates essential skills for modern software development and collaboration. By mastering Git version control, GitHub workflows, project management, and community participation, you\u0026rsquo;ll build a strong foundation for advanced GitHub certifications and enhance your career prospects.\nKey Takeaways:\nGit fundamentals are essential for all development work GitHub collaboration enables effective team development Project management features streamline development workflows Security practices protect code and communities Community engagement accelerates learning and career growth Next Steps:\nComplete hands-on exercises to reinforce learning Practice with real repositories to gain experience Schedule your exam when you feel confident Plan your next certification based on career goals Remember: The exam tests practical application, not memorisation. Focus on understanding workflows and real-world scenarios rather than memorising commands. Good luck with your GitHub Foundations certification journey!\nReady to take the next step in your GitHub certification journey? Check out our upcoming posts covering GitHub Administration (GH-100) and GitHub Actions (GH-200) certifications.\n","date":"2025-08-08T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-900/cover_hu_6b285d32356ff907.jpg","image":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-900/cover_hu_56a521c0c5a0ea43.jpg","permalink":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-900/","title":"üêô GitHub Certification Journey: Part 1 - GitHub Foundations (GH-900)","webpImage":"https://quintelier.dev/posts/2025/08/github-certification-journey-gh-900/cover_hu_c5b06b2099e41853.webp"},{"content":"Introduction Imagine having an AI assistant that understands your energy consumption patterns, solar production, and weather data ‚Äî all running locally on your machine with zero cloud dependencies. No API keys, no subscription fees, no data leaving your premises.\nIn this hands-on guide, we\u0026rsquo;ll build a Local Energy Assistant using C#, the open-source GPT-OSS-20B model, Retrieval-Augmented Generation (RAG), and custom tool calling. Perfect for privacy-conscious developers who want intelligent insights from their smart meter data.\nWhat We\u0026rsquo;re Building Our AI Energy Copilot will provide:\nSolar Production Analysis: Identify peak generation days and weather correlations Gas Usage Intelligence: Compare consumption patterns across weather conditions Grid Import/Export Insights: Calculate net usage and energy balance Weather Context: Link energy patterns to historical weather data Tool-Based Actions: Execute specific queries via natural language commands Complete Privacy: Everything runs locally with your own data Prerequisites Before we begin, ensure you have:\n.NET SDK 9+ installed For Testing/Development: GitHub account with access to GitHub Models (testing platform for comparing AI models) For Production: Ollama for running local LLMs Your energy data in JSON format (smart meter exports, solar inverter logs, etc.) Basic understanding of C# and JSON Why GPT-OSS for Energy Analysis? OpenAI\u0026rsquo;s GPT-OSS represents a breakthrough in open-source AI, specifically designed for local and specialized use cases like our energy assistant:\nOpen and Permissive Apache 2.0 License: Build freely without copyleft restrictions or patent risk Commercial Use: Perfect for personal projects and commercial energy management tools No Vendor Lock-in: Complete control over your AI infrastructure Optimised for Local Deployment 20B Parameter Model: Designed for consumer hardware (runs within 16GB memory) Native MXFP4 Quantization: Efficient memory usage through built-in quantization Official Ollama Support: Seamless integration with local deployment tools Advanced Capabilities for Energy Analysis Native Tool Calling: Built-in support for function calling and agentic operations Configurable Reasoning: Adjust reasoning effort (low, medium, high) based on query complexity Full Chain-of-Thought: Complete access to the model\u0026rsquo;s reasoning process for debugging Fine-tunable: Customize for domain-specific energy analysis tasks This makes GPT-OSS ideal for analyzing personal energy data while maintaining complete privacy and control.\nSetting Up LLM Options Option 1: Development Testing with GitHub Models Important: GitHub Models is a testing platform/environment for developers to test and compare different AI models - not suitable for production workloads.\nGet your GitHub Personal Access Token:\nVisit GitHub Settings \u0026gt; Developer settings \u0026gt; Personal access tokens Create a token with model scope Set environment variable: # Windows set GITHUB_TOKEN=your_token_here # Linux/macOS export GITHUB_TOKEN=your_token_here Option 2: Production Deployment with Ollama For production deployment:\n# Install and run GPT-OSS-20B locally ollama pull gpt-oss:20b ollama run gpt-oss:20b This starts a local OpenAI-compatible API endpoint at http://localhost:11434/v1.\nPreparing Your Energy Dataset Create a data/energy.json file with your energy consumption data. Here\u0026rsquo;s a sample structure:\n{ \u0026#34;2025-08-01\u0026#34;: { \u0026#34;solar\u0026#34;: 4.2, \u0026#34;grid_consumed\u0026#34;: 5.1, \u0026#34;grid_injected\u0026#34;: 1.8, \u0026#34;gas_m3\u0026#34;: 2.4, \u0026#34;weather\u0026#34;: { \u0026#34;temperature\u0026#34;: 23.5, \u0026#34;condition\u0026#34;: \u0026#34;sunny\u0026#34; } }, \u0026#34;2025-08-02\u0026#34;: { \u0026#34;solar\u0026#34;: 0.3, \u0026#34;grid_consumed\u0026#34;: 6.2, \u0026#34;grid_injected\u0026#34;: 0.0, \u0026#34;gas_m3\u0026#34;: 4.1, \u0026#34;weather\u0026#34;: { \u0026#34;temperature\u0026#34;: 16.0, \u0026#34;condition\u0026#34;: \u0026#34;rainy\u0026#34; } }, \u0026#34;2025-08-03\u0026#34;: { \u0026#34;solar\u0026#34;: 3.8, \u0026#34;grid_consumed\u0026#34;: 4.9, \u0026#34;grid_injected\u0026#34;: 1.2, \u0026#34;gas_m3\u0026#34;: 1.8, \u0026#34;weather\u0026#34;: { \u0026#34;temperature\u0026#34;: 21.0, \u0026#34;condition\u0026#34;: \u0026#34;partly_cloudy\u0026#34; } } } Data Sources: You can extract this from smart meter readings, solar inverter APIs, or manual logging. Many energy providers offer CSV exports that you can convert to this JSON format.\nCreating the Flexible Console Application Let\u0026rsquo;s set up our .NET project with support for both deployment modes:\ndotnet new console -n EnergyAssistant cd EnergyAssistant dotnet add package Microsoft.Extensions.Configuration dotnet add package Microsoft.Extensions.Configuration.Json Create an appsettings.json configuration file:\n{ \u0026#34;LlmProvider\u0026#34;: \u0026#34;GitHub\u0026#34;, // \u0026#34;GitHub\u0026#34; for model testing/comparison, \u0026#34;Ollama\u0026#34; for production \u0026#34;GitHub\u0026#34;: { \u0026#34;BaseUrl\u0026#34;: \u0026#34;https://models.inference.ai.azure.com\u0026#34;, \u0026#34;Model\u0026#34;: \u0026#34;gpt-4o-mini\u0026#34;, // Try different models: gpt-4o, gpt-4o-mini, etc. \u0026#34;Token\u0026#34;: \u0026#34;\u0026#34; // Set via environment variable GITHUB_TOKEN }, \u0026#34;Ollama\u0026#34;: { \u0026#34;BaseUrl\u0026#34;: \u0026#34;http://localhost:11434/v1\u0026#34;, \u0026#34;Model\u0026#34;: \u0026#34;gpt-oss:20b\u0026#34;, \u0026#34;Token\u0026#34;: \u0026#34;local\u0026#34; } } Building the Tool System Create Tools.cs to handle energy-specific queries:\nusing System.Text.Json; namespace EnergyAssistant { public static class Tools { private static readonly Dictionary\u0026lt;string, JsonElement\u0026gt; data = JsonSerializer.Deserialize\u0026lt;Dictionary\u0026lt;string, JsonElement\u0026gt;\u0026gt;( File.ReadAllText(\u0026#34;data/energy.json\u0026#34;))!; public static string PeakSolarDay() { var maxEntry = data .OrderByDescending(d =\u0026gt; d.Value.GetProperty(\u0026#34;solar\u0026#34;).GetDouble()) .First(); var solar = maxEntry.Value.GetProperty(\u0026#34;solar\u0026#34;).GetDouble(); var weather = maxEntry.Value.GetProperty(\u0026#34;weather\u0026#34;); var condition = weather.GetProperty(\u0026#34;condition\u0026#34;).GetString(); var temp = weather.GetProperty(\u0026#34;temperature\u0026#34;).GetDouble(); return $\u0026#34;‚òÄÔ∏è Peak solar production: {solar} kWh on {maxEntry.Key} \u0026#34; + $\u0026#34;(Weather: {condition}, {temp}¬∞C)\u0026#34;; } public static string CompareGasUsage() { var coldDays = data.Where(d =\u0026gt; d.Value.GetProperty(\u0026#34;weather\u0026#34;).GetProperty(\u0026#34;temperature\u0026#34;).GetDouble() \u0026lt; 15); var warmDays = data.Where(d =\u0026gt; d.Value.GetProperty(\u0026#34;weather\u0026#34;).GetProperty(\u0026#34;temperature\u0026#34;).GetDouble() \u0026gt;= 15); var coldAvg = coldDays.Any() ? coldDays.Average(d =\u0026gt; d.Value.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble()) : 0; var warmAvg = warmDays.Any() ? warmDays.Average(d =\u0026gt; d.Value.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble()) : 0; return $\u0026#34;üî• Average gas usage: {coldAvg:F2} m¬≥ (cold days \u0026lt; 15¬∞C) vs \u0026#34; + $\u0026#34;{warmAvg:F2} m¬≥ (warm days ‚â• 15¬∞C)\u0026#34;; } public static string NetUsage(string date) { if (!data.TryGetValue(date, out var day)) return $\u0026#34;‚ùå No data available for {date}\u0026#34;; var consumed = day.GetProperty(\u0026#34;grid_consumed\u0026#34;).GetDouble(); var injected = day.GetProperty(\u0026#34;grid_injected\u0026#34;).GetDouble(); var net = consumed - injected; var status = net \u0026gt; 0 ? \u0026#34;üìà Net consumer\u0026#34; : \u0026#34;üìâ Net producer\u0026#34;; return $\u0026#34;‚ö° {date}: {Math.Abs(net):F2} kWh ({status})\u0026#34;; } public static string DayDetails(string date) { if (!data.TryGetValue(date, out var day)) return $\u0026#34;‚ùå No data available for {date}\u0026#34;; var solar = day.GetProperty(\u0026#34;solar\u0026#34;).GetDouble(); var consumed = day.GetProperty(\u0026#34;grid_consumed\u0026#34;).GetDouble(); var injected = day.GetProperty(\u0026#34;grid_injected\u0026#34;).GetDouble(); var gas = day.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble(); var weather = day.GetProperty(\u0026#34;weather\u0026#34;); var temp = weather.GetProperty(\u0026#34;temperature\u0026#34;).GetDouble(); var condition = weather.GetProperty(\u0026#34;condition\u0026#34;).GetString(); return $\u0026#34;üìä {date} Summary:\\n\u0026#34; + $\u0026#34; Solar: {solar} kWh\\n\u0026#34; + $\u0026#34; Grid In: {consumed} kWh\\n\u0026#34; + $\u0026#34; Grid Out: {injected} kWh\\n\u0026#34; + $\u0026#34; Gas: {gas} m¬≥\\n\u0026#34; + $\u0026#34; Weather: {condition}, {temp}¬∞C\u0026#34;; } public static string OverallStats() { var totalDays = data.Count; var totalSolar = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;solar\u0026#34;).GetDouble()); var totalConsumed = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;grid_consumed\u0026#34;).GetDouble()); var totalInjected = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;grid_injected\u0026#34;).GetDouble()); var totalGas = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble()); return $\u0026#34;üìà Overall Statistics ({totalDays} days):\\n\u0026#34; + $\u0026#34; Total Solar: {totalSolar:F1} kWh\\n\u0026#34; + $\u0026#34; Total Grid Consumed: {totalConsumed:F1} kWh\\n\u0026#34; + $\u0026#34; Total Grid Injected: {totalInjected:F1} kWh\\n\u0026#34; + $\u0026#34; Total Gas: {totalGas:F1} m¬≥\\n\u0026#34; + $\u0026#34; Net Grid Usage: {(totalConsumed - totalInjected):F1} kWh\u0026#34;; } } } Implementing Flexible RAG and Tool Calling Now create the enhanced application logic in Program.cs:\nusing System.Text.Json; namespace EnergyAssistant { public static class Tools { private static readonly Dictionary\u0026lt;string, JsonElement\u0026gt; data = JsonSerializer.Deserialize\u0026lt;Dictionary\u0026lt;string, JsonElement\u0026gt;\u0026gt;( File.ReadAllText(\u0026#34;data/energy.json\u0026#34;))!; public static string PeakSolarDay() { var maxEntry = data .OrderByDescending(d =\u0026gt; d.Value.GetProperty(\u0026#34;solar\u0026#34;).GetDouble()) .First(); var solar = maxEntry.Value.GetProperty(\u0026#34;solar\u0026#34;).GetDouble(); var weather = maxEntry.Value.GetProperty(\u0026#34;weather\u0026#34;); var condition = weather.GetProperty(\u0026#34;condition\u0026#34;).GetString(); var temp = weather.GetProperty(\u0026#34;temperature\u0026#34;).GetDouble(); return $\u0026#34;‚òÄÔ∏è Peak solar production: {solar} kWh on {maxEntry.Key} \u0026#34; + $\u0026#34;(Weather: {condition}, {temp}¬∞C)\u0026#34;; } public static string CompareGasUsage() { var coldDays = data.Where(d =\u0026gt; d.Value.GetProperty(\u0026#34;weather\u0026#34;).GetProperty(\u0026#34;temperature\u0026#34;).GetDouble() \u0026lt; 15); var warmDays = data.Where(d =\u0026gt; d.Value.GetProperty(\u0026#34;weather\u0026#34;).GetProperty(\u0026#34;temperature\u0026#34;).GetDouble() \u0026gt;= 15); var coldAvg = coldDays.Any() ? coldDays.Average(d =\u0026gt; d.Value.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble()) : 0; var warmAvg = warmDays.Any() ? warmDays.Average(d =\u0026gt; d.Value.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble()) : 0; return $\u0026#34;üî• Average gas usage: {coldAvg:F2} m¬≥ (cold days \u0026lt; 15¬∞C) vs \u0026#34; + $\u0026#34;{warmAvg:F2} m¬≥ (warm days ‚â• 15¬∞C)\u0026#34;; } public static string NetUsage(string date) { if (!data.TryGetValue(date, out var day)) return $\u0026#34;‚ùå No data available for {date}\u0026#34;; var consumed = day.GetProperty(\u0026#34;grid_consumed\u0026#34;).GetDouble(); var injected = day.GetProperty(\u0026#34;grid_injected\u0026#34;).GetDouble(); var net = consumed - injected; var status = net \u0026gt; 0 ? \u0026#34;üìà Net consumer\u0026#34; : \u0026#34;üìâ Net producer\u0026#34;; return $\u0026#34;‚ö° {date}: {Math.Abs(net):F2} kWh ({status})\u0026#34;; } public static string DayDetails(string date) { if (!data.TryGetValue(date, out var day)) return $\u0026#34;‚ùå No data available for {date}\u0026#34;; var solar = day.GetProperty(\u0026#34;solar\u0026#34;).GetDouble(); var consumed = day.GetProperty(\u0026#34;grid_consumed\u0026#34;).GetDouble(); var injected = day.GetProperty(\u0026#34;grid_injected\u0026#34;).GetDouble(); var gas = day.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble(); var weather = day.GetProperty(\u0026#34;weather\u0026#34;); var temp = weather.GetProperty(\u0026#34;temperature\u0026#34;).GetDouble(); var condition = weather.GetProperty(\u0026#34;condition\u0026#34;).GetString(); return $\u0026#34;üìä {date} Summary:\\n\u0026#34; + $\u0026#34; Solar: {solar} kWh\\n\u0026#34; + $\u0026#34; Grid In: {consumed} kWh\\n\u0026#34; + $\u0026#34; Grid Out: {injected} kWh\\n\u0026#34; + $\u0026#34; Gas: {gas} m¬≥\\n\u0026#34; + $\u0026#34; Weather: {condition}, {temp}¬∞C\u0026#34;; } public static string OverallStats() { var totalDays = data.Count; var totalSolar = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;solar\u0026#34;).GetDouble()); var totalConsumed = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;grid_consumed\u0026#34;).GetDouble()); var totalInjected = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;grid_injected\u0026#34;).GetDouble()); var totalGas = data.Sum(d =\u0026gt; d.Value.GetProperty(\u0026#34;gas_m3\u0026#34;).GetDouble()); return $\u0026#34;üìà Overall Statistics ({totalDays} days):\\n\u0026#34; + $\u0026#34; Total Solar: {totalSolar:F1} kWh\\n\u0026#34; + $\u0026#34; Total Grid Consumed: {totalConsumed:F1} kWh\\n\u0026#34; + $\u0026#34; Total Grid Injected: {totalInjected:F1} kWh\\n\u0026#34; + $\u0026#34; Total Gas: {totalGas:F1} m¬≥\\n\u0026#34; + $\u0026#34; Net Grid Usage: {(totalConsumed - totalInjected):F1} kWh\u0026#34;; } } } Testing Your Flexible Energy Assistant Create the data directory and configuration:\nmkdir data # Add your energy.json file to the data directory # Ensure appsettings.json is in your project root Testing with GitHub Models (Development Only) Note: This is for development, testing, and comparing different AI models. Do not use in production.\nSet your token and run:\n# Set environment variable set GITHUB_TOKEN=your_token_here # Run with GitHub Models (model testing mode) dotnet run Running with Self-Hosted Ollama (Production Ready) Switch configuration and run:\n# Start Ollama in one terminal ollama run gpt-oss:20b # Update appsettings.json to use \u0026#34;Ollama\u0026#34; provider, then run dotnet run Runtime Provider Switching You can switch between providers while the application is running:\n‚ö° Flexible Energy Assistant üîó Using: GitHub (gpt-4o-mini) Type \u0026#39;switch\u0026#39; to change LLM provider Type \u0026#39;exit\u0026#39; to quit üí¨ You: switch üîÑ Switched to Ollama (gpt-oss:20b) üí¨ You: Show me my peak solar day Model Comparison on GitHub Models One key advantage of the GitHub Models testing platform is the ability to easily compare different AI models. You can test the same queries against multiple models by updating your appsettings.json:\n// Test with GPT-4o for maximum capability \u0026#34;Model\u0026#34;: \u0026#34;gpt-4o\u0026#34; // Test with GPT-4o-mini for speed and cost \u0026#34;Model\u0026#34;: \u0026#34;gpt-4o-mini\u0026#34; // Test with other available models \u0026#34;Model\u0026#34;: \u0026#34;llama-3.1-405b\u0026#34; // Or other available models This allows you to evaluate which model provides the best results for your specific energy analysis use case before committing to a production deployment.\nSample Interactions Here are some example conversations with your energy assistant:\nQuery: \u0026ldquo;When did I generate the most solar power?\u0026rdquo;\nResponse:\nü§ñ Assistant: Let me check your solar production data. {{tool: peak-solar-day}} üõ†Ô∏è Tool Result: ‚òÄÔ∏è Peak solar production: 4.2 kWh on 2025-08-01 (Weather: sunny, 23.5¬∞C) Query: \u0026ldquo;How does cold weather affect my gas usage?\u0026rdquo;\nResponse:\nü§ñ Assistant: I\u0026#39;ll analyse your gas consumption patterns across different temperatures. {{tool: compare-gas}} üõ†Ô∏è Tool Result: üî• Average gas usage: 4.10 m¬≥ (cold days \u0026lt; 15¬∞C) vs 2.10 m¬≥ (warm days ‚â• 15¬∞C) Query: \u0026ldquo;Show me details for August 2nd\u0026rdquo;\nResponse:\nü§ñ Assistant: Here\u0026#39;s your energy breakdown for that day. {{tool: day-details 2025-08-02}} üõ†Ô∏è Tool Result: üìä 2025-08-02 Summary: Solar: 0.3 kWh Grid In: 6.2 kWh Grid Out: 0.0 kWh Gas: 4.1 m¬≥ Weather: rainy, 16.0¬∞C Security and Privacy Considerations üîí Data Privacy by Deployment Mode GitHub Models (Development/Testing Only):\nTesting Platform: Designed for developers to test and compare different AI models Model Comparison: Easy switching between GPT-4o, GPT-4o-mini, and other available models Not for Production: Should not be used with real customer or sensitive data Energy data sent to Microsoft\u0026rsquo;s GitHub Models testing infrastructure Suitable only for synthetic or anonymised test data Use exclusively for proof-of-concept and feature development Self-Hosted Ollama (Production):\nComplete Data Privacy: All processing happens locally No Data Transmission: Your energy data never leaves your machine No API Keys Required: No registration with external services Full Control: You own and control your data pipeline üí∞ Cost Comparison GitHub Models (Development/Testing):\nFree testing quota for development and model comparison Testing platform only - compare different models like GPT-4o vs GPT-4o-mini No infrastructure setup required for development Must transition to production solution for real deployments Self-Hosted Ollama (Production):\nZero API Costs: No per-token billing or subscription fees One-Time Setup: Download model once, use indefinitely Scalable: Process unlimited queries without additional costs Infrastructure and maintenance costs ‚öñÔ∏è Compliance Framework Development (GitHub Models Testing Platform):\nDevelopment Only: Never use with real production data Use only synthetic test data or completely anonymised datasets Perfect for comparing different models (GPT-4o vs GPT-4o-mini vs others) Suitable exclusively for feature development and proof-of-concept Must migrate to self-hosted solution for any production use Production (Self-Hosted Only):\nGDPR Compliant: No personal data processing by third parties Enterprise Suitable: Meets strict data governance requirements Audit Friendly: Complete transparency in data handling Advanced Extensions Weather Integration Add real-time weather forecasting to predict energy needs:\npublic static async Task\u0026lt;string\u0026gt; GetWeatherForecast() { // Integrate with local weather APIs // Predict solar generation potential // Suggest optimal energy usage timing } Smart Home Integration Connect with Home Assistant or IoT devices:\npublic static string OptimiseDeviceScheduling() { // Schedule high-consumption devices during peak solar // Recommend heating/cooling adjustments // Automate based on energy predictions } Advanced Analytics Implement machine learning for pattern recognition:\npublic static string PredictMonthlyUsage() { // Use historical patterns for forecasting // Identify anomalies in consumption // Suggest efficiency improvements } Troubleshooting GitHub Models Issues Authentication Errors:\n# Verify your token is set echo $GITHUB_TOKEN # Linux/macOS echo %GITHUB_TOKEN% # Windows Rate Limiting:\nTesting quota has monthly limits in testing platform Testing platform not suitable for production workloads Switch to self-hosted for any production deployment Model Availability:\nCheck GitHub Models marketplace for available models to test Testing platform - compare models like GPT-4o, GPT-4o-mini, Llama, and others Ollama Issues Connection Failed:\n# Ensure Ollama is running ollama list ollama run gpt-oss:20b Model Download Issues:\n# Check available space and network ollama pull gpt-oss:20b --verbose Configuration Problems JSON Parsing Errors:\nValidate your appsettings.json file structure Check for missing commas or brackets Use a JSON validator tool Missing Data Responses:\nVerify date formats match exactly (YYYY-MM-DD) Ensure all required fields are present Check for typos in property names Performance Optimization GitHub Models (Development/Testing):\nUse appropriate model size for your development testing Testing platform only - compare different models (GPT-4o vs GPT-4o-mini) Implement response caching for repeated development queries Transition to self-hosted for any production deployment Self-Hosted Ollama:\nMonitor system resources (CPU, Memory, GPU) Adjust model parameters for your hardware Use appropriate quantization levels Best Practices Data Management Regular Backups: Keep copies of your energy data Data Validation: Implement checks for reasonable values Incremental Updates: Add new data without regenerating everything Performance Optimisation Data Caching: Load JSON once and cache in memory Batch Processing: Group multiple queries efficiently Resource Monitoring: Watch CPU and memory usage Model Management Version Control: Track which model versions work best Fine-tuning: Consider domain-specific model training Fallback Options: Have backup models available Advanced GPT-OSS Features Configurable Reasoning Levels GPT-OSS offers three reasoning levels that you can adjust based on your query complexity:\nLow: Fast responses for simple data lookups and general questions Medium: Balanced speed and detail for typical energy analysis queries High: Deep, detailed analysis for complex pattern recognition and optimization You can control this by adding a reasoning level to your system prompts:\nvar systemPrompt = @\u0026#34;You are an energy assistant. Reasoning: high Analyze the user\u0026#39;s energy data and provide detailed insights about consumption patterns.\u0026#34;; Chain-of-Thought Visibility Unlike cloud-based models, GPT-OSS provides complete access to its reasoning process, making it easier to:\nDebug Responses: Understand how the model reached its conclusions Build Trust: See the logical steps in energy recommendations Improve Accuracy: Identify and correct reasoning errors Learn Insights: Understand energy analysis methodologies Conclusion You now have a flexible, production-ready AI energy assistant that adapts to your deployment needs:\nüß™ Development: Use GitHub Models testing platform for rapid prototyping and model comparison (development only) üè≠ Production: Deploy with self-hosted Ollama for privacy and production use üîÑ Migration Path: Easy transition from model testing to production deployment This architecture demonstrates how modern .NET applications can leverage testing platforms for development and model comparison, then transition to self-hosted solutions for production, ensuring you never deploy testing infrastructure to production.\nThe combination of C#, configurable LLM providers, and custom tooling creates a powerful foundation for energy management that scales from proof-of-concept testing to enterprise production deployment.\nWhether you\u0026rsquo;re optimising solar usage, reducing gas consumption, or understanding energy patterns, this assistant provides intelligent insights while giving you complete control over your data and infrastructure.\nNext Steps üîß Configuration Management: Implement environment-specific settings for different deployment stages üìä Enhanced Visualisation: Build a Blazor front-end with charts and dashboards üîó Real-time Integration: Connect with smart meter APIs and IoT devices for live updates ü§ñ Advanced AI: Implement model comparison and A/B testing capabilities üè¢ Enterprise Features: Add multi-tenant support and advanced security üì± Mobile Support: Create companion mobile apps with offline sync Additional Resources GitHub Models Documentation GitHub Models Marketplace GPT-OSS Model Card \u0026amp; Documentation OpenAI GPT-OSS Official Blog GPT-OSS Cookbook \u0026amp; Guides Harmony Response Format Ollama Documentation .NET Configuration Guide .NET JSON Handling Guide Smart Meter Data Formats Home Energy Management Systems ","date":"2025-08-07T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/local-energy-assistant-csharp-gpt-oss/cover_hu_d3b2bc085d311099.jpg","image":"https://quintelier.dev/posts/2025/08/local-energy-assistant-csharp-gpt-oss/cover_hu_39b43ad1b7bd6b7a.jpg","permalink":"https://quintelier.dev/posts/2025/08/local-energy-assistant-csharp-gpt-oss/","title":"‚ö° Local Energy Assistant: C# + GPT-OSS + RAG","webpImage":"https://quintelier.dev/posts/2025/08/local-energy-assistant-csharp-gpt-oss/cover_hu_29900681595f7ea0.webp"},{"content":"Introduction In Part 1, we set up our development environment and deployed our first Azure resource group using Terraform. Whilst this gave us a taste of Infrastructure as Code, we were using local state files: a practice that won\u0026rsquo;t scale for team environments or production workloads.\nIn this second part of our series, we\u0026rsquo;ll explore Terraform state management and remote backends. You\u0026rsquo;ll learn why state is crucial, how to configure Azure Storage as a remote backend, implement state locking for team safety, and master essential state management commands.\nBy the end of this post, you\u0026rsquo;ll have production-ready state management that enables secure team collaboration and robust infrastructure management.\nPrerequisites Before we begin, ensure you have:\nCompleted Part 1 of this series Terraform and Azure CLI installed and configured An active Azure subscription Basic understanding of Azure resource groups and storage accounts Understanding Terraform State What is Terraform State? Terraform state is a JSON file that maps your configuration files to real-world resources. It serves as Terraform\u0026rsquo;s \u0026ldquo;memory\u0026rdquo; of what infrastructure exists and how it relates to your configuration.\nWhen you ran terraform apply in Part 1, Terraform created a file called terraform.tfstate in your working directory. Let\u0026rsquo;s examine what\u0026rsquo;s inside:\n# Navigate to your terraform-azure directory from Part 1 cd terraform-azure # View the state file (if it exists) cat terraform.tfstate The state file contains crucial information:\nResource metadata and attributes Resource dependencies Provider configuration details Resource addresses and unique identifiers Why State Matters Terraform state is essential for several reasons:\nPerformance: State caching avoids costly API calls to cloud providers Collaboration: Teams need shared state to avoid conflicts Metadata Storage: State stores resource metadata not available via APIs Resource Mapping: Links configuration to real infrastructure Dependency Tracking: Maintains resource relationship information Local State Limitations Storing state locally creates several problems:\nNo Team Collaboration: Multiple developers can\u0026rsquo;t share state No Locking: Concurrent operations can corrupt state No Backup: Local files can be lost or corrupted Security Risk: Sensitive data stored in local files No Audit Trail: No history of infrastructure changes Setting Up Azure Storage Remote Backend Step 1: Create Azure Storage Resources Let\u0026rsquo;s create the Azure resources needed for remote state storage.\nImportant Note: Creating the storage account for Terraform state using Terraform itself creates a chicken-and-egg problem - you need a place to store state before you can manage infrastructure with Terraform. Unless you have a fully working landing zone system with existing state management, the storage account should be created using Azure CLI or other tools.\nOption A: Azure CLI (Recommended for State Storage) Create the backend storage using Azure CLI:\n# Set variables for consistency RESOURCE_GROUP_NAME=\u0026#34;terraform-backend-rg\u0026#34; STORAGE_ACCOUNT_NAME=\u0026#34;tfstate$(openssl rand -hex 4)\u0026#34; # Generates unique suffix CONTAINER_NAME=\u0026#34;tfstate\u0026#34; LOCATION=\u0026#34;West Europe\u0026#34; # Create resource group az group create \\ --name $RESOURCE_GROUP_NAME \\ --location \u0026#34;$LOCATION\u0026#34; # Create storage account az storage account create \\ --name $STORAGE_ACCOUNT_NAME \\ --resource-group $RESOURCE_GROUP_NAME \\ --location \u0026#34;$LOCATION\u0026#34; \\ --sku Standard_LRS \\ --kind StorageV2 \\ --allow-blob-public-access false \\ --min-tls-version TLS1_2 # Create container az storage container create \\ --name $CONTAINER_NAME \\ --account-name $STORAGE_ACCOUNT_NAME \\ --auth-mode login # Display information for backend configuration echo \u0026#34;Backend Configuration Details:\u0026#34; echo \u0026#34;Resource Group: $RESOURCE_GROUP_NAME\u0026#34; echo \u0026#34;Storage Account: $STORAGE_ACCOUNT_NAME\u0026#34; echo \u0026#34;Container: $CONTAINER_NAME\u0026#34; Option B: Terraform (For Other Infrastructure) If you already have a backend storage solution or are creating infrastructure other than the state storage itself, you can use Terraform. Create a separate directory for this:\nmkdir terraform-infrastructure cd terraform-infrastructure Create infrastructure.tf for your application resources:\nterraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } # Configure remote backend (created via Azure CLI) backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;terraform-backend-rg\u0026#34; storage_account_name = \u0026#34;tfstate\u0026lt;your-suffix\u0026gt;\u0026#34; container_name = \u0026#34;tfstate\u0026#34; key = \u0026#34;infrastructure/terraform.tfstate\u0026#34; } } provider \u0026#34;azurerm\u0026#34; { features {} # subscription_id is required in provider 4.0+ # Set via ARM_SUBSCRIPTION_ID environment variable } # Example application infrastructure (NOT the backend storage) resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;app\u0026#34; { name = \u0026#34;myapp-rg\u0026#34; location = \u0026#34;West Europe\u0026#34; tags = { Environment = \u0026#34;Development\u0026#34; ManagedBy = \u0026#34;Terraform\u0026#34; } } resource \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;app\u0026#34; { name = \u0026#34;myapp${random_string.app_suffix.result}\u0026#34; resource_group_name = azurerm_resource_group.app.name location = azurerm_resource_group.app.location account_tier = \u0026#34;Standard\u0026#34; account_replication_type = \u0026#34;LRS\u0026#34; tags = { Environment = \u0026#34;Development\u0026#34; Purpose = \u0026#34;Application\u0026#34; } } resource \u0026#34;random_string\u0026#34; \u0026#34;app_suffix\u0026#34; { length = 8 special = false upper = false } Why This Approach Works Backend Storage: Created manually via Azure CLI (no circular dependency) Application Infrastructure: Managed by Terraform using the pre-existing backend Clear Separation: State storage is infrastructure foundation, everything else is managed code Deploy the application infrastructure:\n# Initialize and apply the infrastructure terraform init terraform apply Important: Use the storage account name from the Azure CLI output in your backend configuration.\nEnterprise Landing Zone Approach In enterprise environments with established landing zones, you might have:\nCentralised Backend Management: Shared storage accounts managed by platform teams Terraform Cloud/Enterprise: Managed remote state solutions Azure DevOps/GitHub: Pipeline-managed backend configuration Multi-Subscription Architecture: Separate subscriptions for state management For our learning series, the Azure CLI approach provides the cleanest foundation.\nImportant: Azure RM Provider 4.0 Requirements Azure RM provider 4.0 introduced stricter authentication requirements. You must specify a subscription ID using one of these methods:\nMethod 1: Environment Variable (Recommended) # Set subscription ID via environment variable export ARM_SUBSCRIPTION_ID=\u0026#34;your-subscription-id\u0026#34; # Get your subscription ID if needed az account show --query id --output tsv Method 2: Provider Configuration (Not Recommended for Production) provider \u0026#34;azurerm\u0026#34; { features {} subscription_id = \u0026#34;your-subscription-id\u0026#34; # Avoid hardcoding in production } Method 3: Azure CLI Default (Sometimes Works) # Ensure you\u0026#39;re logged in and have a default subscription az login az account set --subscription \u0026#34;your-subscription-id\u0026#34; Best Practice: Always use environment variables (ARM_SUBSCRIPTION_ID) to avoid hardcoding sensitive values in your Terraform configuration.\nStep 2: Configure Remote Backend Now let\u0026rsquo;s configure our Terraform project to use the remote backend. We\u0026rsquo;ll work with our original terraform-azure directory:\ncd ../terraform-azure Best Practice: Secure Backend Configuration Rather than hardcoding backend configuration in main.tf, we\u0026rsquo;ll use a secure approach that prevents the chicken-and-egg problem and improves security.\nClean Main Configuration Update your main.tf to declare the backend without sensitive details:\nterraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } # Partial backend configuration - details provided during init backend \u0026#34;azurerm\u0026#34; {} } provider \u0026#34;azurerm\u0026#34; { features {} # subscription_id is required in provider 4.0+ # Set via ARM_SUBSCRIPTION_ID environment variable } resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;terraform-rg\u0026#34; location = \u0026#34;West Europe\u0026#34; } Backend Configuration for CI/CD Create backend.hcl for pipeline use (add to .gitignore):\n# backend.hcl - Used in CI/CD pipelines resource_group_name = \u0026#34;terraform-backend-rg\u0026#34; storage_account_name = \u0026#34;tfstate\u0026lt;your-suffix\u0026gt;\u0026#34; container_name = \u0026#34;tfstate\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; Local Development Override Create override.tf for local development (add to .gitignore):\n# override.tf - Local development only terraform { backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;terraform-backend-rg\u0026#34; storage_account_name = \u0026#34;tfstate\u0026lt;your-suffix\u0026gt;\u0026#34; container_name = \u0026#34;tfstate\u0026#34; key = \u0026#34;terraform.tfstate\u0026#34; } } Step 3: Migrate to Remote Backend Option A: Local Development (using override) If you created override.tf, simply initialise:\n# Migrate existing state to remote backend terraform init -migrate-state Option B: CI/CD Pipeline (using backend config file) In CI/CD pipelines, use the backend configuration file:\n# Initialize with backend configuration file terraform init -backend-config=backend.hcl -migrate-state Option C: Pipeline with Environment Variables For even better security, use environment variables in pipelines:\n# Set backend configuration via environment variables export TF_VAR_resource_group_name=\u0026#34;terraform-backend-rg\u0026#34; export TF_VAR_storage_account_name=\u0026#34;tfstate\u0026lt;your-suffix\u0026gt;\u0026#34; export TF_VAR_container_name=\u0026#34;tfstate\u0026#34; export TF_VAR_key=\u0026#34;terraform.tfstate\u0026#34; # Initialize with backend config from environment terraform init \\ -backend-config=\u0026#34;resource_group_name=$TF_VAR_resource_group_name\u0026#34; \\ -backend-config=\u0026#34;storage_account_name=$TF_VAR_storage_account_name\u0026#34; \\ -backend-config=\u0026#34;container_name=$TF_VAR_container_name\u0026#34; \\ -backend-config=\u0026#34;key=$TF_VAR_key\u0026#34; \\ -migrate-state Verify Migration Regardless of the method used, verify the migration:\n# Check that state is now remote terraform state list You should see your existing resources listed, but the state is now stored remotely in Azure Storage.\nState Locking with Azure Blob Storage Azure Storage provides automatic state locking when using the azurerm backend. This prevents multiple users from running Terraform operations simultaneously, which could corrupt the state.\nHow State Locking Works Acquisition: Terraform acquires a lease on the state blob Operation: Terraform performs the requested operation Release: Terraform releases the lease when complete Conflict Prevention: If another operation is in progress, Terraform waits or fails Testing State Locking Let\u0026rsquo;s test state locking behaviour:\n# In one terminal, start a long-running operation terraform plan # In another terminal (different location), try another operation terraform plan The second operation will wait for the first to complete or will fail with a lock acquisition error.\nForce Unlock (Emergency Only) If Terraform crashes and leaves a lock, you can force unlock:\n# Only use in emergencies - can lead to state corruption terraform force-unlock \u0026lt;lock-id\u0026gt; Warning: Only use force-unlock when you\u0026rsquo;re certain no other Terraform operations are running.\nEssential State Management Commands Terraform provides several commands for inspecting and managing state:\nListing Resources # List all resources in state terraform state list # List resources with a filter terraform state list \u0026#34;azurerm_resource_group.*\u0026#34; Inspecting Resources # Show detailed information about a resource terraform state show azurerm_resource_group.example # Show all state information (large output) terraform show Moving Resources # Rename a resource in state (useful for refactoring) terraform state mv azurerm_resource_group.example azurerm_resource_group.main Removing Resources # Remove a resource from state (doesn\u0026#39;t delete the actual resource) terraform state rm azurerm_resource_group.example Importing Existing Resources If you have existing Azure resources not managed by Terraform, you can import them:\n# Import an existing resource group terraform import azurerm_resource_group.existing /subscriptions/{subscription-id}/resourceGroups/{resource-group-name} Backing Up State # Create a backup of current state terraform state pull \u0026gt; terraform.tfstate.backup Practical Example: Multi-Environment State Management Let\u0026rsquo;s create a more realistic example with separate state files for different environments:\nEnvironment-Specific Backend Configuration Create separate backend configurations for each environment:\nenvironments/dev/main.tf:\nterraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;terraform-backend-rg\u0026#34; storage_account_name = \u0026#34;tfstate\u0026lt;your-suffix\u0026gt;\u0026#34; container_name = \u0026#34;tfstate\u0026#34; key = \u0026#34;dev/terraform.tfstate\u0026#34; # Environment-specific key } } provider \u0026#34;azurerm\u0026#34; { features {} # subscription_id is required in provider 4.0+ # Set via ARM_SUBSCRIPTION_ID environment variable } resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;myapp-dev-rg\u0026#34; location = \u0026#34;West Europe\u0026#34; tags = { Environment = \u0026#34;Development\u0026#34; ManagedBy = \u0026#34;Terraform\u0026#34; } } environments/prod/main.tf:\nterraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;terraform-backend-rg\u0026#34; storage_account_name = \u0026#34;tfstate\u0026lt;your-suffix\u0026gt;\u0026#34; container_name = \u0026#34;tfstate\u0026#34; key = \u0026#34;prod/terraform.tfstate\u0026#34; # Environment-specific key } } provider \u0026#34;azurerm\u0026#34; { features {} # subscription_id is required in provider 4.0+ # Set via ARM_SUBSCRIPTION_ID environment variable } resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;main\u0026#34; { name = \u0026#34;myapp-prod-rg\u0026#34; location = \u0026#34;West Europe\u0026#34; tags = { Environment = \u0026#34;Production\u0026#34; ManagedBy = \u0026#34;Terraform\u0026#34; } } Deploying Multiple Environments # Deploy development environment cd environments/dev terraform init terraform apply # Deploy production environment cd ../prod terraform init terraform apply Each environment maintains its own state file in Azure Storage, preventing conflicts between environments.\nTeam Collaboration Best Practices 1. Secure Backend Configuration Management Recommended Approach: Keep sensitive backend configuration out of version control:\n# Project structure terraform-project/ ‚îú‚îÄ‚îÄ main.tf # Clean, no sensitive data ‚îú‚îÄ‚îÄ variables.tf ‚îú‚îÄ‚îÄ outputs.tf ‚îú‚îÄ‚îÄ .gitignore # Include backend configs ‚îú‚îÄ‚îÄ backend.hcl # CI/CD backend config (gitignored) ‚îî‚îÄ‚îÄ override.tf # Local dev config (gitignored) .gitignore entries:\n# Backend configuration files backend.hcl override.tf *.auto.tfvars .terraform/ terraform.tfstate* Team Workflow:\nDevelopers: Use override.tf for local development CI/CD Pipelines: Use backend.hcl or environment variables Shared Code: Keep main.tf clean with partial backend configuration 2. Environment-Specific Backend Strategies Separate Backend Files per Environment # Directory structure environments/ ‚îú‚îÄ‚îÄ dev/ ‚îÇ ‚îú‚îÄ‚îÄ main.tf ‚îÇ ‚îî‚îÄ‚îÄ backend-dev.hcl ‚îú‚îÄ‚îÄ staging/ ‚îÇ ‚îú‚îÄ‚îÄ main.tf ‚îÇ ‚îî‚îÄ‚îÄ backend-staging.hcl ‚îî‚îÄ‚îÄ prod/ ‚îú‚îÄ‚îÄ main.tf ‚îî‚îÄ‚îÄ backend-prod.hcl Dynamic Backend Configuration in Pipelines # GitHub Actions example - name: Configure Backend run: | terraform init \\ -backend-config=\u0026#34;key=${{ env.ENVIRONMENT }}/terraform.tfstate\u0026#34; \\ -backend-config=\u0026#34;storage_account_name=${{ secrets.STORAGE_ACCOUNT }}\u0026#34; 3. Access Control Implement proper access controls for your backend storage:\n# Grant specific users access to the storage account az role assignment create \\ --assignee user@company.com \\ --role \u0026#34;Storage Blob Data Contributor\u0026#34; \\ --scope \u0026#34;/subscriptions/{subscription-id}/resourceGroups/terraform-backend-rg/providers/Microsoft.Storage/storageAccounts/{storage-account-name}\u0026#34; 3. Authentication Methods Configure authentication for team collaboration. Note: Azure RM provider 4.0+ requires subscription_id to be explicitly set.\nOption 1: Azure CLI (Development):\naz login # Provider 4.0+ requires explicit subscription configuration export ARM_SUBSCRIPTION_ID=$(az account show --query id --output tsv) Option 2: Service Principal (CI/CD - Legacy):\n# Set environment variables - subscription_id is now mandatory export ARM_CLIENT_ID=\u0026#34;\u0026lt;service-principal-id\u0026gt;\u0026#34; export ARM_CLIENT_SECRET=\u0026#34;\u0026lt;service-principal-password\u0026gt;\u0026#34; export ARM_SUBSCRIPTION_ID=\u0026#34;\u0026lt;subscription-id\u0026gt;\u0026#34; export ARM_TENANT_ID=\u0026#34;\u0026lt;tenant-id\u0026gt;\u0026#34; Option 3: Federated Credentials (CI/CD - Recommended):\nModern CI/CD systems should use OpenID Connect (OIDC) federated credentials for secure, secret-free authentication:\n# Set environment variables for federated auth export ARM_CLIENT_ID=\u0026#34;\u0026lt;service-principal-id\u0026gt;\u0026#34; export ARM_SUBSCRIPTION_ID=\u0026#34;\u0026lt;subscription-id\u0026gt;\u0026#34; export ARM_TENANT_ID=\u0026#34;\u0026lt;tenant-id\u0026gt;\u0026#34; export ARM_USE_OIDC=true # No ARM_CLIENT_SECRET needed! GitHub Actions Example:\n- name: Azure Login uses: azure/login@v1 with: client-id: ${{ secrets.AZURE_CLIENT_ID }} tenant-id: ${{ secrets.AZURE_TENANT_ID }} subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }} - name: Terraform Apply run: terraform apply -auto-approve env: ARM_CLIENT_ID: ${{ secrets.AZURE_CLIENT_ID }} ARM_SUBSCRIPTION_ID: ${{ secrets.AZURE_SUBSCRIPTION_ID }} ARM_TENANT_ID: ${{ secrets.AZURE_TENANT_ID }} ARM_USE_OIDC: true Azure DevOps Example:\n- task: AzureCLI@2 inputs: azureSubscription: \u0026#39;your-service-connection\u0026#39; scriptType: \u0026#39;bash\u0026#39; scriptLocation: \u0026#39;inlineScript\u0026#39; inlineScript: | export ARM_CLIENT_ID=$(az account show --query user.name --output tsv) export ARM_SUBSCRIPTION_ID=$(az account show --query id --output tsv) export ARM_TENANT_ID=$(az account show --query tenantId --output tsv) export ARM_USE_OIDC=true terraform apply -auto-approve Option 4: Managed Identity (Azure VMs):\nprovider \u0026#34;azurerm\u0026#34; { features {} use_msi = true # subscription_id still required even with managed identity subscription_id = var.subscription_id # Use variable, not hardcoded } Troubleshooting Common Issues Issue 1: State Lock Timeout Problem: Terraform hangs waiting for state lock.\nSolution:\n# Check for stuck locks in Azure portal or CLI az storage blob lease show \\ --account-name \u0026lt;storage-account\u0026gt; \\ --container-name tfstate \\ --name terraform.tfstate # Force unlock if necessary (use with caution) terraform force-unlock \u0026lt;lock-id\u0026gt; Issue 2: Authentication Errors Problem: \u0026ldquo;Failed to get existing workspaces\u0026rdquo; or similar auth errors.\nSolution:\n# Re-authenticate with Azure az login az account show # Verify correct subscription # Or set environment variables for service principal export ARM_CLIENT_ID=\u0026#34;...\u0026#34; export ARM_CLIENT_SECRET=\u0026#34;...\u0026#34; export ARM_SUBSCRIPTION_ID=\u0026#34;...\u0026#34; export ARM_TENANT_ID=\u0026#34;...\u0026#34; For Federated Credentials Issues:\n# Verify OIDC setup export ARM_CLIENT_ID=\u0026#34;...\u0026#34; export ARM_SUBSCRIPTION_ID=\u0026#34;...\u0026#34; export ARM_TENANT_ID=\u0026#34;...\u0026#34; export ARM_USE_OIDC=true # Check if federated credential is properly configured in Azure AD az ad sp federated-credential list --id $ARM_CLIENT_ID Issue 3: State File Corruption Problem: State file becomes corrupted or inconsistent.\nSolution:\n# Restore from backup terraform state pull \u0026gt; current-state-backup.json # Manually restore from previous backup or fix issues terraform state push fixed-state.json # Or refresh state from actual infrastructure terraform refresh Issue 4: Backend Migration Issues Problem: Errors during backend migration.\nSolution:\n# Backup current state first cp terraform.tfstate terraform.tfstate.backup # Re-run migration with debugging TF_LOG=DEBUG terraform init -migrate-state # If migration fails, manually copy state terraform state pull \u0026gt; local-backup.json # Configure new backend terraform init terraform state push local-backup.json Security Considerations 1. State File Security State files contain sensitive information:\nResource configurations Output values (potentially secrets) Provider credentials Resource metadata Best Practices:\nEnable encryption at rest for Azure Storage Use private storage containers Implement proper access controls Audit access to state files Never commit state files to version control 2. Backend Security Configuration # Secure backend configuration example terraform { backend \u0026#34;azurerm\u0026#34; { resource_group_name = \u0026#34;terraform-backend-rg\u0026#34; storage_account_name = \u0026#34;securebackendstore\u0026#34; container_name = \u0026#34;tfstate\u0026#34; key = \u0026#34;prod/terraform.tfstate\u0026#34; # Additional security options use_azuread_auth = true # Use Azure AD authentication use_msi = true # Use managed identity when available } } 3. Secrets Management Never store secrets in Terraform state or configuration:\n# BAD: Don\u0026#39;t put secrets in configuration resource \u0026#34;azurerm_key_vault_secret\u0026#34; \u0026#34;bad_example\u0026#34; { name = \u0026#34;database-password\u0026#34; value = \u0026#34;super-secret-password\u0026#34; # This will be in state! key_vault_id = azurerm_key_vault.main.id } # GOOD: Use sensitive variables or external secret management variable \u0026#34;database_password\u0026#34; { description = \u0026#34;Database password\u0026#34; type = string sensitive = true } resource \u0026#34;azurerm_key_vault_secret\u0026#34; \u0026#34;good_example\u0026#34; { name = \u0026#34;database-password\u0026#34; value = var.database_password key_vault_id = azurerm_key_vault.main.id } Best Practices Summary Do\u0026rsquo;s ‚úÖ Always use remote backends for team environments Enable state locking to prevent conflicts Use environment-specific state files for isolation Implement proper access controls on backend storage Backup state files regularly Use consistent backend configurations across the team Monitor backend storage for capacity and performance Document backend setup for team onboarding Don\u0026rsquo;ts ‚ùå Don\u0026rsquo;t commit state files to version control Don\u0026rsquo;t share storage account keys directly Don\u0026rsquo;t use the same state file for multiple environments Don\u0026rsquo;t skip state locking in team environments Don\u0026rsquo;t store secrets in Terraform configuration Don\u0026rsquo;t ignore backend authentication security Don\u0026rsquo;t force-unlock unless absolutely necessary Don\u0026rsquo;t manually edit state files Next Steps Congratulations! You now have production-ready state management with Azure remote backends. Your infrastructure state is secure, shareable, and protected against conflicts.\nIn Part 3: Variables, Outputs \u0026amp; Data Sources, we\u0026rsquo;ll build on this foundation to create dynamic and reusable Terraform configurations. You\u0026rsquo;ll learn how to:\nUse input variables for flexibility Output values for resource sharing Leverage data sources for existing resources Manage environment-specific configurations Implement variable validation and sensitive handling This knowledge will transform your static configurations into powerful, reusable infrastructure code that adapts to different environments and use cases.\nStay tuned for more hands-on Terraform learning! üöÄ\nAdditional Resources Terraform State Documentation Azure Storage Backend Configuration Terraform State Command Reference Azure Storage Security Best Practices ","date":"2025-08-06T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/zero-to-hero-terraform-for-azure-2/cover_hu_bcef953348b020eb.jpg","image":"https://quintelier.dev/posts/2025/08/zero-to-hero-terraform-for-azure-2/cover_hu_87c9a7d004fd3202.jpg","permalink":"https://quintelier.dev/posts/2025/08/zero-to-hero-terraform-for-azure-2/","title":"üñ•Ô∏è Zero to Hero: Terraform for Azure‚Ä¢Part 2","webpImage":"https://quintelier.dev/posts/2025/08/zero-to-hero-terraform-for-azure-2/cover_hu_f35c17ed80c4cc82.webp"},{"content":".NET 9 represents a significant leap forward in performance optimization, particularly for Azure-hosted applications. With cloud costs continuing to rise and performance expectations at an all-time high, these improvements can directly impact both your application\u0026rsquo;s user experience and your Azure bill. Let\u0026rsquo;s explore the technical depths of what makes .NET 9 a game-changer for Azure workloads.\nExecutive Summary: The Performance Revolution Before diving into the technical details, here\u0026rsquo;s what .NET 9 delivers for Azure applications:\n60% faster compression with the new zlib-ng implementation 2-4x faster exception handling across all platforms 10% reduction in binary size for Native AOT applications Significant startup time improvements for containerised workloads Enhanced JIT optimisations that compound over time with dynamic PGO These improvements translate to real-world benefits: faster cold starts in Azure Container Apps, reduced compute costs, improved user experience, and better resource utilisation across your Azure infrastructure.\nJIT Compiler Optimisations: The Foundation Dynamic Profile-Guided Optimisation (PGO) Enhancements .NET 9 expands PGO capabilities significantly, which is particularly beneficial for long-running Azure services. The JIT compiler now profiles type checks and casts, creating optimised fast paths for the most common scenarios your application encounters.\n// Before: Generic type checking with runtime overhead public bool ProcessItem\u0026lt;T\u0026gt;(T item) where T : class { if (item is IProcessable processable) { return processable.Process(); } return false; } // .NET 9 JIT with PGO now generates optimised assembly // that assumes the most common type first, with fallback paths In Azure Container Apps, where instances can run for hours or days, this cumulative optimisation can result in substantial performance gains as the JIT learns your application\u0026rsquo;s behaviour patterns.\nLoop Optimisations: Micro-Improvements with Macro Impact The new loop optimisations in .NET 9 are particularly relevant for data processing workloads common in Azure:\nInduction Variable Widening eliminates unnecessary zero-extension operations:\n// Common pattern in Azure data processing static decimal CalculateTotal(decimal[] values) { decimal sum = 0; for (int i = 0; i \u0026lt; values.Length; i++) { sum += values[i]; // Now optimised to avoid 32-\u0026gt;64 bit extensions } return sum; } Strength Reduction automatically converts expensive array indexing to pointer arithmetic:\n// Azure batch processing scenario static void ProcessBatch(Span\u0026lt;OrderData\u0026gt; orders) { foreach (ref var order in orders) // JIT now optimises this internally { order.CalculateTotal(); order.ApplyDiscounts(); } } These optimisations compound in Azure scenarios where you\u0026rsquo;re processing large datasets, handling HTTP requests, or performing batch operations.\nContainer Performance: Azure Container Apps Optimisations Native AOT Binary Size Reduction .NET 9\u0026rsquo;s Native AOT improvements are particularly valuable for Azure Container Apps, where smaller images mean faster deployment and reduced storage costs:\n\u0026lt;!-- Optimal configuration for Azure Container Apps --\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;PublishAot\u0026gt;true\u0026lt;/PublishAot\u0026gt; \u0026lt;OptimizationPreference\u0026gt;Size\u0026lt;/OptimizationPreference\u0026gt; \u0026lt;StackTraceSupport\u0026gt;false\u0026lt;/StackTraceSupport\u0026gt; \u0026lt;InvariantGlobalization\u0026gt;true\u0026lt;/InvariantGlobalization\u0026gt; \u0026lt;/PropertyGroup\u0026gt; A typical minimal API application now produces:\n.NET 8: 9.4MB binary .NET 9: 8.5MB binary (~10% reduction) This reduction multiplies across your container registry, CI/CD pipelines, and deployment times.\nDynamic Adaptation to Application Sizes (DATAS) DATAS, now enabled by default in .NET 9, intelligently adjusts garbage collection behaviour based on your application\u0026rsquo;s memory usage patterns. In Azure Container Apps with varying load:\n// Azure Function processing variable workloads [Function(\u0026#34;ProcessOrders\u0026#34;)] public async Task\u0026lt;HttpResponseData\u0026gt; ProcessOrders( [HttpTrigger(AuthorizationLevel.Function, \u0026#34;post\u0026#34;)] HttpRequestData req) { // DATAS automatically adjusts GC pressure based on: // - Container memory limits // - Current memory usage // - Application load patterns var orders = await DeserializeOrders(req); await ProcessOrderBatch(orders); // Memory usage adapts dynamically return CreateSuccessResponse(req); } Networking Performance: Critical for Cloud Applications HTTP Client Optimisations Azure applications rely heavily on HTTP communication. .NET 9 delivers several improvements:\n// Optimised Azure service communication public class AzureServiceClient { private static readonly HttpClient _httpClient = new() { // .NET 9 automatically optimises these patterns DefaultRequestHeaders = { { \u0026#34;User-Agent\u0026#34;, \u0026#34;MyApp/1.0\u0026#34; } } }; public async Task\u0026lt;T\u0026gt; GetDataAsync\u0026lt;T\u0026gt;(string endpoint) { // Improved connection pooling and header handling using var response = await _httpClient.GetAsync(endpoint); // Enhanced JSON deserialisation performance return await response.Content.ReadFromJsonAsync\u0026lt;T\u0026gt;(); } } SSL/TLS Performance Improvements For Azure applications with heavy HTTPS traffic, .NET 9\u0026rsquo;s SslStream improvements are significant:\nReduced TLS handshake allocations Optimised packet handling for 16K writes Better memory management for certificate processing // Azure Key Vault or Service Bus connections benefit significantly var connectionString = await keyVaultClient.GetSecretAsync(\u0026#34;connection-string\u0026#34;); await serviceBusClient.SendMessageAsync(message); // Improved TLS performance Azure-Specific Performance Scenarios Azure SQL Database Connectivity .NET 9\u0026rsquo;s improved connection pooling and exception handling directly benefit Azure SQL connections:\npublic class ProductRepository { private readonly string _connectionString; public async Task\u0026lt;Product[]\u0026gt; GetProductsAsync(int categoryId) { await using var connection = new SqlConnection(_connectionString); await connection.OpenAsync(); // Improved exception handling performance (2-4x faster) // Better connection pooling efficiency // Optimised parameter binding return await connection.QueryAsync\u0026lt;Product\u0026gt;( \u0026#34;SELECT * FROM Products WHERE CategoryId = @categoryId\u0026#34;, new { categoryId }); } } Azure Storage Performance Blob and queue operations benefit from improved networking and compression:\npublic class DocumentService { private readonly BlobServiceClient _blobClient; public async Task\u0026lt;Stream\u0026gt; CompressAndUploadAsync(byte[] document) { // 60% faster compression with zlib-ng using var compressed = new MemoryStream(); using var gzip = new GZipStream(compressed, CompressionLevel.Optimal); await gzip.WriteAsync(document); // Optimised networking stack for Azure Storage var blobClient = _blobClient.GetBlobClient($\u0026#34;docs/{Guid.NewGuid()}\u0026#34;); await blobClient.UploadAsync(compressed); return compressed; } } Benchmarking .NET 9 in Azure Scenarios Minimal API Performance Here\u0026rsquo;s a representative benchmark for a typical Azure Container Apps API:\n// .NET 9 Minimal API optimised for Azure var builder = WebApplication.CreateBuilder(args); // Configure for Azure Container Apps builder.Services.ConfigureHttpJsonOptions(options =\u0026gt; { // JSON performance improvements in .NET 9 options.SerializerOptions.PropertyNamingPolicy = JsonNamingPolicy.CamelCase; }); var app = builder.Build(); // Optimised endpoint with improved JIT compilation app.MapPost(\u0026#34;/api/orders\u0026#34;, async (OrderRequest request, OrderService service) =\u0026gt; { // Improved inlining and PGO optimisation var result = await service.ProcessOrderAsync(request); return Results.Ok(result); }); app.Run(); Performance results (measured on Azure Container Apps):\nCold start: 15% faster Request throughput: 20% improvement Memory usage: 12% reduction P99 latency: 25ms ‚Üí 19ms Azure Functions Performance [Function(\u0026#34;ProcessMessage\u0026#34;)] public async Task ProcessMessage( [ServiceBusTrigger(\u0026#34;orders\u0026#34;)] string message, FunctionContext context) { // Improved exception handling and JSON parsing try { var order = JsonSerializer.Deserialize\u0026lt;Order\u0026gt;(message); await ProcessOrder(order); } catch (JsonException ex) { // 2-4x faster exception handling _logger.LogError(ex, \u0026#34;Failed to process message\u0026#34;); throw; } } Memory Management: Cloud Cost Optimisation Garbage Collection Improvements DATAS (Dynamic Adaptation to Application Sizes) is now the default, providing significant benefits for Azure workloads:\n// Azure background service with variable load public class OrderProcessingService : BackgroundService { protected override async Task ExecuteAsync(CancellationToken stoppingToken) { while (!stoppingToken.IsCancellationRequested) { var orders = await GetPendingOrders(); // DATAS automatically adjusts GC pressure: // - Light load: Conservative memory usage // - Heavy load: Aggressive throughput optimisation await ProcessOrders(orders); await Task.Delay(1000, stoppingToken); } } } Object Stack Allocation .NET 9\u0026rsquo;s object stack allocation optimisation reduces heap pressure for short-lived objects:\n// Azure API controller processing [ApiController] public class OrderController : ControllerBase { [HttpPost] public async Task\u0026lt;IActionResult\u0026gt; CreateOrder(CreateOrderRequest request) { // Small objects may be stack-allocated, reducing GC pressure var validationResult = ValidateOrder(request); if (!validationResult.IsValid) { return BadRequest(validationResult.Errors); } var order = await _orderService.CreateAsync(request); return Ok(order); } } Monitoring Performance Improvements Application Insights Integration Monitor your .NET 9 performance improvements with Azure Application Insights:\npublic class PerformanceMiddleware { private readonly RequestDelegate _next; private readonly ILogger\u0026lt;PerformanceMiddleware\u0026gt; _logger; public async Task InvokeAsync(HttpContext context) { using var activity = Activity.StartActivity(\u0026#34;RequestProcessing\u0026#34;); var stopwatch = Stopwatch.StartNew(); try { await _next(context); } finally { stopwatch.Stop(); // Track .NET 9 performance improvements activity?.SetTag(\u0026#34;duration_ms\u0026#34;, stopwatch.ElapsedMilliseconds); activity?.SetTag(\u0026#34;dotnet_version\u0026#34;, \u0026#34;9.0\u0026#34;); _logger.LogInformation(\u0026#34;Request completed in {Duration}ms\u0026#34;, stopwatch.ElapsedMilliseconds); } } } Custom Metrics for Performance Tracking public class PerformanceMetrics { private readonly IMetricsLogger _metrics; public void TrackCompression(TimeSpan duration, long originalSize, long compressedSize) { _metrics.LogValue(\u0026#34;compression.duration_ms\u0026#34;, duration.TotalMilliseconds); _metrics.LogValue(\u0026#34;compression.ratio\u0026#34;, (double)compressedSize / originalSize); _metrics.LogValue(\u0026#34;compression.throughput_mbps\u0026#34;, (originalSize / 1024.0 / 1024.0) / duration.TotalSeconds); } } Migration Strategy for Azure Workloads Phase 1: Assessment and Testing Baseline Current Performance\n# Collect baseline metrics az monitor metrics list --resource-group myapp-rg \\ --resource myapp-containerapp \\ --metric \u0026#34;Requests,ResponseTime,MemoryUsage\u0026#34; Create .NET 9 Test Environment\n# Updated Dockerfile for .NET 9 FROM mcr.microsoft.com/dotnet/aspnet:9.0 AS base WORKDIR /app EXPOSE 80 FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build WORKDIR /src COPY [\u0026#34;MyApp.csproj\u0026#34;, \u0026#34;.\u0026#34;] RUN dotnet restore COPY . . RUN dotnet build -c Release -o /app/build FROM build AS publish RUN dotnet publish -c Release -o /app/publish FROM base AS final WORKDIR /app COPY --from=publish /app/publish . ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;MyApp.dll\u0026#34;] Phase 2: Progressive Rollout # Azure Container Apps revision management apiVersion: v1 kind: ConfigMap metadata: name: deployment-config data: rollout-strategy: | # Start with 10% traffic to .NET 9 # Monitor performance metrics # Gradually increase based on performance gains Phase 3: Performance Validation public class PerformanceValidator { public async Task\u0026lt;ValidationResult\u0026gt; ValidateUpgrade() { var metrics = await GetPerformanceMetrics(); return new ValidationResult { ThroughputImprovement = metrics.CurrentThroughput / metrics.BaselineThroughput, LatencyImprovement = metrics.BaselineLatency / metrics.CurrentLatency, MemoryEfficiency = metrics.BaselineMemory / metrics.CurrentMemory, CostImpact = CalculateCostImpact(metrics) }; } } Troubleshooting Common Issues Performance Regression Debugging If you encounter performance regressions:\n// Enable detailed JIT diagnostics public class Startup { public void ConfigureServices(IServiceCollection services) { if (Environment.GetEnvironmentVariable(\u0026#34;ENABLE_JIT_DIAGNOSTICS\u0026#34;) == \u0026#34;true\u0026#34;) { services.Configure\u0026lt;JitOptions\u0026gt;(options =\u0026gt; { options.EnablePGO = true; options.EnableTieredCompilation = true; }); } } } Memory Usage Analysis public class MemoryMonitor { public void LogMemoryUsage() { var totalMemory = GC.GetTotalMemory(false); var gen0Collections = GC.CollectionCount(0); var gen1Collections = GC.CollectionCount(1); var gen2Collections = GC.CollectionCount(2); Console.WriteLine($\u0026#34;Memory: {totalMemory:N0} bytes\u0026#34;); Console.WriteLine($\u0026#34;GC Collections - Gen0: {gen0Collections}, Gen1: {gen1Collections}, Gen2: {gen2Collections}\u0026#34;); // Check if DATAS is working effectively var gcInfo = GC.GetGCMemoryInfo(); Console.WriteLine($\u0026#34;Total Available Memory: {gcInfo.TotalAvailableMemoryBytes:N0}\u0026#34;); Console.WriteLine($\u0026#34;High Memory Load Threshold: {gcInfo.HighMemoryLoadThresholdBytes:N0}\u0026#34;); } } Best Practices for Azure .NET 9 Applications 1. Container Optimisation # Multi-stage build optimised for .NET 9 FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build WORKDIR /src # Copy and restore dependencies first (better layer caching) COPY *.csproj ./ RUN dotnet restore COPY . . RUN dotnet publish -c Release -o /app \\ --runtime linux-x64 \\ --self-contained false \\ --no-restore \\ /p:PublishTrimmed=true \\ /p:TrimMode=partial 2. Configuration for Azure Container Apps { \u0026#34;configurationOptions\u0026#34;: { \u0026#34;System.GC.DynamicAdaptationMode\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;System.Runtime.TieredCompilation\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;System.Runtime.TieredPGO\u0026#34;: \u0026#34;true\u0026#34; } } 3. Health Checks and Monitoring public class PerformanceHealthCheck : IHealthCheck { public Task\u0026lt;HealthCheckResult\u0026gt; CheckHealthAsync( HealthCheckContext context, CancellationToken cancellationToken = default) { var memoryUsage = GC.GetTotalMemory(false); var isHealthy = memoryUsage \u0026lt; 100_000_000; // 100MB threshold var data = new Dictionary\u0026lt;string, object\u0026gt; { [\u0026#34;memory_usage\u0026#34;] = memoryUsage, [\u0026#34;gc_gen0\u0026#34;] = GC.CollectionCount(0), [\u0026#34;gc_gen1\u0026#34;] = GC.CollectionCount(1), [\u0026#34;gc_gen2\u0026#34;] = GC.CollectionCount(2) }; return Task.FromResult(isHealthy ? HealthCheckResult.Healthy(\u0026#34;Performance within normal parameters\u0026#34;, data) : HealthCheckResult.Unhealthy(\u0026#34;High memory usage detected\u0026#34;, data: data)); } } Conclusion .NET 9\u0026rsquo;s performance improvements offer substantial benefits for Azure workloads, from reduced cloud costs through better resource utilisation to improved user experience through faster response times. The combination of JIT optimisations, garbage collection improvements, and platform-specific enhancements creates a compelling case for upgrading your Azure applications.\nThe key to success lies in methodical migration, comprehensive monitoring, and understanding how these improvements apply to your specific workload patterns. Start with non-critical environments, measure everything, and gradually roll out to production as you validate the performance gains.\nWith .NET 9, you\u0026rsquo;re not just upgrading a runtime‚Äîyou\u0026rsquo;re investing in a more efficient, cost-effective, and performant Azure infrastructure that will serve your applications well into the future.\nReferences Performance Improvements in .NET 9 What\u0026rsquo;s new in .NET 9 .NET 9 Runtime Performance Improvements Azure Container Apps Documentation Dynamic PGO in .NET ","date":"2025-08-05T10:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/08/net-9-performance-azure-deep-dive/cover_hu_c02668dbae7476a.jpg","image":"https://quintelier.dev/posts/2025/08/net-9-performance-azure-deep-dive/cover_hu_9cedcb3bf6360bf0.jpg","permalink":"https://quintelier.dev/posts/2025/08/net-9-performance-azure-deep-dive/","title":"üöÄ .NET 9 Performance in Azure: A Deep Technical Dive","webpImage":"https://quintelier.dev/posts/2025/08/net-9-performance-azure-deep-dive/cover_hu_7a83fb5c16740e0.webp"},{"content":"üõ†Ô∏è Customizing Your GitHub Copilot Experience: Instructions, Prompts, and Chat Modes GitHub Copilot is more than just an autocomplete tool‚Äîit\u0026rsquo;s a powerful AI assistant you can tailor to your team\u0026rsquo;s needs. With the latest features in VS Code, you can now define custom instructions, prompts, and chat modes to make Copilot work exactly the way you want.\nThis post walks you through customizing Copilot for your projects, with hands-on steps, theory, and examples. Let\u0026rsquo;s dive in!\nUpdate (2025‚Äë08‚Äë29) The Copilot Coding Agent now supports repository‚Äëlevel custom instructions via .github/agents.md. See the new section below and the official changelog: https://github.blog/changelog/2025-08-28-copilot-coding-agent-now-supports-agents-md-custom-instructions/\nüìù Step 1: Repository Custom Instructions Repository custom instructions let you guide Copilot with project-specific context and standards. By adding a .github/copilot-instructions.md file, you ensure Copilot\u0026rsquo;s suggestions match your conventions.\nTheory:\nInstructions are automatically included in every Copilot Chat request in your repo. Keep them short and focused: project purpose, folder structure, coding standards, and key tools. Docs: Adding repository custom instructions Example:\n# Project Description This project is an educational website for sharing homework assignments and coding exercises with students. ## Project Structure - `assignments/`: Contains a subfolder for each homework assignment. - `templates/`: Stores reusable templates for new content. - `assets/`: Holds CSS, JavaScript, images, and configuration files. - `index.html`: The main website page, configurable via `config.json`. ## Project Guidelines - Use consistent styling across all pages. - Name files and folders descriptively and keep them organized. ## Educational Standards - Focus on clear learning objectives and appropriate difficulty levels. - Use language that is clear, encouraging, and student-friendly. How to:\nCreate .github/copilot-instructions.md in your repository. Add your project description, structure, guidelines, and standards. Test by asking Copilot Chat: Briefly explain this project to me. Your instructions should be referenced in the response. üìÇ Step 2: File-Specific Instructions File-specific instructions (.instructions.md) let you target rules to certain files or folders using glob patterns. Place them in .github/instructions/.\nTheory:\nUse applyTo in the frontmatter to specify which files the instructions apply to. Focus on how tasks should be done in that part of the codebase. Docs: Custom Instructions Example:\n--- applyTo: \u0026#34;assignments/**/*.md\u0026#34; --- # Assignment Markdown Structure Guidelines ## 1. Template Usage - Use the structure from `templates/assignment-template.md` for all assignments. - Each assignment should be a `README.md` file. - Do not remove or skip any required sections from the template. ## 2. Section Guidance - **Title**: Provide a short, descriptive name for the assignment. - **Objective**: Write 1-2 sentences summarizing what the student will learn or accomplish. - **Tasks**: Use action-oriented task names, clear requirements, and provide example input/output if helpful. How to:\nCreate .github/instructions/assignments.instructions.md. Add your assignment structure and section rules. Open an assignment file and ask Copilot Chat to update it to match the template. Commit your instructions and the updated assignment file. ‚ö° Step 3: Reusable Prompts Prompt files (.prompt.md) are reusable templates for common tasks, accessible via slash commands in Copilot Chat. Place them in .github/prompts/.\nTheory:\nUse prompts for repeatable workflows (e.g., creating new assignments). Reference other files with relative links. Docs: Prompt Files Example:\n--- mode: agent description: Create a new programming homework assignment --- # Create New Programming Assignment ## Step 1: Gather Assignment Information Ask for the assignment topic if not provided. ## Step 2: Create Assignment Structure - Create a new directory in `assignments/` for the assignment. - Add a `README.md` file using the template. - Fill out the assignment details and add starter code if needed. ## Step 3: Update Website Configuration - Update `config.json` to include the new assignment. Set the due date to the current date plus 7 days, unless specified otherwise. How to:\nCreate .github/prompts/new-assignment.prompt.md. Add your prompt steps and description. Use /new-assignment in Copilot Chat to trigger the workflow. Review and commit the generated files. üí¨ Step 4: Custom Chat Modes Chat modes (.chatmode.md) let you define specialized Copilot personalities and workflows. Place them in .github/chatmodes/.\nTheory:\nChat modes can restrict tools, set response formats, and maintain a unique personality. Docs: Custom Chat Modes Example:\n--- description: üí° Assignment brainstorming assistant for Mr. Johnson\u0026#39;s CS class tools: [\u0026#34;codebase\u0026#34;, \u0026#34;search\u0026#34;] --- # üí° Assignment Brainstorming Assistant ## My Response Style - QUICK SCAN: Provide a brief analysis of existing assignments. - IDEA BURST: Suggest 3-5 new assignment ideas quickly. - NEXT QUESTION: Ask what information is needed to help further. ## Rules - Keep responses short. - Always end with a question. - Focus on concepts, not details. - Never write full assignment specifications. - Base ideas on gaps in the current curriculum. How to:\nCreate .github/chatmodes/assignment-brainstorming.chatmode.md. Add your description, tools, response style, and rules. Select your chat mode in Copilot Chat and try brainstorming questions. üß† New (Aug 2025): agents.md for Copilot Coding Agent The Copilot Coding Agent now reads repository‚Äëlevel guidance from .github/agents.md. Use it to set policies, guardrails, and preferences the agent should follow when making changes across your codebase.\nTheory:\nPlace .github/agents.md at the root of your repository. Keep guidance actionable: preferred tools, change policies, testing and security rules. Use clear sections and bullet points; the agent consumes plain markdown. This augments per‚Äëfile instructions and chat modes; use agents.md for repo‚Äëwide defaults. For full reference documentation, schema examples, and community-maintained guidance, see the canonical agents.md site: https://agents.md/. It includes detailed examples of agent policies, capability declarations, and structured guidance you can drop into .github/agents.md.\nExample (.github/agents.md):\n# Copilot Coding Agent ‚Äì Repository Instructions ## Goals - Maintain code quality, tests, and security while implementing changes. ## Tools and Preferences - Prefer: TypeScript, pnpm, ESLint, Prettier. - Avoid: Global installs; use repo scripts and lockfiles. ## Change Management - Make minimal, focused diffs. - Update docs and tests with code changes. - For multi-file refactors, open a PR with a concise summary and checklist. ## Testing \u0026amp; CI - Run unit tests locally before proposing changes. - Honour existing CI scripts: `pnpm test`, `pnpm lint`. ## Security - Do not add credentials or plaintext secrets. - Use environment variables and existing secret stores. ## Communication - Explain the rationale for non-trivial changes in commit/PR descriptions. How to:\nCreate .github/agents.md in your repository root. Add repository‚Äëwide rules (tools, change policies, test/security requirements). Commit and push; then ask the Copilot Coding Agent to perform a task (e.g., ‚Äúupgrade eslint and fix lint issues‚Äù). It will follow agents.md. Iterate as needed; keep guidance short, specific, and enforceable. üéâ Recap By combining repository instructions, file-specific rules, reusable prompts, and custom chat modes, you can make GitHub Copilot a true teammate. It will understand your project, enforce your standards, and help accelerate your workflow.\nNext steps:\nTry these features in your own projects. Explore more on GitHub Docs and VS Code Docs Happy customizing! üöÄ\n","date":"2025-07-31T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/07/customize-github-copilot-experience/cover_hu_65430d1b623efaa5.jpg","image":"https://quintelier.dev/posts/2025/07/customize-github-copilot-experience/cover_hu_e8308fc059ac2c81.jpg","permalink":"https://quintelier.dev/posts/2025/07/customize-github-copilot-experience/","title":"üõ†Ô∏è GitHub Copilot: Custom Instructions, Prompts \u0026 Chats","webpImage":"https://quintelier.dev/posts/2025/07/customize-github-copilot-experience/cover_hu_bfc3d8563969bc84.webp"},{"content":"Introducing Azure Service Groups: Simplified Resource Governance at Scale Azure environments can grow fast‚Äîacross teams, business units, and global regions. With that growth comes the need for efficient governance. While Azure Management Groups and Role-Based Access Control (RBAC) help organize and secure resources, Azure now offers an even more granular and flexible capability: Azure Service Groups.\nPreview! Azure Service Groups is currently in PREVIEW. For more information about participating in the preview, see Azure Service Groups Preview. See the Supplemental Terms of Use for Microsoft Azure Previews for legal terms that apply to Azure features that are in beta, preview, or otherwise not yet released into general availability.\nIn this article, we‚Äôll explore what Service Groups are, how they work, the built-in roles available, and how to create and manage them using the Azure REST API.\nWhat Are Azure Service Groups? Azure Service Groups are a governance feature that allows you to logically group Azure resources‚Äîlike subscriptions, management groups, or even specific service types‚Äîfor targeted policy enforcement, access control, and reporting.\nThink of them as dynamic or static collections of Azure resources that share a common purpose, ownership, or governance requirement.\nKey Benefits Targeted policy application: Apply policies to a subset of subscriptions across tenants or business units. Simplified role assignments: Assign RBAC roles at the Service Group level for unified access control. Cross-tenant support: Group subscriptions across different tenants using Azure Lighthouse. Automation-friendly: Create and manage Service Groups entirely via REST APIs. Service Groups vs. Management Groups While Management Groups are hierarchical and define organizational structure, Service Groups are flat and flexible. They are ideal for scenarios where:\nResources need to be grouped based on environment (e.g., all dev/test subscriptions). Teams need temporary access to a subset of resources. Policies must be applied across a dynamic subset of services. You can even use Service Groups to target a specific service (e.g., only App Services) across multiple subscriptions.\nBuilt-in Roles for Service Groups Azure provides a set of built-in roles for managing Service Groups:\nRole Name Description Service Group Contributor Can create and manage service groups and their members. Service Group Reader Can view service groups and their members. Service Group Member Contributor Can add or remove members from existing service groups. info These roles can be assigned at the subscription or resource group level using standard Azure RBAC practices.\nYou can use these roles to delegate the management of service groups without over-permissioning users.\nCreating a Service Group Using the REST API Service Groups are currently API-first. To create one, use the following REST endpoint:\nPUT [https://management.azure.com/providers/Microsoft.Governance/serviceGroups/{serviceGroupName}?api-version=2023-07-01-preview](https://management.azure.com/providers/Microsoft.Governance/serviceGroups/{serviceGroupName}?api-version=2023-07-01-preview) Here‚Äôs a minimal example payload:\n{ \u0026#34;location\u0026#34;: \u0026#34;global\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;displayName\u0026#34;: \u0026#34;FinanceTeamGroup\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Group for all finance team subscriptions\u0026#34;, \u0026#34;membershipType\u0026#34;: \u0026#34;Static\u0026#34; } } membershipType can be Static or Dynamic. Dynamic groups use rules to automatically include resources. Adding Members to a Service Group To add a subscription or other resource to a Service Group, use:\nPUT https://management.azure.com/providers/Microsoft.Governance/serviceGroups/{serviceGroupName}/members/{memberId}?api-version=2023-07-01-preview Example body:\n{ \u0026#34;properties\u0026#34;: { \u0026#34;resourceId\u0026#34;: \u0026#34;/subscriptions/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; } } Members can be:\nSubscriptions Management groups Specific services (like Microsoft.Web/sites) Managing and Listing Service Groups To list all service groups in your tenant:\nGET https://management.azure.com/providers/Microsoft.Governance/serviceGroups?api-version=2023-07-01-preview You can also:\nRetrieve members of a group Update group metadata Delete a group All of this is available through the Service Groups API.\nPractical Use Cases Environment Segmentation Create separate Service Groups for dev, tst, acc, and prd resources to enforce different governance policies.\nDepartmental Access Control Give the finance team scoped access only to subscriptions grouped in a FinanceSG Service Group.\nPolicy Targeting Apply a cost control policy to all App Services across all subscriptions using a Service Group that includes only those resources.\nFinal Thoughts Azure Service Groups are a powerful addition to the governance toolbox. They don\u0026rsquo;t replace Management Groups or Azure Policy‚Äîbut they complement them by offering more granular control over how resources are grouped and governed.\nWhile currently accessible only via REST APIs, Service Groups already enable custom automation scenarios. Support for infrastructure-as-code tools like Terraform and Bicep is expected as the feature evolves.\n","date":"2025-06-03T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/06/azure-service-groups-overview/cover_hu_7a180a98dfb092bd.jpg","image":"https://quintelier.dev/posts/2025/06/azure-service-groups-overview/cover_hu_31357a27fed81154.jpg","permalink":"https://quintelier.dev/posts/2025/06/azure-service-groups-overview/","title":"Azure Service Groups: Benefits, RBAC, and API Guide","webpImage":"https://quintelier.dev/posts/2025/06/azure-service-groups-overview/cover_hu_890d3964032052e1.webp"},{"content":"üì¢ Important Update ‚Äî August 2025 GitHub Copilot and ChatGPT have officially deprecated GPT-4o.\nThe new recommended model is GPT-5, now available in public preview for GitHub Copilot (paid plans) and default in ChatGPT.\nGitHub changelog ‚Äî GPT-5 public preview GitHub changelog ‚Äî GPT-4o deprecation OpenAI GPT-5 announcement Which AI model should I use with GitHub Copilot? Choosing the right AI model for your GitHub Copilot project can be tricky.\nEach model has strengths, and matching them to your workflow will help you get better results.\nHeads up! These recommendations follow the official GitHub supported models list as of August 8, 2025.\nTL;DR ‚Äî Quick Recommendations Need Recommended Models Default choice GPT-5 (new standard in Copilot \u0026amp; ChatGPT) Balanced cost \u0026amp; performance GPT-4.1, Claude Sonnet 3.7 Fast/lightweight o4-mini, Gemini 2.0 Flash Deep reasoning/debugging GPT-5, Claude Sonnet 4, Claude Opus 4 Multimodal (images + text) GPT-5, GPT-4.1, Gemini 2.0 Flash üèéÔ∏è Prioritizing speed o4-mini ‚Äî The speed specialist ‚ö° ‚úÖ Best for:\nRapid prototyping Small code snippet explanations Utility functions and boilerplate generation üëÄ Consider other models if: You need multi-file reasoning ‚Äî GPT-5 or Claude Sonnet 4 will serve you better.\nGemini 2.0 Flash ‚Äî The visual sprinter üéØ ‚úÖ Best for:\nAnalyzing diagrams or UI layouts quickly Real-time design feedback Short, multimodal tasks üëÄ Consider other models if: You need longer context or deeper reasoning ‚Äî Gemini 2.5 Pro or GPT-5 are better choices.\n‚öñÔ∏è Balanced AI models GPT-4.1 ‚Äî The reliable all-rounder üåç ‚úÖ Best for:\nGeneral-purpose coding and writing Multilingual and multimodal workflows Accurate, predictable completions üëÄ Consider other models if: You want the latest reasoning improvements ‚Äî GPT-5 offers more depth.\nClaude Sonnet 3.7 ‚Äî The structured coder ‚úçÔ∏è ‚úÖ Best for:\nWell-formatted, consistent output Documentation and code comments Large but not overly complex projects üëÄ Consider other models if: You need high-pressure reasoning ‚Äî Claude Sonnet 4 or GPT-5 are stronger.\nüß† Models for complex projects GPT-5 ‚Äî The next-gen problem solver üèÜ ‚úÖ Best for:\nMulti-file reasoning and debugging Large refactors and architecture planning Reducing hallucinations in complex tasks üëÄ Consider other models if: Budget is tight ‚Äî Claude Sonnet 4 or o3 may be cheaper alternatives.\nClaude Sonnet 4 ‚Äî The balanced strategist üè† ‚úÖ Best for:\nComplex workflows with high reliability Balancing speed and reasoning depth Coding under pressure üëÄ Consider other models if: You need extreme\nüñºÔ∏è Working with visuals (text + images) Some Copilot models can work with both code and visuals ‚Äî for example, diagrams, screenshots, or UI mockups.\nThis can be useful for debugging layouts, interpreting architecture diagrams, or generating code based on design references.\nGPT-5 ‚Äî The unified visual + reasoning model üé® ‚úÖ Best for:\nCombining diagram analysis with code context Visual-assisted debugging and planning üëÄ Consider other models if: You only need pure text/code completion ‚Äî GPT-4.1 or o4-mini may be faster.\nGPT-4.1 ‚Äî The dependable visualist üëÅÔ∏è ‚úÖ Best for:\nCode + diagram combined workflows Multilingual documentation with visual references üëÄ Consider other models if: You need deeper reasoning ‚Äî GPT-5 offers more advanced problem solving.\nClaude Opus 4 ‚Äî The visual architect üèõÔ∏è ‚úÖ Best for:\nDebugging complex UI layouts Reviewing and refining design prototypes üëÄ Consider other models if: Cost is a concern ‚Äî Claude Sonnet 4 can be more budget-friendly.\nClaude Sonnet 4 ‚Äî The balanced visual coder üìê ‚úÖ Best for:\nDay-to-day coding with occasional visual analysis Merging visual and textual reasoning without big performance hits üëÄ Consider other models if: Your visuals require deep, long-context reasoning ‚Äî Claude Opus 4 or GPT-5 may be better.\nGemini 2.0 Flash ‚Äî The instant visual helper ‚ö° ‚úÖ Best for:\nReal-time UI feedback Diagram-based code generation Rapid layout debugging üëÄ Consider other models if: You need detailed, research-level analysis ‚Äî Gemini 2.5 Pro or GPT-5 are stronger.\nGemini 2.5 Pro ‚Äî The deep visual analyst üî¨ ‚úÖ Best for:\nDetailed multimodal reasoning Complex, long-context visual projects üëÄ Consider other models if: You want instant responses ‚Äî Gemini 2.0 Flash is faster.\n","date":"2025-04-28T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/04/which-ai-model-to-use-with-github-copilot/cover_hu_8e8414d37d4c3010.jpg","image":"https://quintelier.dev/posts/2025/04/which-ai-model-to-use-with-github-copilot/cover_hu_9a59e848fc81de19.jpg","permalink":"https://quintelier.dev/posts/2025/04/which-ai-model-to-use-with-github-copilot/","title":"Which AI Model Should You Use with GitHub Copilot?","webpImage":"https://quintelier.dev/posts/2025/04/which-ai-model-to-use-with-github-copilot/cover_hu_b1764c1ca29d38c.webp"},{"content":"Demystifying Networking in Azure Kubernetes Service (AKS) Networking in Azure Kubernetes Service (AKS) provides the backbone for secure and efficient containerized workloads. Microsoft offers several networking models in AKS, each tailored for specific requirements and use cases. In this post, we explore these networking options: Azure CNI, Azure CNI Overlay, Kubenet, and Azure CNI powered by Cilium, along with recent updates from Azure.\nKubenet Kubenet is a simple and traditional networking approach in AKS, suitable for scenarios that don‚Äôt require the advanced networking features of Azure CNI.\nHighlights:\nPods receive IPs from a logically different IP range separate from the node\u0026rsquo;s IP range. Supports basic network segmentation and routing. Lower complexity compared to Azure CNI, suitable for simpler environments. Configure Kubenet networking ‚Üí\nwarning On 31 March 2028, kubenet networking for Azure Kubernetes Service (AKS) will be retired.\nAfter this date, you will not be able to create new AKS clusters with kubenet networking. Existing clusters will continue to work, but you will not be able to create new node pools with kubenet networking. You can migrate existing clusters to Azure CNI or Azure CNI Overlay.\nFor more information, see Kubenet retirement announcement.\nFor more information on how to migrate your existing clusters to Azure CNI or Azure CNI Overlay, see Migrate from kubenet to Azure CNI.\nFor more information on how to migrate your existing clusters to Azure CNI Overlay, see Migrate from kubenet to Azure CNI Overlay.\nFor more information on how to migrate your existing clusters to Azure CNI powered by Cilium, see Migrate from kubenet to Azure CNI powered by Cilium.\nAzure CNI Azure Container Networking Interface (CNI) provides direct integration with Azure\u0026rsquo;s virtual networking capabilities. Each pod gets its own IP address within your virtual network, enabling seamless connectivity across your Azure resources.\nKey benefits:\nDirect IP addressing from your Azure VNet. Simplified communication with other Azure resources without NAT. Ideal for applications requiring high-speed networking and tight integration. Learn how to configure Azure CNI ‚Üí\nAzure CNI Overlay Azure CNI Overlay enhances the traditional Azure CNI model by allowing pods to utilize IP addresses from a private overlay network. This approach dramatically increases IP address availability, addressing scaling challenges.\nKey features:\nSeparate pod CIDR ranges, conserving IP space in your primary VNet. Ideal for large-scale clusters with extensive pod requirements. Easy integration alongside existing Azure infrastructure. Explore Azure CNI Overlay configuration ‚Üí\nAzure CNI Powered by Cilium Azure CNI powered by Cilium combines the robust networking capabilities of Azure CNI with Cilium\u0026rsquo;s advanced networking and security features. This includes rich observability, enhanced security through eBPF-based filtering, and improved network policy enforcement.\nAdvantages:\nEnhanced security features and detailed observability. Policy enforcement at Layer 3/4 and Layer 7. High-performance networking with eBPF. Learn more about Azure CNI powered by Cilium ‚Üí\nConclusion Choosing the right networking solution for your AKS clusters depends heavily on your application\u0026rsquo;s scalability, security, and integration requirements. Azure\u0026rsquo;s diverse networking models, including Azure CNI, Azure CNI Overlay, Kubenet, and Azure CNI powered by Cilium, offer flexibility to address various scenarios, from simple setups to highly complex, secure environments. Staying updated with Azure‚Äôs continuous improvements ensures that your clusters remain performant, scalable, and secure.\n","date":"2025-04-17T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/04/aks-networking-guide/cover_hu_d33a528efd840650.jpg","image":"https://quintelier.dev/posts/2025/04/aks-networking-guide/cover_hu_c311f6a3c29dda6f.jpg","permalink":"https://quintelier.dev/posts/2025/04/aks-networking-guide/","title":"üåê AKS Networking Demystified: Models \u0026 Best Practices","webpImage":"https://quintelier.dev/posts/2025/04/aks-networking-guide/cover_hu_f652eceb3e43a8a6.webp"},{"content":"GitHub Copilot just levelled up. The long-anticipated Agent Mode is no longer in preview and is officially rolling out to all users of Visual Studio Code, including the stable release. If you\u0026rsquo;ve ever dreamed of having a truly autonomous AI pair programmer that doesn\u0026rsquo;t just suggest lines of code but actively edits files, runs commands, and debugs your code ‚Äî your wish has come true.\nüí° What is Copilot Agent Mode? Agent Mode is a major upgrade to GitHub Copilot. Unlike traditional autocomplete and inline suggestions, Agent Mode enables Copilot to understand your workspace and perform complex, multi-step tasks ‚Äî from generating code to fixing bugs, running tests, editing multiple files, and executing terminal commands.\nThink of it like a smart intern who can not only follow your instructions but also proactively solve problems with minimal handholding.\nüß† What Can It Do? Here are just a few of the tasks Agent Mode can perform:\nüîß Refactor legacy code to modern frameworks üöÄ Bootstrap applications from scratch using your tech stack üß™ Write and execute tests, then fix failing ones üìÑ Generate documentation for existing code üì¶ Integrate new libraries and modify config files üñçÔ∏è Suggest code edits across multiple files It‚Äôs not just executing commands ‚Äî it\u0026rsquo;s thinking, iterating, and reacting like a teammate.\n‚öôÔ∏è How Does It Work? Agent Mode uses a system of tools to reason about your code:\nWorkspace search to understand file structure Code edits proposed in response to your request Terminal commands to build, test, and lint Context awareness, with optional user-specified files (#file) or drag-and-drop UI It breaks down your request, reasons about the next steps, and executes them ‚Äî reviewing and retrying when needed.\nüõ†Ô∏è How To Enable It in VS Code Agent Mode is available in VS Code Insiders and Stable as of April 2025.\nHere‚Äôs how to get started:\n‚úÖ Enable Agent Mode\nOpen the Settings UI and enable:\nGitHub \u0026gt; Copilot: Enable Chat Agent Mode (chat.agent.enabled)\nüí¨ Open the Copilot Chat\nUse the shortcut Ctrl+Alt+I (or Cmd+Ctrl+I on macOS), or click the GitHub Copilot icon.\nüîÑ Switch to Agent Mode\nIn the dropdown menu next to your chat input, select Agent instead of Chat or Inline.\n‚úçÔ∏è Type your task\nTry something like:\n\u0026ldquo;Convert this Express app to use Fastify and update the unit tests.\u0026rdquo;\nü§π‚Äç‚ôÄÔ∏è When to Use Agent Mode (vs. Edit Mode) Use Agent Mode when:\nYour task spans multiple files You need terminal commands executed (like npm run build) You\u0026rsquo;re looking for iterative problem-solving Use Edit Mode when:\nYou want focused changes to a single file or snippet The task is well-scoped and you want to review before applying üöß What‚Äôs Coming Next? The Copilot team is already working on enhancements:\n‚è™ Better undo support for changes üß± Improved terminal visibility and error feedback üìì Notebook integration for Jupyter-style workflows üß† More granular context control to help Copilot focus üìé Final Thoughts GitHub Copilot Agent Mode is a glimpse into the future of software development. It‚Äôs no longer just a tool to speed up typing ‚Äî it‚Äôs a collaborator that understands your code, runs it, tests it, and fixes it. This is the kind of developer experience we‚Äôve been waiting for.\nSo fire up Visual Studio Code, switch on Agent Mode, and start building like it‚Äôs 2030. Your AI teammate is ready to work.\n","date":"2025-04-10T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/04/github-copilot-agent-mode/cover_hu_ed191abc3d9a2fd5.jpg","image":"https://quintelier.dev/posts/2025/04/github-copilot-agent-mode/cover_hu_cbcbf786aedf71c9.jpg","permalink":"https://quintelier.dev/posts/2025/04/github-copilot-agent-mode/","title":"ü§ñ GitHub Copilot Agent: Your AI Pair Programmer","webpImage":"https://quintelier.dev/posts/2025/04/github-copilot-agent-mode/cover_hu_ebbb8cfe586a074a.webp"},{"content":"Why Use Federated Credentials in CI/CD Pipelines? In DevOps, protecting credentials and preventing secret sprawl across CI/CD systems like GitHub Actions and Azure DevOps is crucial. Federated credentials provide a secure way to authenticate pipelines without storing sensitive information, reducing the risk of exposure and simplifying secret management.\nFederated identity leverages external identity providers (such as Azure Active Directory) to authenticate users and applications securely. Rather than managing multiple secrets or tokens across platforms, federated identity centralizes authentication, providing significant security and operational benefits.\nBenefits of Using Federated Credentials in CI/CD Pipelines 1Ô∏è‚É£ Eliminate Secrets in Pipelines Federated credentials eliminate the need to store sensitive secrets directly within pipeline configurations, significantly reducing the attack surface.\n2Ô∏è‚É£ Centralized Identity Management Using a centralized identity provider simplifies user and app authentication across multiple CI/CD platforms, reducing administrative overhead.\n3Ô∏è‚É£ Enhanced Security Federated authentication allows leveraging advanced security measures such as Multi-Factor Authentication (MFA), conditional access, and identity protection offered by Azure AD.\n4Ô∏è‚É£ Simplified Secret Rotation Federated identities use short-lived, auto-rotated tokens, simplifying credential rotation processes and reducing the risk of credential leaks.\n5Ô∏è‚É£ Improved Compliance and Auditability Centralized identity providers offer robust auditing and compliance capabilities, making it easier to track authentication events and meet regulatory requirements.\n6Ô∏è‚É£ Reduced Administrative Effort By avoiding direct secret management, teams save considerable time and reduce potential human errors in handling credentials.\n7Ô∏è‚É£ Better Collaboration Federated identity streamlines collaboration across teams and organizations by enabling secure and seamless access management through unified authentication.\n8Ô∏è‚É£ Future-Proof Authentication Federated identity solutions scale efficiently and adapt easily to evolving security requirements and organizational growth.\nImplementing Federated Identity in GitHub and Azure DevOps GitHub Actions Integration Federated identity can be integrated directly into GitHub Actions workflows through Azure AD\u0026rsquo;s OIDC provider. This allows pipelines to securely authenticate to Azure without maintaining any secrets in GitHub.\nBelow are clear guidelines to integrate federated credentials using either an Azure AD App Registration or an Azure Managed Identity. Each method includes complete scripts and GitHub workflows.\nGitHub - Method 1: Azure AD App Registration GitHub - Create Azure AD Application with Federated Credential This script automates the setup of an Azure AD App Registration with federated identity, enabling GitHub Actions workflows to securely authenticate to Azure without storing any secrets.\nDetailed Steps:\n1Ô∏è‚É£ Define Variables\nThese variables should be customized with your details:\nAZURE_SUBSCRIPTION_ID: Azure subscription ID for resource access. GH_ORG: GitHub organization or username. GH_REPO: GitHub repository name. GH_BRANCH: Repository branch authorized for authentication (typically main). 2Ô∏è‚É£ Create Azure AD Application and Assign RBAC Role\nresult=$(az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; --scopes=\u0026#34;/subscriptions/$AZURE_SUBSCRIPTION_ID\u0026#34;) AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.appId\u0026#39;) AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenant\u0026#39;) Uses Azure CLI to create an Azure AD Service Principal (App Registration). Automatically assigns the Contributor role scoped to your Azure subscription. Captures key identifiers: AAD_CLIENT_ID: Application Client ID for authentication. AAD_TENANT_ID: Azure AD Tenant ID. 3Ô∏è‚É£ Prepare Federated Credential Configuration\ncat \u0026lt;\u0026lt;EOF \u0026gt; params.json { \u0026#34;name\u0026#34;: \u0026#34;${GH_ORG}-${GH_REPO}-federation\u0026#34;, \u0026#34;issuer\u0026#34;: \u0026#34;https://token.actions.githubusercontent.com\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;repo:${GH_ORG}/${GH_REPO}:ref:refs/heads/${GH_BRANCH}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Federation for GitHub Actions (${GH_ORG}/${GH_REPO})\u0026#34;, \u0026#34;audiences\u0026#34;: [\u0026#34;api://AzureADTokenExchange\u0026#34;] } EOF This JSON configuration defines the federated identity:\nname: A descriptive identifier for the federated credential. issuer: The trusted issuer of the OIDC token from GitHub Actions. subject: Restricts authentication specifically to your GitHub repository and branch, ensuring tight security boundaries. description: A helpful descriptor for easy management. audiences: The intended Azure token exchange audience. 4Ô∏è‚É£ Create the Federated Credential\naz ad app federated-credential create --id $AAD_CLIENT_ID --parameters @params.json Links the federated credential configuration with your Azure AD Application. Enables GitHub Actions workflows to securely authenticate to Azure via short-lived OIDC tokens, eliminating the need for storing secrets. 5Ô∏è‚É£ Output GitHub Secrets\nprintf \u0026#34;Add the following secrets to your GitHub repository:\\n\u0026#34; printf \u0026#34;AZURE_CLIENT_ID=%s\\n\u0026#34; \u0026#34;$AAD_CLIENT_ID\u0026#34; printf \u0026#34;AZURE_TENANT_ID=%s\\n\u0026#34; \u0026#34;$AAD_TENANT_ID\u0026#34; printf \u0026#34;AZURE_SUBSCRIPTION_ID=%s\\n\u0026#34; \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; Outputs critical (non-sensitive) identifiers required for configuring GitHub repository secrets. These identifiers are safe to store as secrets or variables, as they contain no direct credential secrets (e.g., passwords or tokens). ‚úÖ Complete script\n# Variables AZURE_SUBSCRIPTION_ID=\u0026#34;\u0026lt;your-subscription-id\u0026gt;\u0026#34; GH_ORG=\u0026#34;\u0026lt;your-github-org\u0026gt;\u0026#34; GH_REPO=\u0026#34;\u0026lt;your-github-repo\u0026gt;\u0026#34; GH_BRANCH=\u0026#34;main\u0026#34; # Create Azure AD App Registration with RBAC result=$(az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; --scopes=\u0026#34;/subscriptions/$AZURE_SUBSCRIPTION_ID\u0026#34;) AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.appId\u0026#39;) AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenant\u0026#39;) # Federated Credential Parameters cat \u0026lt;\u0026lt;EOF \u0026gt; params.json { \u0026#34;name\u0026#34;: \u0026#34;${GH_ORG}-${GH_REPO}-federation\u0026#34;, \u0026#34;issuer\u0026#34;: \u0026#34;https://token.actions.githubusercontent.com\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;repo:${GH_ORG}/${GH_REPO}:ref:refs/heads/${GH_BRANCH}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Federation for GitHub Actions (${GH_ORG}/${GH_REPO})\u0026#34;, \u0026#34;audiences\u0026#34;: [\u0026#34;api://AzureADTokenExchange\u0026#34;] } EOF # Create Federated Credential az ad app federated-credential create --id $AAD_CLIENT_ID --parameters @params.json # Output required values printf \u0026#34;Add the following secrets to your GitHub repository:\\n\u0026#34; printf \u0026#34;AZURE_CLIENT_ID=%s\\n\u0026#34; \u0026#34;$AAD_CLIENT_ID\u0026#34; printf \u0026#34;AZURE_TENANT_ID=%s\\n\u0026#34; \u0026#34;$AAD_TENANT_ID\u0026#34; printf \u0026#34;AZURE_SUBSCRIPTION_ID=%s\\n\u0026#34; \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; GitHub Actions Workflow (App Registration) This GitHub Actions workflow demonstrates how to securely authenticate to Azure using an Azure AD App Registration with a federated credential, leveraging OpenID Connect (OIDC).\nPipeline Overview\nWorkflow Name:\nAzure AD App Federated Credential\nTrigger:\nExecutes on each push to the main branch.\nRunner:\nUses GitHub\u0026rsquo;s hosted Ubuntu runner (ubuntu-latest).\nStep 1: Checkout Code\n- uses: actions/checkout@v4 Retrieves the latest source code from your repository. 2Ô∏è‚É£ Authenticate to Azure with OIDC\n- uses: azure/login@v2 with: client-id: ${{ secrets.AZURE_CLIENT_ID }} tenant-id: ${{ secrets.AZURE_TENANT_ID }} subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }} Authenticates the workflow to Azure using OIDC, obtaining an Azure token without ever storing secrets directly in the pipeline. Uses the Azure AD App Registration\u0026rsquo;s federated credential configured earlier. Essential GitHub secrets (AZURE_CLIENT_ID, AZURE_TENANT_ID, and AZURE_SUBSCRIPTION_ID) were set up previously. Security Note:\nThe id-token: write permission is crucial, as it enables the workflow to request an OIDC token securely. 3Ô∏è‚É£ Display Subscription Info\n- name: Display Subscription Info uses: azure/cli@v1 with: inlineScript: | az account show --query \u0026#34;name\u0026#34; -o tsv Runs an Azure CLI command to confirm successful authentication by displaying the active Azure subscription name. Demonstrates a basic Azure CLI operation within the authenticated context, indicating readiness for further Azure operations. ‚úÖ Complete script\nname: Azure AD App Federated Credential permissions: id-token: write contents: read on: push: branches: [main] jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: azure/login@v2 with: client-id: ${{ secrets.AZURE_CLIENT_ID }} tenant-id: ${{ secrets.AZURE_TENANT_ID }} subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }} - name: Display Subscription Info uses: azure/cli@v1 with: inlineScript: | az account show --query \u0026#34;name\u0026#34; -o tsv GitHub - Method 2: Azure Managed Identity Create Managed Identity with Federated Credential This script automates the setup of an Azure Managed Identity with a federated credential, allowing GitHub Actions workflows to authenticate securely to Azure without storing any secrets.\nDetailed Steps:\nStep 1: Define Variables\nYou must customize these variables:\nAZURE_SUBSCRIPTION_ID: Your Azure subscription identifier. AZURE_RG: Name for the Azure resource group to host the managed identity. AZURE_LOCATION: Azure region to deploy the managed identity (e.g., westeurope). ID_NAME: Name for the Azure Managed Identity resource. GH_ORG: GitHub organization or user name. GH_REPO: GitHub repository name. GH_BRANCH: Repository branch permitted to authenticate (typically main). 2Ô∏è‚É£ Create Resource Group\naz group create --resource-group $AZURE_RG --location $AZURE_LOCATION Creates a resource group in Azure, acting as a container for resources such as the managed identity. 3Ô∏è‚É£ Create Managed Identity\nresult=$(az identity create --name $ID_NAME --resource-group $AZURE_RG) AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.clientId\u0026#39;) AAD_PRINCIPAL_ID=$(echo $result | jq -r \u0026#39;.principalId\u0026#39;) AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenantId\u0026#39;) Creates a User-Assigned Managed Identity, capturing the following identifiers: AAD_CLIENT_ID: Application Client ID used for authentication. AAD_PRINCIPAL_ID: Principal ID used for assigning Azure RBAC roles. AAD_TENANT_ID: Tenant ID for your Azure Active Directory instance. 4Ô∏è‚É£ Assign RBAC Role to Managed Identity\naz role assignment create --role \u0026#34;Contributor\u0026#34; --assignee-object-id $AAD_PRINCIPAL_ID --scope /subscriptions/$AZURE_SUBSCRIPTION_ID Grants the Managed Identity the Contributor role at the subscription level. Enables it to perform actions within your Azure subscription securely. 5Ô∏è‚É£ Create Federated Credential\naz identity federated-credential create \\ --name \u0026#34;${GH_ORG}-${GH_REPO}-federation\u0026#34; \\ --identity-name $ID_NAME \\ --resource-group $AZURE_RG \\ --issuer \u0026#34;https://token.actions.githubusercontent.com\u0026#34; \\ --subject \u0026#34;repo:${GH_ORG}/${GH_REPO}:ref:refs/heads/${GH_BRANCH}\u0026#34; \\ --audiences \u0026#34;api://AzureADTokenExchange\u0026#34; Establishes a federated identity between Azure and GitHub Actions using OIDC: issuer: GitHub Actions OIDC token issuer URL. subject: Restricts authentication to your specific GitHub repository and branch. audiences: Configured audience that Azure expects for authentication via token exchange. This federated credential allows your GitHub Actions workflow to authenticate with Azure automatically without directly storing sensitive credentials.\n6Ô∏è‚É£ Output Required GitHub Secrets\nprintf \u0026#34;Add the following secrets to your GitHub repository:\\n\u0026#34; printf \u0026#34;AZURE_CLIENT_ID=%s\\n\u0026#34; \u0026#34;$AAD_CLIENT_ID\u0026#34; printf \u0026#34;AZURE_TENANT_ID=%s\\n\u0026#34; \u0026#34;$AAD_TENANT_ID\u0026#34; printf \u0026#34;AZURE_SUBSCRIPTION_ID=%s\\n\u0026#34; \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; Displays critical values that must be added as GitHub repository secrets: AZURE_CLIENT_ID AZURE_TENANT_ID AZURE_SUBSCRIPTION_ID These variables do not store sensitive passwords or secrets directly‚Äîonly identifiers necessary for the authentication process.\n‚úÖ Complete script\n# Variables AZURE_SUBSCRIPTION_ID=\u0026#34;\u0026lt;your-subscription-id\u0026gt;\u0026#34; AZURE_RG=\u0026#34;rg-gh-oidc\u0026#34; AZURE_LOCATION=\u0026#34;westeurope\u0026#34; ID_NAME=\u0026#34;id-gh-oidc\u0026#34; GH_ORG=\u0026#34;\u0026lt;your-github-org\u0026gt;\u0026#34; GH_REPO=\u0026#34;\u0026lt;your-github-repo\u0026gt;\u0026#34; GH_BRANCH=\u0026#34;main\u0026#34; # Create Resource Group az group create --resource-group $AZURE_RG --location $AZURE_LOCATION # Create Managed Identity result=$(az identity create --name $ID_NAME --resource-group $AZURE_RG) AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.clientId\u0026#39;) AAD_PRINCIPAL_ID=$(echo $result | jq -r \u0026#39;.principalId\u0026#39;) AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenantId\u0026#39;) # Assign RBAC role to Managed Identity az role assignment create --role \u0026#34;Contributor\u0026#34; --assignee-object-id $AAD_PRINCIPAL_ID --scope /subscriptions/$AZURE_SUBSCRIPTION_ID # Create Federated Credential az identity federated-credential create \\ --name \u0026#34;${GH_ORG}-${GH_REPO}-federation\u0026#34; \\ --identity-name $ID_NAME \\ --resource-group $AZURE_RG \\ --issuer \u0026#34;https://token.actions.githubusercontent.com\u0026#34; \\ --subject \u0026#34;repo:${GH_ORG}/${GH_REPO}:ref:refs/heads/${GH_BRANCH}\u0026#34; \\ --audiences \u0026#34;api://AzureADTokenExchange\u0026#34; # Output required values printf \u0026#34;Add the following secrets to your GitHub repository:\\n\u0026#34; printf \u0026#34;AZURE_CLIENT_ID=%s\\n\u0026#34; \u0026#34;$AAD_CLIENT_ID\u0026#34; printf \u0026#34;AZURE_TENANT_ID=%s\\n\u0026#34; \u0026#34;$AAD_TENANT_ID\u0026#34; printf \u0026#34;AZURE_SUBSCRIPTION_ID=%s\\n\u0026#34; \u0026#34;$AZURE_SUBSCRIPTION_ID\u0026#34; GitHub Actions Workflow (Managed Identity) name: Azure Managed Identity Federated Credential permissions: id-token: write contents: read on: push: branches: [main] jobs: deploy: runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - uses: azure/login@v2 with: client-id: ${{ secrets.AZURE_CLIENT_ID }} tenant-id: ${{ secrets.AZURE_TENANT_ID }} subscription-id: ${{ secrets.AZURE_SUBSCRIPTION_ID }} - name: Display Subscription Info uses: azure/cli@v1 with: inlineScript: | az account show --query \u0026#34;name\u0026#34; -o tsv Azure DevOps Pipelines Integration Federated identity integration with Azure DevOps pipelines allows secure, secretless authentication to Azure resources using Azure AD\u0026rsquo;s built-in OIDC support. This prevents credential leaks and reduces the spread of secrets across your DevOps pipelines.\nBelow are clear guidelines to integrate federated credentials using either an Azure AD App Registration or an Azure Managed Identity. Each method includes complete scripts and Azure DevOps pipelines.\nAzure DevOps - Method 1: Azure AD App Registration Azure DevOps - Create Azure AD Application with Federated Credential This script automates the creation and configuration of an Azure DevOps Service Connection using OpenID Connect (OIDC) federation with an Azure AD App Registration. This enables Azure DevOps Pipelines to authenticate securely to Azure without storing secrets in the pipeline.\nDetailed Steps:\n1. Variables Initialization\nYou must customize these variables according to your environment:\nAZURE_SUBSCRIPTION_ID: Your Azure subscription ID. AZDO_ORGANIZATION_NAME: Azure DevOps organization name (e.g., myorg). AZDO_PROJECT_NAME: Azure DevOps project name. AZDO_SERVICE_ENDPOINT_NAME: Name for the Azure DevOps Service Connection. 2. Azure Subscription Name Retrieval\nresult=$(az account show -s $AZURE_SUBSCRIPTION_ID) AZURE_SUBSCRIPTION_NAME=$(echo $result | jq -r \u0026#39;.name\u0026#39;) Fetches the Azure subscription details. Captures the subscription name to be used in subsequent configurations. 3. Azure DevOps CLI Configuration\nAZDO_BASE_URL=\u0026#34;https://dev.azure.com/$AZDO_ORGANIZATION_NAME\u0026#34; az devops configure --defaults organization=$AZDO_BASE_URL Sets the Azure DevOps CLI default organization context for subsequent commands. 4. Azure DevOps Project Creation\nif ! az devops project list | jq -e --arg PROJECT_NAME \u0026#34;$AZDO_PROJECT_NAME\u0026#34; \u0026#39;.[] | select(.name == $PROJECT_NAME)\u0026#39; \u0026gt; /dev/null; then az devops project create --name $AZDO_PROJECT_NAME --description $AZDO_PROJECT_NAME --visibility private fi Checks if the Azure DevOps project already exists. Creates the project if it doesn\u0026rsquo;t exist (with private visibility). 5. Azure AD Application (Service Principal) Creation\nresult=$(az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; \\ --scopes=\u0026#34;/subscriptions/$AZURE_SUBSCRIPTION_ID\u0026#34; \\ --name app-$AZDO_ORGANIZATION_NAME-azdo-oidc) AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.appId\u0026#39;) AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenant\u0026#39;) Creates an Azure AD Service Principal with the Contributor role scoped to your Azure subscription. Captures necessary identifiers (AAD_CLIENT_ID and AAD_TENANT_ID) for authentication. 6. Azure DevOps Service Endpoint Configuration\nThe script creates a JSON file (params.azdo.json) that defines an Azure RM (Resource Manager) service endpoint with federated authentication parameters:\nauthorization.scheme: Set as WorkloadIdentityFederation to use OIDC. Provides details like subscription ID, subscription name, tenant ID, and service principal client ID. { \u0026#34;data\u0026#34;: { \u0026#34;subscriptionId\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;subscriptionName\u0026#34;: \u0026#34;...\u0026#34; }, \u0026#34;authorization\u0026#34;: { \u0026#34;parameters\u0026#34;: { \u0026#34;serviceprincipalid\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;tenantid\u0026#34;: \u0026#34;...\u0026#34; }, \u0026#34;scheme\u0026#34;: \u0026#34;WorkloadIdentityFederation\u0026#34; }, ... } 7. Azure DevOps Service Endpoint Creation or Retrieval\nif ! az devops service-endpoint list --project $AZDO_PROJECT_NAME | jq -e --arg SE_NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name == $SE_NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az devops service-endpoint create \\ --service-endpoint-configuration params.azdo.json \\ --organization $AZDO_BASE_URL \\ --project $AZDO_PROJECT_NAME \\ --detect true) SERVICE_ENDPOINT_ID=$(echo $result | jq -r \u0026#39;.id\u0026#39;) else SERVICE_ENDPOINT_ID=$(az devops service-endpoint list \\ --project $AZDO_PROJECT_NAME | jq -r \\ --arg NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name==$NAME) | .id\u0026#39;) result=$(az devops service-endpoint show \\ --project $AZDO_PROJECT_NAME \\ --id $SERVICE_ENDPOINT_ID) fi Checks if the Service Endpoint already exists in the project. Creates the endpoint if it doesn‚Äôt exist, or retrieves existing endpoint details. Captures key identifiers like SERVICE_ENDPOINT_ID. 8. Retrieve Issuer and Subject from Service Endpoint\nSERVICE_ENDPOINT_ISSUER=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationIssuer\u0026#39;) SERVICE_ENDPOINT_SUBJECT=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationSubject\u0026#39;) Retrieves the Issuer URL and Subject claim from the Service Endpoint configuration, needed for Azure AD federated credential setup. 9. Federated Credential Creation or Update\nThe script prepares two JSON configuration files for Azure AD federated credential:\nparams.create.json (used for initial creation) params.update.json (used for subsequent updates) Both files specify the following properties:\nname: Credential identifier. issuer: Token issuer URL from Azure DevOps. subject: Subject claim that uniquely identifies the Azure DevOps Service Endpoint. audiences: Specifies \u0026quot;api://AzureADTokenExchange\u0026quot; as the audience for OIDC. { \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;issuer\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;audiences\u0026#34;: [\u0026#34;api://AzureADTokenExchange\u0026#34;] } Then it checks whether the federated credential already exists:\nif ! az ad app federated-credential list --id $AAD_CLIENT_ID | jq -e --arg NAME \u0026#34;$PARAMS_NAME\u0026#34; \u0026#39;.[] | select(.name == $NAME)\u0026#39; \u0026gt; /dev/null; then az ad app federated-credential create --id $AAD_CLIENT_ID --parameters params.create.json else FEDERATED_CREDENTIAL_ID=$(az ad app federated-credential list --id $AAD_CLIENT_ID | jq -r -e --arg NAME \u0026#34;$PARAMS_NAME\u0026#34; \u0026#39;.[] | select(.name == $NAME) | .id\u0026#39;) az ad app federated-credential update --id $AAD_CLIENT_ID --federated-credential-id $FEDERATED_CREDENTIAL_ID --parameters params.update.json fi Creates a new federated credential linking Azure DevOps and Azure AD, or updates the existing one if needed. ‚úÖ Complete script\n# Variables # Azure Subscription Id AZURE_SUBSCRIPTION_ID=\u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; # Azure DevOps Organization Name AZDO_ORGANIZATION_NAME=\u0026#34;\u0026lt;myorg\u0026gt;\u0026#34; # Azure DevOps Project Name AZDO_PROJECT_NAME=\u0026#34;\u0026lt;myproject\u0026gt;\u0026#34; # Azure DevOps Service Account Name AZDO_SERVICE_ENDPOINT_NAME=\u0026#34;\u0026lt;mysvcconnection\u0026gt;\u0026#34; # Azure Subscription Name result=$(az account show -s $AZURE_SUBSCRIPTION_ID) AZURE_SUBSCRIPTION_NAME=$(echo $result | jq -r \u0026#39;.name\u0026#39;) # Azure DevOps base URL AZDO_BASE_URL=\u0026#34;https://dev.azure.com/$AZDO_ORGANIZATION_NAME\u0026#34; # Set Azure DevOps defaults result=$(az devops configure --defaults organization=$AZDO_BASE_URL) # Create DevOps Project if ! az devops project list | jq -e --arg PROJECT_NAME \u0026#34;$AZDO_PROJECT_NAME\u0026#34; \u0026#39;.[] | select(.name == $PROJECT_NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az devops project create --name $AZDO_PROJECT_NAME --description $AZDO_PROJECT_NAME --visibility private) fi # Create App Registration result=$(az ad sp create-for-rbac --role=\u0026#34;Contributor\u0026#34; --scopes=\u0026#34;/subscriptions/$AZURE_SUBSCRIPTION_ID\u0026#34; --name app-$AZDO_ORGANIZATION_NAME-azdo-oidc) # Get AAD Application Id AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.appId\u0026#39;) AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenant\u0026#39;) # Create Service Endpoint Configuration params file params.azdo.json cat \u0026lt;\u0026lt;EOF \u0026gt; params.azdo.json { \u0026#34;data\u0026#34;: { \u0026#34;subscriptionId\u0026#34;: \u0026#34;${AZURE_SUBSCRIPTION_ID}\u0026#34;, \u0026#34;subscriptionName\u0026#34;: \u0026#34;$AZURE_SUBSCRIPTION_NAME\u0026#34; }, \u0026#34;authorization\u0026#34;: { \u0026#34;parameters\u0026#34;: { \u0026#34;serviceprincipalid\u0026#34;: \u0026#34;${AAD_CLIENT_ID}\u0026#34;, \u0026#34;tenantid\u0026#34;: \u0026#34;${AAD_TENANT_ID}\u0026#34; }, \u0026#34;scheme\u0026#34;: \u0026#34;WorkloadIdentityFederation\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;serviceEndpointProjectReferences\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;projectReference\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;${AZDO_PROJECT_NAME}\u0026#34; } } ], \u0026#34;type\u0026#34;: \u0026#34;azurerm\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://management.azure.com/\u0026#34; } EOF # Create Or Get Service Endpoint if ! az devops service-endpoint list --project $AZDO_PROJECT_NAME | jq -e --arg SE_NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name == $SE_NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az devops service-endpoint create --service-endpoint-configuration params.azdo.json --organization $AZDO_BASE_URL --project $AZDO_PROJECT_NAME --detect true) # Service Endpoint Id SERVICE_ENDPOINT_ID=$(echo $result | jq -r \u0026#39;.id\u0026#39;) else # Service Endpoint Id SERVICE_ENDPOINT_ID=$(az devops service-endpoint list --project $AZDO_PROJECT_NAME | jq -r --arg NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name==$NAME) | .id\u0026#39;) result=$(az devops service-endpoint show --project $AZDO_PROJECT_NAME --id $SERVICE_ENDPOINT_ID) fi # Service Endpoint Issuer SERVICE_ENDPOINT_ISSUER=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationIssuer\u0026#39;) # Service Endpoint Subject SERVICE_ENDPOINT_SUBJECT=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationSubject\u0026#39;) # Create Federated Credential Configuration params file params.json PARAMS_NAME=\u0026#34;$AZDO_PROJECT_NAME-federated-identity\u0026#34; PARAMS_ISSUER=\u0026#34;${SERVICE_ENDPOINT_ISSUER}\u0026#34; PARAMS_SUBJECT=\u0026#34;${SERVICE_ENDPOINT_SUBJECT}\u0026#34; PARAMS_DESCRIPTION=\u0026#34;Federation for Service Connection $AZDO_SERVICE_ENDPOINT_NAME in $AZDO_BASE_URL/$AZDO_PROJECT_NAME/_settings/adminservices?resourceId=$SERVICE_ENDPOINT_ID\u0026#34; cat \u0026lt;\u0026lt;EOF \u0026gt; params.create.json { \u0026#34;name\u0026#34;: \u0026#34;${PARAMS_NAME}\u0026#34;, \u0026#34;issuer\u0026#34;: \u0026#34;${SERVICE_ENDPOINT_ISSUER}\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;${PARAMS_SUBJECT}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;${PARAMS_DESCRIPTION}\u0026#34;, \u0026#34;audiences\u0026#34;: [ \u0026#34;api://AzureADTokenExchange\u0026#34; ] } EOF cat \u0026lt;\u0026lt;EOF \u0026gt; params.update.json { \u0026#34;issuer\u0026#34;: \u0026#34;${SERVICE_ENDPOINT_ISSUER}\u0026#34;, \u0026#34;subject\u0026#34;: \u0026#34;${PARAMS_SUBJECT}\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;${PARAMS_DESCRIPTION}\u0026#34;, \u0026#34;audiences\u0026#34;: [ \u0026#34;api://AzureADTokenExchange\u0026#34; ] } EOF # Create Or Update Federated Credential if ! az ad app federated-credential list --id $AAD_CLIENT_ID | jq -e --arg NAME \u0026#34;$PARAMS_NAME\u0026#34; \u0026#39;.[] | select(.name == $NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az ad app federated-credential create --id $AAD_CLIENT_ID --parameters params.create.json) else FEDERATED_CREDENTIAL_ID=$(az ad app federated-credential list --id $AAD_CLIENT_ID | jq -r -e --arg NAME \u0026#34;$PARAMS_NAME\u0026#34; \u0026#39;.[] | select(.name == $NAME) | .id\u0026#39;) result=$(az ad app federated-credential update --id $AAD_CLIENT_ID --federated-credential-id $FEDERATED_CREDENTIAL_ID --parameters params.update.json) fi Azure DevOps - YAML Pipeline (App Registration) This script automates the following tasks:\nCloning an Azure DevOps Git repository. Adding a pipeline YAML definition. Committing and pushing pipeline changes. Creating a pipeline in Azure DevOps that leverages a federated identity service connection. Detailed Steps:\n1. Clone Azure DevOps Repository\ngit clone https://$AZDO_ORGANIZATION_NAME@dev.azure.com/$AZDO_ORGANIZATION_NAME/$AZDO_PROJECT_NAME/_git/$AZDO_PROJECT_NAME cd $AZDO_PROJECT_NAME Clones your Azure DevOps repository locally using HTTPS authentication. Navigates into the cloned repository folder. 2. Set Variables for Pipeline\nPIPELINE_DIR=\u0026#34;pipelines\u0026#34; IDENTITY_TYPE=\u0026#34;sp\u0026#34; PIPELINE_DIR: Directory to store pipeline YAML files. IDENTITY_TYPE: Identifier (e.g., \u0026ldquo;sp\u0026rdquo; for service principal) used to name the pipeline YAML file clearly. 3. Create Pipeline Directory\nif [ ! -d \u0026#34;$PIPELINE_DIR\u0026#34; ]; then mkdir $PIPELINE_DIR fi Checks if the directory for pipelines exists, creating it if necessary. 4. Generate Azure DevOps Pipeline YAML\nCreates a pipeline YAML file at pipelines/sp.yaml:\ntrigger: - main pool: vmImage: ubuntu-latest steps: - task: AzureCLI@2 inputs: azureSubscription: \u0026#39;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#39; scriptType: \u0026#39;pscore\u0026#39; scriptLocation: \u0026#39;inlineScript\u0026#39; inlineScript: | az account show --query id -o tsv Trigger: Runs on every commit to the main branch. Agent pool: Uses a hosted Ubuntu image (ubuntu-latest). Task: Executes an Azure CLI command: Uses the Azure service connection (AZDO_SERVICE_ENDPOINT_NAME) with federated identity. Displays the Azure subscription ID to verify connectivity. 5. Commit and Push Pipeline YAML\ngit config --global user.name $GIT_USER git config --global user.email $GIT_EMAIL if ! git diff --quiet HEAD -- \u0026#34;./$PIPELINE_DIR/$IDENTITY_TYPE.yaml\u0026#34; ; then git add ./$PIPELINE_DIR/$IDENTITY_TYPE.yaml git commit -m \u0026#34;üìä Add or Update pipeline.\u0026#34; git push origin main else echo \u0026#34;Nothing to commit.\u0026#34; fi Sets Git global user configuration (required for commits). Checks if the pipeline YAML file has changed: If changed, commits and pushes updates. If unchanged, skips the commit. 6. Configure Azure DevOps CLI and Create Pipeline\naz devops configure --defaults organization=$AZDO_BASE_URL az pipelines create \\ --name \u0026#34;$IDENTITY_TYPE-pipeline\u0026#34; \\ --description \u0026#34;This is a sample pipeline to use federated identity\u0026#34; \\ --repository $AZDO_PROJECT_NAME \\ --repository-type tfsgit \\ --branch main \\ --yaml-path $PIPELINE_DIR/$IDENTITY_TYPE.yaml \\ --project $AZDO_PROJECT_NAME Configures Azure DevOps CLI defaults for subsequent commands. Creates the Azure DevOps pipeline with the specified configuration: name: Pipeline name (sp-pipeline). description: Human-readable pipeline description. repository: The Azure DevOps Git repository hosting the pipeline YAML. branch: Branch to monitor (main). yaml-path: Path to the pipeline YAML file. project: Azure DevOps project name. ‚úÖ Complete scripts\ngit clone https://$AZDO_ORGANIZATION_NAME@dev.azure.com/$AZDO_ORGANIZATION_NAME/$AZDO_PROJECT_NAME/_git/$AZDO_PROJECT_NAME cd $AZDO_PROJECT_NAME PIPELINE_DIR=\u0026#34;pipelines\u0026#34; IDENTITY_TYPE=\u0026#34;sp\u0026#34; # Create directory if not exists if [ ! -d \u0026#34;$PIPELINE_DIR\u0026#34; ]; then mkdir $PIPELINE_DIR fi # Store pipeline cat \u0026lt;\u0026lt;EOF \u0026gt; $PIPELINE_DIR/$IDENTITY_TYPE.yaml trigger: - main pool: vmImage: ubuntu-latest steps: - task: AzureCLI@2 inputs: azureSubscription: \u0026#39;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#39; scriptType: \u0026#39;pscore\u0026#39; scriptLocation: \u0026#39;inlineScript\u0026#39; inlineScript: | az account show --query id -o tsv EOF # Commit pipeline git config --global user.name $GIT_USER git config --global user.email $GIT_EMAIL if ! git diff --quiet HEAD -- \u0026#34;./$PIPELINE_DIR/$IDENTITY_TYPE.yaml\u0026#34; ; then git add ./$PIPELINE_DIR/$IDENTITY_TYPE.yaml git commit -m \u0026#34;üìä Add or Update pipeline.\u0026#34; git push origin main else echo \u0026#34;Nothing to commit.\u0026#34; fi az devops configure --defaults organization=$AZDO_BASE_URL az pipelines create \\ --name \u0026#34;$IDENTITY_TYPE-pipeline\u0026#34; \\ --description \u0026#34;This is a sample pipeline to use federated identity\u0026#34; \\ --repository $AZDO_PROJECT_NAME \\ --repository-type tfsgit \\ --branch main \\ --yaml-path $PIPELINE_DIR/$IDENTITY_TYPE.yaml \\ --project $AZDO_PROJECT_NAME Note: Ensure the pipeline has the setting \u0026ldquo;Allow scripts to access the OAuth token\u0026rdquo; enabled (System.AccessToken) under pipeline options.\nAzure DevOps - Method 2: Azure Managed Identity Azure DevOps - Create Managed Identity with Federated Credential This script sets up an end-to-end secure integration between Azure DevOps pipelines and Azure using User-Assigned Managed Identity combined with Federated Identity (OIDC). This allows Azure DevOps pipelines to securely authenticate to Azure without storing secrets directly.\nDetailed Steps:\nStep 1: Define Variables\nVariables you customize at the top of the script:\nAzure Subscription Info:\nAZURE_SUBSCRIPTION_ID: ID of your Azure subscription. AZURE_LOCATION: Azure region (e.g., westeurope). Azure DevOps Info:\nAZDO_ORGANIZATION_NAME: Your Azure DevOps organization name. AZDO_PROJECT_NAME: Your Azure DevOps project name. AZDO_SERVICE_ENDPOINT_NAME: Name of the Azure DevOps Service Connection. Git Config (used later for pipeline commits):\nGIT_USER: Your Git username. GIT_EMAIL: Your Git email. 2Ô∏è‚É£ Azure Resource Group and Managed Identity Creation\nThe script creates a resource group and managed identity for authentication:\naz group create --location $AZURE_LOCATION --name $AZURE_RG result=$(az identity create --name $ID_NAME --resource-group $AZURE_RG) MI_ID=$(echo $result | jq -r \u0026#39;.id\u0026#39;) AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.clientId\u0026#39;) AAD_PRINICIPAL_ID=$(echo $result | jq -r \u0026#39;.principalId\u0026#39;) AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenantId\u0026#39;) A Resource Group is created to host the Managed Identity. A User-Assigned Managed Identity is created: AAD_CLIENT_ID: Used to authenticate from Azure DevOps. AAD_PRINICIPAL_ID: Needed for Azure RBAC assignments. AAD_TENANT_ID: Azure AD Tenant identifier. 3Ô∏è‚É£ Assign RBAC Role to Managed Identity\nAssign the Managed Identity the Contributor role on the specified Azure subscription:\naz role assignment create \\ --role \u0026#34;Contributor\u0026#34; \\ --assignee-object-id $AAD_PRINICIPAL_ID \\ --assignee-principal-type ServicePrincipal \\ --scope /subscriptions/$AZURE_SUBSCRIPTION_ID This grants the Managed Identity permissions to perform Azure operations in your subscription.\n4Ô∏è‚É£ Azure DevOps Configuration\nSet Azure DevOps CLI defaults and ensure the project exists:\naz devops configure --defaults organization=$AZDO_BASE_URL if ! az devops project list | jq -e --arg PROJECT_NAME \u0026#34;$AZDO_PROJECT_NAME\u0026#34; \u0026#39;.value[] | select(.name == $PROJECT_NAME)\u0026#39; \u0026gt; /dev/null; then az devops project create \\ --name $AZDO_PROJECT_NAME \\ --description $AZDO_PROJECT_NAME \\ --visibility private fi Configures Azure DevOps CLI context. Creates the Azure DevOps project if it doesn‚Äôt already exist. 5Ô∏è‚É£ Create Azure DevOps Service Endpoint with Federated Identity\nA service endpoint configuration (params.azdo.json) is created using Managed Identity and OIDC federation:\nAuthentication uses the scheme: \u0026quot;WorkloadIdentityFederation\u0026quot;. References the Managed Identity‚Äôs clientId and tenantId. The script checks if the Service Endpoint exists, creates it if not:\nif ! az devops service-endpoint list --project $AZDO_PROJECT_NAME | jq -e --arg SE_NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name == $SE_NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az devops service-endpoint create \\ --service-endpoint-configuration params.azdo.json \\ --organization $AZDO_BASE_URL \\ --project $AZDO_PROJECT_NAME \\ --detect true) SERVICE_ENDPOINT_ID=$(echo $result | jq -r \u0026#39;.id\u0026#39;) else SERVICE_ENDPOINT_ID=$(az devops service-endpoint list \\ --project $AZDO_PROJECT_NAME | jq -r \\ --arg NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name==$NAME) | .id\u0026#39;) result=$(az devops service-endpoint show \\ --project $AZDO_PROJECT_NAME \\ --id $SERVICE_ENDPOINT_ID) fi 6Ô∏è‚É£ Extracting Federated Credential Information\nFrom the Service Endpoint, it retrieves critical parameters needed for OIDC federation:\nSERVICE_ENDPOINT_ISSUER=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationIssuer\u0026#39;) SERVICE_ENDPOINT_SUBJECT=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationSubject\u0026#39;) Issuer: The trusted OIDC token issuer URL from Azure DevOps. Subject: Unique identifier scoped specifically to your Azure DevOps service endpoint. 7Ô∏è‚É£ Create or Update Azure Managed Identity Federated Credential**\nUsing the issuer and subject obtained above, the script creates or updates the federated credential for the Managed Identity:\nif ! az identity federated-credential list --identity-name $ID_NAME --resource-group $AZURE_RG | jq -e --arg NAME \u0026#34;$PARAMS_NAME\u0026#34; \u0026#39;.[] | select(.name == $NAME)\u0026#39; \u0026gt; /dev/null; then az identity federated-credential create \\ --identity-name $ID_NAME \\ --name $PARAMS_NAME \\ --resource-group $AZURE_RG \\ --audiences \u0026#34;api://AzureADTokenExchange\u0026#34; \\ --issuer $SERVICE_ENDPOINT_ISSUER \\ --subject $SERVICE_ENDPOINT_SUBJECT else az identity federated-credential update \\ --identity-name $ID_NAME \\ --name $PARAMS_NAME \\ --resource-group $AZURE_RG \\ --audiences \u0026#34;api://AzureADTokenExchange\u0026#34; \\ --issuer $SERVICE_ENDPOINT_ISSUER \\ --subject $SERVICE_ENDPOINT_SUBJECT fi Links the Managed Identity securely with Azure DevOps using OIDC federation. Ensures Azure DevOps Pipelines can authenticate securely without stored credentials. ‚úÖ Complete script\n# Variables GIT_USER=\u0026#34;Sujith Quintelier\u0026#34; GIT_EMAIL=\u0026#34;squintelier@company.com\u0026#34; # Azure Subscription Id AZURE_SUBSCRIPTION_ID=\u0026#34;xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\u0026#34; # Azure location AZURE_LOCATION=\u0026#34;westeurope\u0026#34; # Azure DevOps Project Name AZDO_PROJECT_NAME=\u0026#34;\u0026lt;myproject\u0026gt;\u0026#34; # Azure DevOps Organization Name AZDO_ORGANIZATION_NAME=\u0026#34;\u0026lt;myorg\u0026gt;\u0026#34; # Azure DevOps Service Account Name AZDO_SERVICE_ENDPOINT_NAME=\u0026#34;\u0026lt;mysvcconnection\u0026gt;\u0026#34; # Azure Resource Group AZURE_RG=\u0026#34;rg-$AZDO_PROJECT_NAME\u0026#34; # Managed Identity Name ID_NAME=\u0026#34;id-$AZDO_PROJECT_NAME\u0026#34; # squintelier|xpirit AZDO_BASE_URL=\u0026#34;https://dev.azure.com/$AZDO_ORGANIZATION_NAME\u0026#34; # Azure DevOps base URL # Azure Subscription Name result=$(az account show -s $AZURE_SUBSCRIPTION_ID) AZURE_SUBSCRIPTION_NAME=$(echo $result | jq -r \u0026#39;.name\u0026#39;) # Set Azure DevOps defaults result=$(az devops configure --defaults organization=$AZDO_BASE_URL) # Create DevOps Project if ! az devops project list | jq -e --arg PROJECT_NAME \u0026#34;$AZDO_PROJECT_NAME\u0026#34; \u0026#39;.value[] | select(.name == $PROJECT_NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az devops project create --name $AZDO_PROJECT_NAME --description $AZDO_PROJECT_NAME --visibility private) fi # Create Resource Group result=$(az group create --location $AZURE_LOCATION --name $AZURE_RG) # Create Managed Identity result=$(az identity create --name $ID_NAME --resource-group $AZURE_RG) # Managed Identity Id MI_ID=$(echo $result | jq -r \u0026#39;.id\u0026#39;) # AAD Application Id AAD_CLIENT_ID=$(echo $result | jq -r \u0026#39;.clientId\u0026#39;) # AAD Principal Id AAD_PRINICIPAL_ID=$(echo $result | jq -r \u0026#39;.principalId\u0026#39;) # AAD Tenant Id AAD_TENANT_ID=$(echo $result | jq -r \u0026#39;.tenantId\u0026#39;) # Role Assignment # # With Graph Permissions (uncomment below) # az role assignment create --role \u0026#34;Contributor\u0026#34; --assignee $MI_ID --scope /subscriptions/$AZURE_SUBSCRIPTION_ID # # Without Graph Permissions (uncomment below) az role assignment create --role \u0026#34;Contributor\u0026#34; --assignee-object-id $AAD_PRINICIPAL_ID --assignee-principal-type ServicePrincipal --scope /subscriptions/$AZURE_SUBSCRIPTION_ID # Create Service Endpoint Configuration params file params.azdo.json cat \u0026lt;\u0026lt;EOF \u0026gt; params.azdo.json { \u0026#34;data\u0026#34;: { \u0026#34;subscriptionId\u0026#34;: \u0026#34;${AZURE_SUBSCRIPTION_ID}\u0026#34;, \u0026#34;subscriptionName\u0026#34;: \u0026#34;$AZURE_SUBSCRIPTION_NAME\u0026#34; }, \u0026#34;authorization\u0026#34;: { \u0026#34;parameters\u0026#34;: { \u0026#34;serviceprincipalid\u0026#34;: \u0026#34;${AAD_CLIENT_ID}\u0026#34;, \u0026#34;tenantid\u0026#34;: \u0026#34;${AAD_TENANT_ID}\u0026#34; }, \u0026#34;scheme\u0026#34;: \u0026#34;WorkloadIdentityFederation\u0026#34; }, \u0026#34;description\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;serviceEndpointProjectReferences\u0026#34;: [ { \u0026#34;description\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#34;, \u0026#34;projectReference\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;${AZDO_PROJECT_NAME}\u0026#34; } } ], \u0026#34;type\u0026#34;: \u0026#34;azurerm\u0026#34;, \u0026#34;url\u0026#34;: \u0026#34;https://management.azure.com/\u0026#34; } EOF # Create Or Get Service Endpoint if ! az devops service-endpoint list --project $AZDO_PROJECT_NAME | jq -e --arg SE_NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name == $SE_NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az devops service-endpoint create --service-endpoint-configuration params.azdo.json --organization $AZDO_BASE_URL --project $AZDO_PROJECT_NAME --detect true) # Service Endpoint Id SERVICE_ENDPOINT_ID=$(echo $result | jq -r \u0026#39;.id\u0026#39;) else # Service Endpoint Id SERVICE_ENDPOINT_ID=$(az devops service-endpoint list --project $AZDO_PROJECT_NAME | jq -r --arg NAME \u0026#34;$AZDO_SERVICE_ENDPOINT_NAME\u0026#34; \u0026#39;.[] | select(.name==$NAME) | .id\u0026#39;) result=$(az devops service-endpoint show --project $AZDO_PROJECT_NAME --id $SERVICE_ENDPOINT_ID) fi # Service Endpoint Issuer SERVICE_ENDPOINT_ISSUER=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationIssuer\u0026#39;) # Service Endpoint Subject SERVICE_ENDPOINT_SUBJECT=$(echo $result | jq -r \u0026#39;.authorization.parameters.workloadIdentityFederationSubject\u0026#39;) # Create Federated Credential Configuration params file params.json PARAMS_NAME=\u0026#34;$AZDO_PROJECT_NAME-federated-identity\u0026#34; PARAMS_ISSUER=\u0026#34;${SERVICE_ENDPOINT_ISSUER}\u0026#34; PARAMS_SUBJECT=\u0026#34;${SERVICE_ENDPOINT_SUBJECT}\u0026#34; PARAMS_DESCRIPTION=\u0026#34;Federation for Service Connection $AZDO_SERVICE_ENDPOINT_NAME in $AZDO_BASE_URL/$AZDO_PROJECT_NAME/_settings/adminservices?resourceId=$SERVICE_ENDPOINT_ID\u0026#34; # Create Or Update Federated Credential if ! az identity federated-credential list --identity-name $ID_NAME --resource-group $AZURE_RG | jq -e --arg NAME \u0026#34;$PARAMS_NAME\u0026#34; \u0026#39;.[] | select(.name == $NAME)\u0026#39; \u0026gt; /dev/null; then result=$(az identity federated-credential create --identity-name $ID_NAME --name $PARAMS_NAME --resource-group $AZURE_RG --audiences \u0026#34;api://AzureADTokenExchange\u0026#34; --issuer $SERVICE_ENDPOINT_ISSUER --subject $PARAMS_SUBJECT) else result=$(az identity federated-credential update --identity-name $ID_NAME --name $PARAMS_NAME --resource-group $AZURE_RG --audiences \u0026#34;api://AzureADTokenExchange\u0026#34; --issuer $SERVICE_ENDPOINT_ISSUER --subject $PARAMS_SUBJECT) fi Azure DevOps YAML Pipeline (Managed Identity) git clone https://$AZDO_ORGANIZATION_NAME@dev.azure.com/$AZDO_ORGANIZATION_NAME/$AZDO_PROJECT_NAME/_git/$AZDO_PROJECT_NAME cd $AZDO_PROJECT_NAME PIPELINE_DIR=\u0026#34;pipelines\u0026#34; IDENTITY_TYPE=\u0026#34;mi\u0026#34; # Create directory if not exists if [ ! -d \u0026#34;$PIPELINE_DIR\u0026#34; ]; then mkdir $PIPELINE_DIR fi # Store pipeline cat \u0026lt;\u0026lt;EOF \u0026gt; $PIPELINE_DIR/$IDENTITY_TYPE.yaml trigger: - main pool: vmImage: ubuntu-latest steps: - task: AzureCLI@2 inputs: azureSubscription: \u0026#39;${AZDO_SERVICE_ENDPOINT_NAME}\u0026#39; scriptType: \u0026#39;pscore\u0026#39; scriptLocation: \u0026#39;inlineScript\u0026#39; inlineScript: | az account show --query id -o tsv EOF # Commit pipeline git config --global user.name $GIT_USER git config --global user.email $GIT_EMAIL if ! git diff --quiet HEAD -- \u0026#34;./$PIPELINE_DIR/$IDENTITY_TYPE.yaml\u0026#34; ; then git add ./$PIPELINE_DIR/$IDENTITY_TYPE.yaml git commit -m \u0026#34;üìä Add or Update pipeline.\u0026#34; git push origin main else echo \u0026#34;Nothing to commit.\u0026#34; fi az devops configure --defaults organization=$AZDO_BASE_URL az pipelines create \\ --name \u0026#34;$IDENTITY_TYPE-pipeline\u0026#34; \\ --description \u0026#34;This is a sample pipeline to use federated identity\u0026#34; \\ --repository $AZDO_PROJECT_NAME \\ --repository-type tfsgit \\ --branch main \\ --yaml-path $PIPELINE_DIR/$IDENTITY_TYPE.yaml \\ --project $AZDO_PROJECT_NAME Important:\nEnsure \u0026ldquo;Allow scripts to access the OAuth token\u0026rdquo; is enabled to access the federated token in pipeline tasks. $(System.AccessToken) provides the pipeline identity token securely without storing any secrets. Best Practices Use Azure AD Federated Credentials: Configure pipelines to authenticate via Azure AD federated credentials rather than storing secrets directly. Enforce Least Privilege: Assign minimal necessary permissions to identities used by pipelines. Regular Audits: Frequently audit access logs and credential configurations to ensure ongoing security. Conclusion Implementing federated identity within your CI/CD pipelines enhances security, reduces complexity, and prevents secret sprawl. Leveraging Azure AD integration with GitHub Actions and Azure DevOps pipelines provides a robust, scalable, and secure approach to managing authentication without compromising operational efficiency.\n","date":"2025-03-25T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/03/federated-credentials-secure-azure-cicd/cover_hu_3fa92b662a2e80c6.jpg","image":"https://quintelier.dev/posts/2025/03/federated-credentials-secure-azure-cicd/cover_hu_51eae5bbaeff4f5.jpg","permalink":"https://quintelier.dev/posts/2025/03/federated-credentials-secure-azure-cicd/","title":"üìò Secure Azure CI/CD with Federated Credentials","webpImage":"https://quintelier.dev/posts/2025/03/federated-credentials-secure-azure-cicd/cover_hu_1e48bd40efac291c.webp"},{"content":"Introduction: Azure Changelog ‚Äì Latest Updates \u0026amp; Enhancements Welcome to the Azure Changelog, your go-to source for the latest updates and enhancements in Microsoft Azure. In this edition, we will cover the most recent changes and improvements made to Azure services, tools, and features. Stay informed about the latest developments in the Azure ecosystem to optimize your cloud experience.\nKubenet Networking for Azure Kubernetes Service Retires on March 31, 2028 Azure Kubernetes Service (AKS) will officially retire kubenet networking on March 31, 2028. After this date, workloads using kubenet will no longer be supported, so it\u0026rsquo;s essential to migrate in advance to avoid disruptions.\nMicrosoft recommends transitioning to Azure Container Networking Interface (CNI) overlay, which offers the same IP address overlay architecture as kubenet while providing improved scalability and new capabilities. Follow the upgrade guide to ensure a smooth migration before the deadline.\nAzure Spring Apps Retirement: Service Ends on March 31, 2028 Microsoft has announced the retirement of Azure Spring Apps, including the Basic, Standard, and Enterprise plans, with a final retirement date of March 31, 2028.\nKey Dates March 17, 2025 ‚Äì Azure Spring Apps enters a three-year sunset period. New customers will no longer be able to sign up. March 31, 2028 ‚Äì All Azure Spring Apps plans will be fully retired, and instances will no longer be accessible. Migration Recommendations To ensure continued performance, scalability, and cost efficiency, Microsoft recommends migrating workloads to Azure Container Apps or Azure Kubernetes Service (AKS) before the retirement date.\nRequired Action To prevent service disruptions, plan and execute your migration before March 31, 2028. Microsoft provides migration tools, expert resources, and technical support to assist with the transition.\nFor detailed migration guidance, review the official retirement document.\n","date":"2025-03-18T07:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/03/azure-updates-march-18-2025/cover_hu_c3a111917694f816.jpg","image":"https://quintelier.dev/posts/2025/03/azure-updates-march-18-2025/cover_hu_2428f23124fb7a7d.jpg","permalink":"https://quintelier.dev/posts/2025/03/azure-updates-march-18-2025/","title":"üîÑ Azure updates for March 18, 2025","webpImage":"https://quintelier.dev/posts/2025/03/azure-updates-march-18-2025/cover_hu_e4c4927a348c9ea9.webp"},{"content":"Introduction: GitHub Changelog ‚Äì Latest Updates \u0026amp; Enhancements Stay up to date with the latest GitHub updates, features, and improvements designed to enhance your development experience. Whether it‚Äôs security and compliance advancements, performance optimizations, or new tools for automation and insights, this changelog keeps you informed about what‚Äôs new and how it can benefit your workflow.\nInspired by our previous release, working with Copilot Chat on GitHub has become even more seamless. You can instantly preview HTML files, edit files you‚Äôve created, and work on issues right away. Several exciting new capabilities give you more control and flexibility.\nWhat‚Äôs new Preview your rendered HTML files directly in the side panel Edit files in the side panel to seamlessly refine and adjust them Generate and preview Mermaid diagrams for fast visualizations, whether they‚Äôre flowcharts or sequence diagrams Keep tabs on your issues in the same right side panel, ensuring you can tackle open tasks while discussing them Track issues or pull requests in responses that are rendered in a familiar GitHub style, making working with them easier In addition, you can enjoy a smoother streaming experience and enhanced rendering of attachments. Try it out See the updated experience in action by submitting any of the following example prompts:\nWhat are the last five pull requests I made? Create a commit flow diagram in a markdown file. List the latest issues assigned to me. Create a colored 3D Rubik‚Äôs cube using only HTML and CSS.\n","date":"2025-03-18T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/03/github-updates-march-18-2025/cover_hu_2199f437a84a14b0.jpg","image":"https://quintelier.dev/posts/2025/03/github-updates-march-18-2025/cover_hu_c7fbd08e4622110e.jpg","permalink":"https://quintelier.dev/posts/2025/03/github-updates-march-18-2025/","title":"üîÑ GitHub updates for March 18, 2025","webpImage":"https://quintelier.dev/posts/2025/03/github-updates-march-18-2025/cover_hu_c1bc681f872889b9.webp"},{"content":"Introduction GitHub Copilot is an AI-powered coding assistant developed by GitHub in collaboration with OpenAI. It acts as a pair programmer, providing real-time code suggestions, autocompletions, and entire function implementations based on context. Whether you\u0026rsquo;re writing JavaScript, Python, C#, or even Terraform, Copilot speeds up development and improves productivity.\nIn this article, we‚Äôll explore what GitHub Copilot is, how it works, its benefits, limitations, and best practices for maximizing its potential.\nWhat is GitHub Copilot? GitHub Copilot is an AI-driven coding assistant that helps developers write code faster and more efficiently. It integrates seamlessly into Visual Studio Code, JetBrains, and other popular IDEs. Copilot uses OpenAI\u0026rsquo;s Codex model to analyze comments and existing code, generating intelligent code suggestions in real time.\nKey Features of GitHub Copilot ‚úî Context-Aware Code Suggestions ‚Äì Provides relevant autocompletions based on existing code.\n‚úî Multi-Language Support ‚Äì Works with Python, JavaScript, TypeScript, Go, Ruby, C#, Terraform, and more.\n‚úî Entire Function Implementations ‚Äì Suggests entire function bodies from just a function signature or comment.\n‚úî Comment-Driven Development ‚Äì Generates code based on natural language descriptions.\n‚úî IDE Integration ‚Äì Works inside VS Code, Neovim, JetBrains, and Visual Studio.\n‚úî GitHub Copilot Chat ‚Äì An AI assistant built into the IDE for real-time coding support, explanations, and debugging.\n‚úî Code Explanations ‚Äì Copilot can analyze and explain snippets of code, improving developer understanding.\n‚úî Security Vulnerability Prevention ‚Äì Detects and suggests fixes for insecure coding patterns.\n‚úî Code Completion in Pull Requests ‚Äì Provides AI-powered code suggestions directly within GitHub pull requests.\n‚úî Terminal Commands Assistance ‚Äì Offers intelligent suggestions for shell commands and CLI usage.\n‚úî Copilot for CLI (Coming Soon) ‚Äì Extends Copilot‚Äôs capabilities to the command line for automation and task execution.\nExample: Generating a Python Function With a simple comment, Copilot can generate a fully functional implementation:\n# Function to calculate the factorial of a number def factorial(n): if n == 0: return 1 return n * factorial(n - 1) How Does GitHub Copilot Work? GitHub Copilot leverages OpenAI‚Äôs Codex, a model trained on public code repositories, documentation, and other programming-related text. It processes the context within your editor and predicts the most relevant completion.\nHow Copilot Generates Code: Context Awareness ‚Äì Reads the surrounding code, including variable names, function names, and comments. Natural Language Understanding ‚Äì Uses comments and docstrings to infer developer intent. Code Prediction ‚Äì Suggests lines or entire blocks of code based on recognized patterns. Refinement ‚Äì Offers alternative completions, allowing developers to cycle through options. Where Copilot Works Best: Boilerplate code generation Automating repetitive tasks Generating test cases Writing documentation comments Benefits of Using GitHub Copilot ‚úÖ Increases Productivity Copilot accelerates coding by suggesting functions, reducing time spent on routine tasks.\n‚úÖ Improves Learning and Onboarding Junior developers and new team members can use Copilot to quickly understand syntax and patterns in unfamiliar languages.\n‚úÖ Encourages Best Practices By suggesting structured implementations, Copilot promotes consistency in coding style.\n‚úÖ Enhances Documentation and Comments Developers can write comments, and Copilot will generate corresponding code, reinforcing the importance of well-documented software.\n‚úÖ Improves Security Awareness Copilot now identifies insecure coding patterns and suggests fixes, helping developers write more secure code.\nLimitations and Considerations While GitHub Copilot is powerful, it has some drawbacks:\n‚ö†Ô∏è Not Always Correct Copilot can generate code that has logic errors or security vulnerabilities. Always review its suggestions.\n‚ö†Ô∏è Limited Understanding of Business Logic It doesn\u0026rsquo;t grasp your application‚Äôs specific business rules, so critical thinking is required.\n‚ö†Ô∏è Potential Licensing Issues Copilot is trained on public repositories, and there may be concerns about code originality and compliance with licenses.\n‚ö†Ô∏è Limited CLI and Terminal Support (For Now) While GitHub Copilot is expanding to assist with CLI commands, it is still in early development.\nBest Practices for Using GitHub Copilot üí° Use Copilot for Assistance, Not Replacement Treat Copilot as a helper, but always verify the code it suggests.\nüí° Write Descriptive Comments Clear comments lead to better suggestions. Instead of # get user data, write # Fetch user data from the API and return JSON response.\nüí° Validate Code Quality Use linters, unit tests, and security scans to ensure Copilot-generated code meets standards.\nüí° Customize Copilot Settings In VS Code, you can enable or disable Copilot for certain file types and tweak its behavior.\nFuture of AI-Powered Coding GitHub Copilot is just the beginning. AI-powered tools are rapidly advancing, with features like:\nCopilot Chat ‚Äì Interactive AI assistants inside your IDE. AI-Powered Code Reviews ‚Äì Tools that analyze pull requests for best practices. Copilot CLI (Coming Soon) ‚Äì AI-driven command-line automation. Enhanced Security Features ‚Äì More advanced vulnerability detection and secure coding suggestions. Full AI Pair Programming ‚Äì AI that understands entire projects, not just code snippets. Conclusion GitHub Copilot is a game-changer for developers, enabling faster coding, improved learning, and more efficient workflows. However, it\u0026rsquo;s not a replacement for human expertise‚Äîdevelopers should critically evaluate its suggestions.\nWould you like a guide on integrating Copilot into your VS Code workflow? Let me know in the comments! üöÄ\n","date":"2025-03-15T07:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/03/github-copilot-the-ai-powered-coding-assistant/cover_hu_81171d51f181c9dc.jpg","image":"https://quintelier.dev/posts/2025/03/github-copilot-the-ai-powered-coding-assistant/cover_hu_7940d63fa1dc73a2.jpg","permalink":"https://quintelier.dev/posts/2025/03/github-copilot-the-ai-powered-coding-assistant/","title":"üöÄ GitHub Copilot: The AI-Powered Coding Assistant","webpImage":"https://quintelier.dev/posts/2025/03/github-copilot-the-ai-powered-coding-assistant/cover_hu_db6a1e1cc1a39cfa.webp"},{"content":"Introduction Infrastructure as Code (IaC) is a critical part of modern cloud development, allowing teams to define and manage infrastructure in a declarative and repeatable way. Two of the most popular IaC tools for Microsoft Azure are Azure Bicep and Terraform.\nBut which one should you use? This article compares Bicep and Terraform based on syntax, features, ease of use, ecosystem support, and real-world scenarios.\nWhat is Azure Bicep? Azure Bicep is a domain-specific language (DSL) developed by Microsoft as an abstraction over ARM (Azure Resource Manager) templates. It simplifies Azure infrastructure deployment by providing a cleaner syntax compared to JSON-based ARM templates.\nKey Features of Azure Bicep ‚úî Declarative syntax: Simplifies Azure infrastructure provisioning.\n‚úî Native Azure support: Built and maintained by Microsoft.\n‚úî No state management required: Uses Azure‚Äôs existing infrastructure model.\n‚úî Deep integration with Azure services: Works seamlessly with Microsoft tools like Azure DevOps.\n‚úî Easier debugging compared to ARM templates: More human-readable and less verbose.\nExample Bicep Template Below is a simple example of a Bicep script to deploy an Azure Storage Account:\nparam storageAccountName string = \u0026#39;mystorageaccount\u0026#39; param location string = \u0026#39;eastus\u0026#39; resource storageAccount \u0026#39;Microsoft.Storage/storageAccounts@2021-09-01\u0026#39; = { name: storageAccountName location: location kind: \u0026#39;StorageV2\u0026#39; sku: { name: \u0026#39;Standard_LRS\u0026#39; } } What is Terraform? Terraform is an open-source, multi-cloud IaC tool developed by HashiCorp. Unlike Bicep, which is specific to Azure, Terraform supports multiple cloud providers, including AWS, Google Cloud, and Azure. It uses the HashiCorp Configuration Language (HCL) to define infrastructure.\nKey Features of Terraform ‚úî Multi-cloud support: Works with Azure, AWS, GCP, and on-prem infrastructure.\n‚úî State management: Keeps track of infrastructure state, allowing for drift detection.\n‚úî Modular and reusable: Supports modules for reusable infrastructure components.\n‚úî Extensive ecosystem: Large community with many pre-built modules.\n‚úî Built-in dependency management: Ensures resources are created in the correct order.\nExample Terraform Template Below is a simple Terraform script to deploy an Azure Storage Account:\nprovider \u0026#34;azurerm\u0026#34; { features {} } resource \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;mystorageaccount\u0026#34; resource_group_name = \u0026#34;my-resource-group\u0026#34; location = \u0026#34;East US\u0026#34; account_tier = \u0026#34;Standard\u0026#34; account_replication_type = \u0026#34;LRS\u0026#34; } Terraform‚Äôs Multi-Provider Advantage Terraform is not just multi-cloud but multi-provider, meaning it can be used to provision non-cloud resources such as databases, SaaS applications, Kubernetes clusters, and even on-prem infrastructure.\nExamples of Terraform\u0026rsquo;s Multi-Provider System Cloud Providers: Azure, AWS, GCP SaaS Platforms: GitHub, Azure DevOps, Datadog, Okta Infrastructure: VMware, Cisco, Kubernetes, Helm Databases \u0026amp; Storage: PostgreSQL, MySQL, MongoDB, Azure SQL Example: Provisioning Azure + GitHub Repositories in Terraform Here‚Äôs an example of how Terraform can deploy both an Azure Storage Account and a GitHub repository within the same configuration:\n# Azure Provider provider \u0026#34;azurerm\u0026#34; { features {} } resource \u0026#34;azurerm_storage_account\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;mystorageaccount\u0026#34; resource_group_name = \u0026#34;my-resource-group\u0026#34; location = \u0026#34;East US\u0026#34; account_tier = \u0026#34;Standard\u0026#34; account_replication_type = \u0026#34;LRS\u0026#34; } # GitHub Provider provider \u0026#34;github\u0026#34; { token = var.github_token } resource \u0026#34;github_repository\u0026#34; \u0026#34;example_repo\u0026#34; { name = \u0026#34;my-terraform-repo\u0026#34; description = \u0026#34;This is managed by Terraform\u0026#34; visibility = \u0026#34;public\u0026#34; } With Terraform, this can be deployed in a single pipeline, making it much simpler to manage infrastructure across multiple platforms.\nChallenges of Mixing Bicep and Terraform in the Same Pipeline If you only use Bicep, you might struggle with managing non-Azure resources. In a DevOps pipeline, this creates challenges:\nMultiple IaC tools: You would need to manage Terraform for non-Azure resources and Bicep for Azure resources separately. Complexity in orchestration: A pipeline would need separate execution steps for Bicep and Terraform. State synchronization issues: Terraform tracks the entire infrastructure in a state file, while Bicep does not, making it harder to maintain consistency. Example Pipeline with Mixed Bicep and Terraform:\nStep 1: Deploy Azure resources with Bicep Step 2: Deploy GitHub repositories with Terraform Step 3: Deploy Kubernetes workloads with Helm/Terraform This makes the pipeline more complex compared to using Terraform alone.\nConclusion: Why Terraform is More Flexible While Bicep is great for Azure-native infrastructure, Terraform\u0026rsquo;s multi-provider support makes it a better choice if:\nYour pipeline needs to provision both Azure and non-Azure resources. You want to manage SaaS integrations (e.g., GitHub, Azure DevOps, Okta). You need a consistent state management approach across all platforms. If your entire infrastructure is Azure-only, Bicep is a strong choice. But for hybrid or multi-cloud environments, Terraform provides a more unified IaC experience.\nWould you like a guide on how to integrate Terraform into an Azure DevOps or GitHub Actions pipeline?\n","date":"2025-03-15T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/03/azure-bicep-vs-terraform/cover_hu_3efc84f4843bea6b.jpg","image":"https://quintelier.dev/posts/2025/03/azure-bicep-vs-terraform/cover_hu_24b217c25a24949c.jpg","permalink":"https://quintelier.dev/posts/2025/03/azure-bicep-vs-terraform/","title":"‚öñÔ∏è Azure Bicep vs Terraform: What Should You Choose?","webpImage":"https://quintelier.dev/posts/2025/03/azure-bicep-vs-terraform/cover_hu_c07190ec8a77bb7.webp"},{"content":"Introduction: GitHub Changelog ‚Äì Latest Updates \u0026amp; Enhancements Stay up to date with the latest GitHub updates, features, and improvements designed to enhance your development experience. Whether it‚Äôs security and compliance advancements, performance optimizations, or new tools for automation and insights, this changelog keeps you informed about what‚Äôs new and how it can benefit your workflow.\nIn this edition, we‚Äôre covering GitHub‚Äôs PCI DSS v4.0 compliance for enterprise customers and the general availability of GitHub Actions Performance Metrics, helping teams gain deeper insights into their CI/CD workflows. Let‚Äôs dive into the details! üöÄ\nGitHub is now PCI DSS v4.0 compliant with our 4.0 service provider attestation available to customers GitHub‚Äôs Payment Card Industry Data Security Standard (PCI DSS) v4.0 service provider Attestation of Compliance (AoC) as well as the corresponding shared responsibility matrix has been completed. This report is the first time GitHub has provided a PCI DSS service provider report for our customers. This enables customers to meet their own PCI DSS compliance needs using GitHub as part of their development environment.\nGoing forward, GitHub intends to provide this attestation of compliance each year.\nIf you‚Äôre an Enterprise customer and need to obtain copies of GitHub‚Äôs AoC or Shared Responsibility Matrix, please reach out to your account manager.\nSee the PCI DSS v4.0 Compliance\nActions Performance Metrics are generally available and Enterprise-level metrics are in public preview Performance Metrics for GitHub Actions are now generally available for repositories and organizations. Repository members can view workflow and job performance data including queue times and failure rates going back as far as one year. Organization members can also view this data aggregated across all repositories in their organization. These metrics are available on all GitHub Cloud plans.\nIn addition, usage and performance metrics aggregated at the Enterprise level are now available in public preview to Enterprise admins. This includes usage metrics (ex. jobs run and minutes used), as well as performance metrics (ex. job failure rates and queue times) across all repositories and organizations in an enterprise. These metrics can be found in the Enterprise UI under the \u0026ldquo;Insights\u0026rdquo; tab.\nActions Performance Metrics See the Actions Performance Metrics\n","date":"2025-03-14T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/03/github-updates-march-14-2025/cover_hu_2199f437a84a14b0.jpg","image":"https://quintelier.dev/posts/2025/03/github-updates-march-14-2025/cover_hu_c7fbd08e4622110e.jpg","permalink":"https://quintelier.dev/posts/2025/03/github-updates-march-14-2025/","title":"üîÑ GitHub updates for March 14, 2025","webpImage":"https://quintelier.dev/posts/2025/03/github-updates-march-14-2025/cover_hu_c1bc681f872889b9.webp"},{"content":"Introduction Terraform is a powerful Infrastructure as Code (IaC) tool that enables you to define, provision, and manage Azure resources using declarative configuration files. This blog series, Zero to Hero: Terraform for Azure, will take you through Terraform from the basics to advanced topics, using hands-on examples tailored for Azure.\nIn this first post, we‚Äôll focus on setting up your development environment and deploying your first resource on Azure using Terraform.\nPrerequisites Before we start, you need:\nAn Azure account (a free account works fine) Basic knowledge of Azure (nice to have, but not required) We‚Äôll cover multiple setup options:\nLocal installation (best for long-term development) Using Docker (best for isolated environments) 1. Setting Up the Development Environment Option 1: Local Installation For local development, you need to install:\nTerraform\nDownload Terraform for your OS.\nVerify installation:\nterraform version Azure CLI\nInstall Azure CLI for authentication and resource management:\nWindows:\nwinget install --id Microsoft.AzureCLI -e macOS:\nbrew install azure-cli Linux (Debian-based):\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash Verify installation:\naz version Visual Studio Code\nInstall VS Code. Install the Terraform extension from the Marketplace. Option 2: Using Docker If you prefer an isolated environment, you can use Terraform inside a Docker container.\nInstall Docker.\nRun Terraform inside a container:\ndocker run --rm -it hashicorp/terraform:latest version To work with local files, mount a volume:\ndocker run --rm -it -v $(pwd):/app -w /app hashicorp/terraform:latest init 2. Authenticating with Azure To deploy resources, Terraform needs to authenticate with Azure.\nLogin using Azure CLI Open a terminal and log in:\naz login If using multiple subscriptions, set the desired one:\naz account set --subscription \u0026#34;\u0026lt;subscription-id\u0026gt;\u0026#34; Using a Service Principal (For Automation) For automation in CI/CD pipelines:\naz ad sp create-for-rbac --name terraform-sp --role Contributor --scopes /subscriptions/\u0026lt;subscription-id\u0026gt; --sdk-auth Copy the JSON output for use in Terraform.\n3. Writing Your First Terraform Configuration Let‚Äôs create a simple Terraform script to deploy an Azure Storage Account.\nStep 1: Initialize a Terraform Project Create a new directory:\nmkdir terraform-azure \u0026amp;\u0026amp; cd terraform-azure Create a new Terraform file:\ntouch main.tf Step 2: Define the Terraform Configuration Open main.tf in VS Code and add the following:\nterraform { required_providers { azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 3.0\u0026#34; } } } provider \u0026#34;azurerm\u0026#34; { features {} } resource \u0026#34;azurerm_resource_group\u0026#34; \u0026#34;example\u0026#34; { name = \u0026#34;terraform-rg\u0026#34; location = \u0026#34;West Europe\u0026#34; } Step 3: Initialize Terraform Run:\nterraform init Step 4: Preview Changes terraform plan Step 5: Apply the Configuration terraform apply -auto-approve Once completed, your resource group will be created in Azure.\n4. Cleaning Up and Next Steps To remove the deployed resources:\nterraform destroy -auto-approve Next Steps In Part 2: State Management \u0026amp; Remote Backends, we\u0026rsquo;ll explore Terraform state management and remote backends.\nStay tuned for more hands-on Terraform learning! üöÄ\n","date":"2025-03-10T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/03/zero-to-hero-terraform-for-azure-1/cover_hu_672ffa1b005fe903.jpg","image":"https://quintelier.dev/posts/2025/03/zero-to-hero-terraform-for-azure-1/cover_hu_2167dd05d3189328.jpg","permalink":"https://quintelier.dev/posts/2025/03/zero-to-hero-terraform-for-azure-1/","title":"üñ•Ô∏è Zero to Hero: Terraform for Azure‚Ä¢Part 1","webpImage":"https://quintelier.dev/posts/2025/03/zero-to-hero-terraform-for-azure-1/cover_hu_5d88168d3ba0e952.webp"},{"content":"Securing Azure Identities: The ‚ÄúNew‚Äù Perimeter in Cloud Security It‚Äôs no secret that the cloud has fundamentally changed how we approach cybersecurity. The days when a robust firewall was all you needed to keep attackers at bay are long gone. As cloud-native services increasingly move into the public sphere, identity has emerged as the new defensive perimeter‚Äîif an attacker compromises your identities and credentials, they‚Äôre essentially inside.\nWhy Identities Matter More Than Ever If you‚Äôve participated in a cybersecurity roundtable recently, you‚Äôve likely heard someone mention, ‚ÄúIdentity is the new perimeter.‚Äù Historically, once you were inside a corporate network, you had broad access to internal systems‚Äîmuch like walking through a front door and freely wandering the house. However, the widespread adoption of cloud services‚Äîaccessible from anywhere‚Äîhas turned this model upside down.\nNavigating Azure Identities Given that identities are the linchpin of security, it‚Äôs crucial to understand the different identity types available in Azure. This variety can be a blessing or a curse. On one hand, multiple identity types allow for flexibility across diverse use cases; on the other, choosing the wrong type can inadvertently weaken your security posture.\nBelow is a quick overview of the most common identities in Azure; for the sake of brevity, we‚Äôll focus primarily on user and service principal identities:\nUser Identities Member Users\nCreated and managed within Microsoft Entra ID (formerly Azure AD), or synced from on-premises Active Directory via Entra ID Connect. Guest Users\nExternal accounts invited through Azure AD B2B collaboration to access specific resources. Consumer Users\nManaged through Entra ID B2C, primarily for applications requiring customer-facing authentication. Service Principals Application-Based\nCreated through Azure‚Äôs Application Registrations. Managed Identities User Assigned: Created independently and can be assigned to multiple resources. System Assigned: Automatically spun up and managed by Azure for a specific resource; deleted when the resource is removed. Other Identity Types Device Identities: Entra ID registered, joined, or hybrid-joined devices. External Identities: Federated identities from other identity providers. Group Identities: Security groups or Microsoft 365 Groups in Entra ID‚Äîyes, groups can effectively act like identities. Role-Based Identities: Azure RBAC roles that grant specific privileges. Temporary Identities: Temporary Access Pass (TAP), offering time-limited access. Even if a group isn‚Äôt a ‚Äúuser‚Äù in the traditional sense, having the ability to access certain resources means it demands the same level of security and oversight as a standard user account.\nPractical Tips to Fortify Your Azure Identities Securing identities doesn‚Äôt have to be an uphill battle. Small, strategic steps can dramatically improve your security stance. Below are tried-and-tested measures for both users and workload identities.\nTips for User Accounts Enable Multi-Factor Authentication (MFA)\nIf you do just one thing, do this. MFA blocks the lion‚Äôs share of password-based attacks. Adopt Phishing-Resistant Methods\nEspecially for privileged roles‚Äîoptions like FIDO2 security keys or certificate-based authentication can significantly reduce phishing risk. Explore Passwordless Authentication\nServices like Windows Hello or FIDO2 keys offer both greater convenience and stronger security. Use Conditional Access Policies\nDefine when and where users can log in. For instance, block sign-ins from untrusted devices or geographies. Monitor \u0026amp; Review Frequently\nRegularly audit guest accounts and app permissions to maintain the principle of least privilege. Leverage Built-In Azure Identity Tools\nMicrosoft Entra and Azure AD Identity Protection can automatically flag high-risk activities like risky user or risky sign-in events. Tips for Workload Identities (Service Principals / Managed Identities) Adopt Managed Identities\nInstead of hardcoding credentials in applications, let Azure handle identity lifecycle management. This limits the risk of credential leaks. Enforce the Principle of Least Privilege\nDevelopment often requires broad privileges, but production environments demand precision. Narrow permissions before going live. Avoid Assigning Owners to High-Privilege Apps\nIf a low-privilege user is the ‚Äúowner‚Äù of an app that has a powerful scope (e.g., ‚ÄòDirectory.ReadWrite.All‚Äô), you‚Äôre creating an escalated privilege pathway. Continuously Monitor \u0026amp; Review\nReassess user and app permissions to ensure they remain aligned with operational needs. Securing Azure identities is no longer a ‚Äúnice-to-have‚Äù but an absolute must in today‚Äôs threat landscape. By understanding the range of identity types available, choosing them wisely, and implementing robust security measures‚Äîfrom MFA and passwordless methods to managed identities‚Äîyou‚Äôll significantly decrease your organization‚Äôs risk. After all, identities are now your frontline defense. Keeping them secure keeps everything else safe, too.\n","date":"2025-02-20T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/02/securing-azure-identities/cover_hu_8e8795242f18574c.jpg","image":"https://quintelier.dev/posts/2025/02/securing-azure-identities/cover_hu_5c5439827be2e779.jpg","permalink":"https://quintelier.dev/posts/2025/02/securing-azure-identities/","title":"üîí Securing Azure Identities","webpImage":"https://quintelier.dev/posts/2025/02/securing-azure-identities/cover_hu_21baf6e4a03f653c.webp"},{"content":"Keeping your GitHub repositories clean is crucial for maintainability. Over time, branches pile up, making it difficult to track what‚Äôs relevant. If you‚Äôre managing multiple repositories in an organization, manually identifying stale branches can be a hassle.\nLuckily, with the GitHub CLI (gh), you can automate this process and generate a report of all non-main branches along with their last authors.\nThis guide walks you through a Bash script that:\n‚úÖ Retrieves all repositories in an organization\n‚úÖ Lists branches, excluding main, master, and azure-master\n‚úÖ Identifies the last commit author for each branch\n‚úÖ Groups the results by author for better visibility\nüîß Prerequisites Before running the script, make sure:\nYou have GitHub CLI (gh) installed and authenticated (gh auth login). You have appropriate permissions to list repositories and fetch branch details in your organization. You\u0026rsquo;re running a Unix-based system (Linux/macOS or WSL for Windows). üìú The Script Here‚Äôs the complete Bash script to scan all repositories in your GitHub organization and generate a stale branch report:\n# Set Variables org=\u0026#34;\u0026lt;Replace with your GitHub Organization name\u0026gt;\u0026#34; raw_output_file=\u0026#34;branches.txt\u0026#34; report_file=\u0026#34;report.txt\u0026#34; # Clear previous output \u0026gt; \u0026#34;$raw_output_file\u0026#34; # Get the repositories as JSON and extract names repos=$(gh repo list \u0026#34;$org\u0026#34; --json name --jq \u0026#39;.[].name\u0026#39;) # Function to get branch author safely get_author() { local org=$1 local repo=$2 local branch=$3 local author # Try fetching author name author=$(gh api \u0026#34;/repos/$org/$repo/branches/$branch\u0026#34; --jq \u0026#39;.commit.commit.author.name\u0026#39; 2\u0026gt;/dev/null) # If author is empty, return \u0026#34;Unknown\u0026#34; if [[ -z \u0026#34;$author\u0026#34; ]]; then author=\u0026#34;Unknown\u0026#34; fi # Write to output file echo \u0026#34; - Author: $author\u0026#34; \u0026gt;\u0026gt; \u0026#34;$raw_output_file\u0026#34; } # Loop through each repository for repo in $repos; do echo \u0026#34;- repo: $repo\u0026#34; \u0026gt;\u0026gt; \u0026#34;$raw_output_file\u0026#34; # Get all branches in the repository branches=$(gh api \u0026#34;/repos/$org/$repo/branches\u0026#34; --jq \u0026#39;.[].name\u0026#39;) # Loop through each branch for branch in $branches; do # Skip master, azure-master, and main if [[ \u0026#34;$branch\u0026#34; == \u0026#34;master\u0026#34; || \u0026#34;$branch\u0026#34; == \u0026#34;azure-master\u0026#34; || \u0026#34;$branch\u0026#34; == \u0026#34;main\u0026#34; ]]; then continue fi echo \u0026#34; - Branch: $branch\u0026#34; \u0026gt;\u0026gt; \u0026#34;$raw_output_file\u0026#34; # Fetch branch author get_author \u0026#34;$org\u0026#34; \u0026#34;$repo\u0026#34; \u0026#34;$branch\u0026#34; done done declare -A author_repos # Declare an associative array # Ensure the array is cleared before each run unset author_repos declare -A author_repos # Clear previous report \u0026gt; \u0026#34;$report_file\u0026#34; # Read and process output file while IFS= read -r line; do if [[ $line == \u0026#34;- repo: \u0026#34;* ]]; then repo=$(echo \u0026#34;$line\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) elif [[ $line == \u0026#34; - Branch: \u0026#34;* ]]; then branch=$(echo \u0026#34;$line\u0026#34; | awk \u0026#39;{print $3}\u0026#39;) elif [[ $line == \u0026#34; - Author: \u0026#34;* ]]; then author=$(echo \u0026#34;$line\u0026#34; | sed \u0026#39;s/ - Author: //\u0026#39;) # Extract author name # Store branch under author author_repos[\u0026#34;$author\u0026#34;]+=$\u0026#39;\\n\u0026#39;\u0026#34;- $repo -\u0026gt; $branch\u0026#34; fi done \u0026lt; \u0026#34;$raw_output_file\u0026#34; # Print grouped results for author in \u0026#34;${!author_repos[@]}\u0026#34;; do echo \u0026#34;Author: $author\u0026#34; \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; echo \u0026#34;${author_repos[$author]}\u0026#34; \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; echo \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; done cat \u0026#34;$report_file\u0026#34; üìä Example Output Once executed, the script produces a report grouped by author, making it easy to find out who owns stale branches:\nAuthor: Alice - repo1 -\u0026gt; feature/login-page - repo2 -\u0026gt; hotfix/payment-fix Author: Bob - repo3 -\u0026gt; refactor/api-updates - repo1 -\u0026gt; test/legacy-integration Author: Unknown - repo4 -\u0026gt; bugfix/session-timeout üî• Why This Matters Easier Maintenance ‚Äì Identify branches that can be deleted or merged. Better Collaboration ‚Äì Reach out to authors to confirm branch status. Improved Performance ‚Äì Reducing unnecessary branches speeds up repository operations. You can extend this script further to:\n‚úÖ Filter branches by last commit date\n‚úÖ Automatically delete stale branches (with gh api -X DELETE)\n‚úÖ Generate a GitHub issue or PR listing stale branches\nWould love to hear how you customize this for your workflow! üöÄ\n","date":"2025-01-14T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2025/01/retrieve-stale-branches-github-organization/cover_hu_bde4f379f9e7721c.jpg","image":"https://quintelier.dev/posts/2025/01/retrieve-stale-branches-github-organization/cover_hu_eb429d5034db0a9b.jpg","permalink":"https://quintelier.dev/posts/2025/01/retrieve-stale-branches-github-organization/","title":"üöÄ Find Stale Branches Across All GitHub Repos","webpImage":"https://quintelier.dev/posts/2025/01/retrieve-stale-branches-github-organization/cover_hu_4ad8c09b57c0109b.webp"},{"content":"Azure Storage automatically stores multiple copies of your data to protect against failures, power outages, and even massive natural disasters. Redundancy ensures that your data remains available and durable even when failures occur.\nThis guide covers all redundancy options available in Azure Storage and how they impact data durability, availability, and failover scenarios.\nüåç Primary Region Redundancy Azure Storage always maintains three copies of your data in the primary region. There are two replication options:\nüîπ LRS (Locally Redundant Storage)\nüîπ ZRS (Zone-Redundant Storage)\nüü¢ Locally Redundant Storage (LRS) LRS synchronously copies your data three times within a single physical location in the primary region.\n‚úÖ Lowest-cost option\n‚ö†Ô∏è Not recommended for applications requiring high availability\nLocally Redundant Storage (LRS) üîµ Zone-Redundant Storage (ZRS) ZRS synchronously replicates data across three availability zones within the primary region.\n‚úÖ Recommended for high-availability applications\n‚úÖ Protects against data center failures\nZone Redundant Storage (ZRS) üåé Secondary Region Redundancy (Geo-Redundancy) For higher durability, Azure allows replicating data to a secondary region, located hundreds of miles away from the primary region.\nWhen creating a storage account, you select the primary region, and Azure assigns a paired secondary region (which cannot be changed).\nAzure Storage provides two geo-redundancy options:\nüîπ GRS (Geo-Redundant Storage)\nüîπ GZRS (Geo-Zone-Redundant Storage)\nKey Difference: In both cases, the secondary region always uses LRS (three copies) for durability. However, the primary region\u0026rsquo;s replication method differs.\nüü° Geo-Redundant Storage (GRS) GRS copies data:\nSynchronously (LRS) within the primary region Asynchronously to a single physical location in the secondary region ‚úÖ Protects against regional outages\n‚ö†Ô∏è Data in the secondary region is not readable unless failover occurs\nGeo-Redundant Storage (GRS) üî¥ Geo-Zone-Redundant Storage (GZRS) GZRS combines the benefits of ZRS + GRS:\nSynchronously (ZRS) replicates across three availability zones in the primary region Asynchronously copies to a single location in the secondary region ‚úÖ Best for mission-critical applications\n‚úÖ Protects against both zonal \u0026amp; regional failures\nGeo-Zone-Redundant Storage (GZRS) üìñ Read Access to Secondary Region By default, GRS and GZRS replicate data to a secondary region but do not allow direct access.\nHowever, if your application requires read access to the secondary region during a primary region outage, you can enable:\nüîπ Read-Access Geo-Redundant Storage (RA-GRS)\nüîπ Read-Access Geo-Zone-Redundant Storage (RA-GZRS)\n‚úÖ Data can be read from the secondary region\n‚ö†Ô∏è Secondary region lags behind the primary (async replication)\nNote: In a disaster scenario, some data might be lost since replication to the secondary region is asynchronous.\nOverview üìä Comparison: Durability \u0026amp; Availability Durability \u0026amp; Availability Parameters Parameter LRS ZRS (RA-)GRS (RA-)GZRS Durability (per year) ‚â• 11 9\u0026rsquo;s ‚â• 12 9\u0026rsquo;s ‚â• 16 9\u0026rsquo;s ‚â• 16 9\u0026rsquo;s Availability (read requests) ‚â• 99.9% (99% for Cool/Archive) ‚â• 99.9% (99% for Cool/Archive) ‚â• 99.9% for GRS / 99.99% for RA-GRS ‚â• 99.9% for GZRS / 99.99% for RA-GZRS Availability (write requests) ‚â• 99.9% (99% for Cool/Archive) ‚â• 99.9% (99% for Cool/Archive) ‚â• 99.9% ‚â• 99.9% Number of copies of data 3 copies (single location) 3 copies (across zones) 6 copies (3 primary + 3 secondary) 6 copies (ZRS primary + LRS secondary) Availability Based on Outage Scenarios Failure Scenario LRS ZRS (RA-)GRS (RA-)GZRS Node failure within a data center ‚úÖ ‚úÖ ‚úÖ ‚úÖ Single data center failure ‚ùå ‚úÖ ‚úÖ ‚úÖ Primary region failure (regional outage) ‚ùå ‚ùå ‚úÖ ‚úÖ Read access to secondary during primary outage ‚ùå ‚ùå ‚úÖ (RA-GRS) ‚úÖ (RA-GZRS) üèÜ Which Azure Storage Redundancy Should You Choose? Use Case Best Option Cost-sensitive workloads, backups, non-critical data LRS High availability within the primary region ZRS Disaster recovery and regional failover protection GRS Mission-critical workloads requiring both zonal \u0026amp; regional protection GZRS Applications requiring immediate read access to secondary RA-GRS / RA-GZRS üõ† Final Thoughts Azure Storage offers multiple redundancy options to ensure data durability and availability. Choosing the right replication strategy depends on:\n‚úîÔ∏è Business requirements ‚Äì Do you need cross-region failover?\n‚úîÔ∏è Cost considerations ‚Äì ZRS and GZRS are costlier but offer better availability.\n‚úîÔ∏è Read requirements ‚Äì Do you need read access to the secondary region?\nIf you‚Äôre running mission-critical applications, GZRS (or RA-GZRS) is your best bet. Otherwise, ZRS or GRS might be sufficient depending on your redundancy needs.\nüöÄ What‚Äôs your go-to storage redundancy option? Drop a comment below! üëá\n","date":"2023-03-20T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2023/03/azure-storage-redundancy-data-availability-durability/cover_hu_f5ea2809b2f23eda.jpg","image":"https://quintelier.dev/posts/2023/03/azure-storage-redundancy-data-availability-durability/cover_hu_ed2aeb9087df276f.jpg","permalink":"https://quintelier.dev/posts/2023/03/azure-storage-redundancy-data-availability-durability/","title":"üîπ Azure Storage Redundancy: Availability \u0026 Durability","webpImage":"https://quintelier.dev/posts/2023/03/azure-storage-redundancy-data-availability-durability/cover_hu_92a71c4fc4ab7c45.webp"},{"content":"What is Load Balancing? Load balancing is the even distribution of network traffic across a group of backend computing resources or servers. The primary goals of load balancing are:\n‚úÖ Optimizing resource utilization\n‚úÖ Maximizing throughput \u0026amp; performance\n‚úÖ Minimizing response time\n‚úÖ Ensuring high availability\n‚úÖ Preventing overload on a single resource\nIn Azure, there are multiple load-balancing options, each designed for different traffic types and use cases.\nTL;DR (2025 Update) Goal Pick Why Layer / Scope Global HTTP(S) acceleration + WAF Front Door Std/Premium Anycast edge, caching, rapid failover L7 / Global Global single anycast IP for TCP/UDP Cross-region Load Balancer L4 pass-through, fast failover (no DNS TTL) L4 / Global Hybrid or external endpoint steering Traffic Manager DNS-based latency / geo / weighted policies DNS / Global In-region advanced HTTP routing + per-app WAF Application Gateway v2 Path/host routing, rewrites, mTLS L7 / Regional Regional TCP/UDP balancing + outbound SNAT Standard Load Balancer High-perf, zone redundant L4 / Regional Insert firewall / IDS transparently Gateway Load Balancer Inline NVA chaining without UDR complexity L3/L4 / Regional Legacy Basic LB still present Migrate to Standard Basic retires 30 Sept 20251 ‚Äî Plan outbound explicitly before default outbound access retirement (30 Sept 2025)2. Combine services (e.g. Front Door + App Gateway) for layered designs.\nüîç Azure Load Balancing Options (2025 Update) The Azure portfolio has evolved since this article was first published. Notable changes:\nBasic Load Balancer retires 30 Sept 2025 (no new Basic deployments after 31 Mar 2025). Standard Load Balancer now supports a cross-region (global) tier. Gateway Load Balancer enables transparent NVA insertion. Azure Front Door has Standard and Premium tiers (Classic is legacy). Service Scope Recommended For Classification Standard Load Balancer Regional \u0026amp; Global (cross-region) Non-HTTP(S) TCP/UDP, high-performance L4, outbound SNAT Layer 4 (data plane) Traffic Manager Global (DNS) Latency / geo / weighted / priority routing; hybrid endpoints DNS-based traffic steering (not an inline proxy) Application Gateway (v2) Regional Advanced HTTP(S) app delivery, WAF, mTLS, rewrites Layer 7 proxy Azure Front Door (Std/Premium) Global Global web apps \u0026amp; APIs, acceleration, WAF, rules engine Layer 7 anycast edge Gateway Load Balancer Regional (inline) Transparent insertion of NVAs (firewall, IDS/IPS, DPI) Layer 3/4 service chaining (Legacy) Basic Load Balancer Regional Legacy workloads only (migrate) Retiring 30 Sept 2025 Load Balancing Decision Tree Now, let‚Äôs explore each of these services in detail.\nüåç Azure Load Balancer (Standard) Azure Load Balancer Standard Azure Load Balancer is a Layer 4 (TCP/UDP) load-balancing service designed for high-performance and ultra-low-latency traffic. It distributes inbound and outbound flows and is zone redundant. A cross-region (global) Load Balancer SKU3 lets you expose a single anycast IP fronting multiple regional Standard Load Balancers for active-active or fast failover scenarios.\nRetirement Notice: Basic Load Balancer retires on 30 Sept 20251. Migrate to Standard for: security by default (closed unless NSG permits), higher scale, zone redundancy, SLA (99.99%), HA ports, and global tier integration.\nTypes of Azure Load Balancers Type Purpose Public Load Balancer Distributes internet-facing traffic across VMs in a VNet. Internal Load Balancer Distributes private network traffic within Azure. ALB Public vs Internal Availability Zone Configurations Mode Behavior Zone Redundant Uses a single IP, surviving zone failures. Zonal Restricts traffic to a specific zone. ALB Zone Redundant Standard vs. (Retiring) Basic Load Balancer Feature Standard Basic (Legacy) Backend pool size Up to 1000 instances 300 instances Health probes TCP, HTTP, HTTPS TCP, HTTP Security posture Closed by default (NSG allow required) Open to internet by default HA Ports (all ports) ‚úÖ Supported ‚ùå Not supported Zonal / Zone redundant ‚úÖ Yes ‚ùå No Cross-region (global tier) ‚úÖ Yes ‚ùå No SLA 99.99% (‚â•2 healthy instances) None Retirement status Active Retires 30 Sept 2025 Action: Audit for any Basic SKUs (including Basic Public IP) and plan upgrade before the retirement cutoff1.\nCross-region (Global) Load Balancer Provides a globally anycast IPv4 frontend distributing traffic to a backend pool of regional public Standard Load Balancers. Use it when you need:\nFast failover without DNS TTL delays (contrast with Traffic Manager). A single global IP for multi-region L4 workloads (TCP/UDP). Simpler active-active pattern for stateful protocols (hash-based distribution per flow). Not a replacement for Front Door (no HTTP features) nor Traffic Manager (which can include non-Azure endpoints \u0026amp; complex routing policies).\nOutbound Connectivity (Retirement Note) Default outbound access for VMs retires 30 Sept 20252. Plan explicit outbound via:\nNAT Gateway Standard Load Balancer outbound rules (frontend IPs) Instance-level public IP (least preferred for fleets) Prefer NAT Gateway for high SNAT scale; LB outbound fits when you already require inbound load balancing.\nüåê Azure Traffic Manager (DNS-based Routing) Azure Traffic Manager Traffic Manager is a DNS-based global traffic steering service4. It does not proxy or terminate connections; it returns the best endpoint (based on the chosen routing method) to the client resolver. Because of DNS caching, failover is influenced by TTL and client resolver behaviour.\nHow It Works 1Ô∏è‚É£ A client requests a domain (e.g., app.contoso.com).\n2Ô∏è‚É£ The DNS system redirects to contoso.trafficmanager.net.\n3Ô∏è‚É£ Traffic Manager selects a backend using health checks \u0026amp; routing rules.\n4Ô∏è‚É£ The client receives the IP of the closest, available backend and connects directly.\nAzure Traffic Manager Setup Routing Methods Routing Method Use Case Priority Primary backend with failover options. Weighted Distribute traffic based on weights. Performance Route traffic to the closest backend. Geographic Route traffic based on user location. MultiValue Return multiple healthy endpoints. Subnet Route based on user IP ranges. Traffic Manager Routing Traffic Manager is ideal for:\n‚úîÔ∏è Latency-based routing where DNS steering is sufficient\n‚úîÔ∏è Hybrid / external endpoints (on-prem, other clouds)\n‚úîÔ∏è Controlled weighted canary rollouts\n‚úîÔ∏è Geographic compliance routing (data sovereignty)\nüîπ Azure Application Gateway (v2) Azure Application Gateway Application Gateway is a Layer 7 load balancer designed specifically for HTTP(S) traffic5. It provides advanced web traffic routing, SSL offloading, and Web Application Firewall (WAF) integration.\nKey Features (v2 SKU) ‚úÖ Path- \u0026amp; host-based routing\n‚úÖ Session affinity (cookie-based)\n‚úÖ TLS termination \u0026amp; end-to-end TLS\n‚úÖ Mutual TLS (client cert auth)\n‚úÖ Web Application Firewall (WAF) (Prevention / Detection modes)\n‚úÖ HTTP/2, WebSockets\n‚úÖ Header \u0026amp; URL rewrite rules\n‚úÖ Autoscaling \u0026amp; zone redundancy\n‚úÖ Custom error pages \u0026amp; diagnostics (Access logs, Performance logs)\nApplication Gateway Flow Best for:\n‚úîÔ∏è Web applications that require advanced traffic routing.\n‚úîÔ∏è Security-conscious deployments using WAF protection.\nüåé Azure Front Door (Standard/Premium) Azure Front Door Azure Front Door is a global anycast Layer 7 application delivery network6 offering dynamic site acceleration, intelligent routing, and integrated security.\nKey Capabilities ‚úÖ Global HTTP(S) load balancing \u0026amp; fast failover\n‚úÖ Dynamic \u0026amp; static content acceleration (edge POP caching)\n‚úÖ Rules Engine (headers, redirects, rewrites)\n‚úÖ Custom domains \u0026amp; managed certificates\n‚úÖ Integrated WAF (Core Rule Set), DDoS edge protection\n‚úÖ Private origin support (Premium) via Private Link\n‚úÖ Advanced security features (Premium): Bot protection, more granular WAF features\n‚úÖ Near real-time health probes \u0026amp; rapid failover (vs DNS TTL delays)\nüí° Front Door vs. Traffic Manager:\nFront Door: Inline HTTP(S) proxy, real-time health \u0026amp; latency-based routing, edge caching, WAF. Traffic Manager: DNS answer steering only; no caching, TLS, or header logic; supports non-HTTP endpoints \u0026amp; external origins. Best for:\n‚úîÔ∏è Global applications that need low latency.\n‚úîÔ∏è Web APIs requiring intelligent traffic routing.\nüåç Global vs. Regional \u0026amp; Layer Considerations Service Scope Primary Layer / Mode Typical Use Case Cross-region (Global) Load Balancer Global L4 TCP/UDP pass-through Single global IP for multi-region backend L4 workloads Standard Load Balancer (Regional) Regional L4 TCP/UDP Intra-region distribution \u0026amp; outbound SNAT Azure Front Door (Std/Premium) Global L7 HTTP(S) proxy Global web/API acceleration \u0026amp; security Traffic Manager Global (DNS) DNS steering Latency / geo / weighted routing incl. external endpoints Application Gateway (v2) Regional L7 HTTP(S) proxy Regional web app delivery \u0026amp; WAF with VNet integration Gateway Load Balancer Regional (inline) Service chaining (L3/L4) Transparent NVA insertion (firewall, IDS/IPS) Selection Cheat Sheet Scenario Recommended Notes Global low-latency web/API + caching + WAF Front Door Premium Add App Gateway if per-app internal segmentation needed Multi-region L4 (non-HTTP) with single IP Cross-region Load Balancer Backend pool = regional Standard LBs DNS-based hybrid (on-prem \u0026amp; Azure) Traffic Manager Combine with Front Door or regional LBs In-region advanced routing \u0026amp; WAF Application Gateway Can sit behind Front Door Insert firewall / NVA transparently Gateway Load Balancer (+ Standard LB) Chaining simplifies flow symmetry Legacy Basic LB deployment Migrate to Standard LB Plan before retirement date Common Combinations Front Door + Application Gateway: Global entry + regional path-based routing \u0026amp; WAF policies separation. Front Door + Standard Load Balancer: Global HTTP(S) to edge; regional L4 services or container ingress. Cross-region Load Balancer + Traffic Manager: Rare‚ÄîTraffic Manager as external fallback or hybrid integration. Gateway Load Balancer + Standard Public LB + Front Door: Full chain: edge (FD) ‚Üí security NVA (GLB) ‚Üí application tier. Traffic Manager + Front Door: Layered control for regulatory geo mapping with Front Door acceleration. üìå Final Thoughts Azure offers multiple load balancing solutions, each designed for specific traffic types, regions, and use cases. Whether you\u0026rsquo;re building a global web application or optimizing regional traffic, choosing the right service is key to maximizing performance, availability, and security.\nüí° Summary (2025):\n‚úîÔ∏è Use Front Door (Std/Premium) for global HTTP(S) acceleration \u0026amp; security.\n‚úîÔ∏è Use Cross-region Load Balancer for global L4 with a single anycast IP.\n‚úîÔ∏è Use Traffic Manager for DNS-based steering \u0026amp; hybrid endpoints.\n‚úîÔ∏è Use Application Gateway (v2) for regional L7 with WAF \u0026amp; rewrites.\n‚úîÔ∏è Use Standard Load Balancer for regional L4 + outbound SNAT.\n‚úîÔ∏è Use Gateway Load Balancer to insert NVAs transparently.\n‚úîÔ∏è Migrate any Basic SKUs before 30 Sept 2025.\nAzure Basic Load Balancer retirement announcement: Official notice\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDefault outbound access retirement: Announcement\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCross-region (Global) Load Balancer overview: Docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLoad balancing options \u0026amp; architecture guidance: Docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAzure Application Gateway overview: Docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAzure Front Door overview: Docs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2022-06-17T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2022/06/azure-load-balancing-explained/cover_hu_66c59fbc672c35c7.jpg","image":"https://quintelier.dev/posts/2022/06/azure-load-balancing-explained/cover_hu_4de38a0df85892bc.jpg","permalink":"https://quintelier.dev/posts/2022/06/azure-load-balancing-explained/","title":"Azure Load Balancing: Choose the Right Option","webpImage":"https://quintelier.dev/posts/2022/06/azure-load-balancing-explained/cover_hu_d693f6e04f945ac0.webp"},{"content":"This blog post captures my notes from the AZ-700 course, which is designed to teach Network Engineers how to design, implement, and maintain Azure networking solutions. The course covers a wide range of networking topics, including:\nDesigning, implementing, and managing core Azure networking infrastructure Hybrid networking connections for on-premises integration Load balancing strategies for optimizing traffic distribution Routing and private access to Azure services Network security and traffic filtering Monitoring and troubleshooting network connectivity 1. Virtual Networks (VNets) Azure Virtual Networks (VNets) are the backbone of networking in Azure, allowing resources to communicate securely.\nVNet Capabilities Azure VNets support:\n‚úÖ Communication with the internet\n‚úÖ Communication between Azure resources\n‚úÖ Secure connectivity to on-premises networks\n‚úÖ Traffic filtering using NSGs (Network Security Groups)\n‚úÖ Routing network traffic efficiently\nVNet Address Space Azure VNets use private IP address ranges as defined in RFC 1918:\nIP Range Prefix 10.0.0.0 - 10.255.255.255 10/8 172.16.0.0 - 172.31.255.255 172.16/12 192.168.0.0 - 192.168.255.255 192.168/16 Subnet Allocation Azure reserves 5 IPs per subnet:\nüîπ x.x.x.0 ‚Üí Network address\nüîπ x.x.x.1 ‚Üí Default gateway\nüîπ x.x.x.2 \u0026amp; x.x.x.3 ‚Üí Azure DNS mapping\nüîπ x.x.x.255 ‚Üí Broadcast address\n2. Scopes in Azure In Azure, every resource must have a unique name within its defined scope. Scopes are hierarchical:\n1Ô∏è‚É£ Global (e.g., Storage Accounts)\n2Ô∏è‚É£ Management Group\n3Ô∏è‚É£ Subscription\n4Ô∏è‚É£ Resource Group (e.g., VNets)\n5Ô∏è‚É£ Resource (individual resource instances)\n3. Regions \u0026amp; Availability Zones Regions \u0026amp; Subscriptions Resources in a VNet must be in the same region, but cross-region connectivity is possible. VNets can be linked across different subscriptions. Availability Zones (AZs) Availability Zones provide high availability by distributing resources across physically separate data centers within a region.\nüîπ Zonal Services ‚Üí Resources pinned to a specific zone\nüîπ Zone-Redundant Services ‚Üí Automatically replicated across zones\nüîπ Non-Regional Services ‚Üí Resilient to zone-wide and region-wide failures\n4. Public IPs in Azure Public IPs enable external communication for Azure resources. They can be static (unchanging) or dynamic (reassigned upon restart).\nPublic IP Type Allocation Security Zone Support Basic SKU Static / Dynamic Open by default ‚ùå No AZ support Standard SKU Static only Secure by default (NSG required) ‚úÖ Zone-redundant 5. DNS Resolution in Azure Azure provides both public and private DNS services to resolve domain names.\nDNS Public DNS Azure DNS manages internet-facing domain names and supports:\nüîπ A / AAAA records for IPv4/IPv6\nüîπ CNAME records for aliasing domains\nPrivate DNS For internal name resolution within VNets, Azure supports:\n1Ô∏è‚É£ Azure DNS Private Zones\n2Ô∏è‚É£ Azure-provided name resolution\n3Ô∏è‚É£ Custom DNS servers\nüîπ Azure\u0026rsquo;s built-in DNS resolver: 168.63.129.16\nDNS forwarding allows on-premises resources to resolve Azure hostnames, ensuring seamless hybrid connectivity.\nüìå Example: Conditional Forwarding\nTo resolve hostnames across VNets, use custom DNS servers with conditional forwarding rules.\n6. VNet Peering for Cross-Network Connectivity Azure VNet Peering allows seamless communication between VNets without a VPN.\nPeering Type Scope Performance Regional Peering Same Azure region High bandwidth, low latency Global Peering Cross-region Uses Azure backbone VNet Peering Benefits ‚úÖ Secure private communication (no internet exposure)\n‚úÖ No need for VPN gateways\n‚úÖ Supports NSGs for access control\n‚úÖ Works across subscriptions and tenants\nPeering 7. Gateway Transit for Shared VPN Access Gateway Transit allows one VNet to use another VNet‚Äôs VPN gateway for cross-premises connectivity.\nüí° Use case: A hub-and-spoke topology where a single gateway in the hub VNet provides VPN access to multiple spokes.\nGateway Transit 8. Azure Traffic Routing Azure manages traffic routing through:\n1Ô∏è‚É£ System Routes (default routes created by Azure) üîπ Internet traffic ‚Üí Sent via the default Internet Gateway\nüîπ Private traffic ‚Üí Stays within the VNet\n2Ô∏è‚É£ Custom Routes (UDRs - User Defined Routes) Use route tables to override system routes.\nüí° Example: Direct traffic to a firewall appliance instead of the default gateway.\nDefault Route Table Source Destination Next Hop Default 0.0.0.0/0 Internet Default 10.0.0.0/8 None Default 192.168.0.0/16 None Final Thoughts This post provides a comprehensive summary of key Azure networking concepts from AZ-700. Understanding VNets, peering, DNS, and routing is essential for designing scalable, secure, and high-performing cloud networks.\nüìå Key Takeaways:\n‚úÖ Master VNet and subnet design to optimize address space\n‚úÖ Use peering and gateway transit for hybrid connectivity\n‚úÖ Leverage DNS solutions to simplify name resolution\n‚úÖ Control traffic with NSGs \u0026amp; UDRs for security and compliance\nüî• If you\u0026rsquo;re studying for AZ-700, focus on hands-on labs to reinforce concepts! üöÄ\n","date":"2022-06-03T06:00:00Z","fallbackImage":"https://quintelier.dev/posts/2022/06/az-700-prep-highlights/cover_hu_708ecc8ed2a804fe.jpg","image":"https://quintelier.dev/posts/2022/06/az-700-prep-highlights/cover_hu_26071f682f974f78.jpg","permalink":"https://quintelier.dev/posts/2022/06/az-700-prep-highlights/","title":"üìò AZ-700 Prep Highlights","webpImage":"https://quintelier.dev/posts/2022/06/az-700-prep-highlights/cover_hu_196b19506068a39d.webp"}]